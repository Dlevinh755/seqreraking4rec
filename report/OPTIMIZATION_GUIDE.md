# Optimization Guide - TƒÉng t·ªëc Semantic Summary v√† LLM Inference

## üìä T·ªïng quan

H∆∞·ªõng d·∫´n n√†y m√¥ t·∫£ c√°c c√°ch ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô:
1. **Semantic Summary Generation** (Qwen3-VL)
2. **LLM Inference** (Qwen reranker)

## üöÄ Optimizations cho Semantic Summary Generation

### 1. TƒÉng Batch Size

**Hi·ªán t·∫°i**: `BATCH_SIZE = 4`, nh∆∞ng ch·ªâ group images, kh√¥ng batch inference th·ª±c s·ª±.

**Optimization**: 
- TƒÉng `BATCH_SIZE` n·∫øu GPU memory cho ph√©p (8, 16, 32)
- Implement batch inference th·ª±c s·ª± n·∫øu model support

**C√°ch s·ª≠ d·ª•ng**:
```python
# Trong config.py ho·∫∑c command line
--semantic_summary_batch_size 8  # TƒÉng t·ª´ 4 l√™n 8
```

### 2. Model Quantization

**8-bit Quantization**:
```python
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
)
model = Qwen3VLForConditionalGeneration.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto",
)
```

**4-bit Quantization** (ti·∫øt ki·ªám memory h∆°n):
```python
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
)
```

### 3. torch.compile() (PyTorch 2.0+)

**Compile model ƒë·ªÉ tƒÉng t·ªëc**:
```python
model = torch.compile(model, mode="reduce-overhead")
```

**L∆∞u √Ω**: C·∫ßn PyTorch 2.0+ v√† c√≥ th·ªÉ m·∫•t th·ªùi gian compile l·∫ßn ƒë·∫ßu.

### 4. Flash Attention 2

**S·ª≠ d·ª•ng Flash Attention 2** (n·∫øu model support):
```python
model = Qwen3VLForConditionalGeneration.from_pretrained(
    model_name,
    attn_implementation="flash_attention_2",
)
```

**C√†i ƒë·∫∑t**:
```bash
pip install flash-attn --no-build-isolation
```

### 5. Parallel Processing

**Process multiple images song song** (n·∫øu kh√¥ng th·ªÉ batch):
```python
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=2) as executor:
    futures = [executor.submit(process_image, img) for img in images]
    results = [f.result() for f in futures]
```

### 6. Caching Processed Images

**Cache resized images** ƒë·ªÉ tr√°nh resize l·∫°i:
```python
from functools import lru_cache
from PIL import Image

@lru_cache(maxsize=1000)
def load_and_resize_image(image_path, max_size=448):
    img = Image.open(image_path).convert("RGB")
    # Resize logic...
    return img
```

## üöÄ Optimizations cho LLM Inference

### 1. Batch Inference

**Hi·ªán t·∫°i**: Process t·ª´ng prompt m·ªôt.

**Optimization**: Batch multiple prompts:
```python
def predict_probs_batch(self, prompts, num_candidates_list):
    """Batch inference cho multiple prompts."""
    # Tokenize all prompts
    inputs = self.tokenizer(
        prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
    ).to(self.model.device)
    
    with torch.no_grad():
        outputs = self.model(**inputs)
        logits = outputs.logits[:, -1]  # [batch_size, vocab_size]
    
    # Process each prompt's logits
    results = []
    for i, num_candidates in enumerate(num_candidates_list):
        # Extract probabilities for this prompt
        # ... (similar to predict_probs)
        results.append(prob_array)
    
    return results
```

### 2. Model Quantization

**8-bit ho·∫∑c 4-bit quantization**:
```python
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="Qwen/Qwen3-0.6B",
    max_seq_length=2048,
    dtype=torch.float16,
    load_in_4bit=True,  # 4-bit quantization
)
```

### 3. torch.compile()

**Compile model**:
```python
self.model = torch.compile(self.model, mode="reduce-overhead")
```

### 4. vLLM (Very Fast LLM Inference)

**S·ª≠ d·ª•ng vLLM** cho faster inference:
```bash
pip install vllm
```

**S·ª≠ d·ª•ng**:
```python
from vllm import LLM, SamplingParams

llm = LLM(model="Qwen/Qwen3-0.6B", quantization="awq")
sampling_params = SamplingParams(temperature=0, max_tokens=1)

prompts = ["prompt1", "prompt2", ...]
outputs = llm.generate(prompts, sampling_params)
```

**L∆∞u √Ω**: vLLM y√™u c·∫ßu GPU v√† c√≥ th·ªÉ kh√¥ng support t·∫•t c·∫£ models.

### 5. Text Generation Inference (TGI)

**S·ª≠ d·ª•ng TGI** t·ª´ Hugging Face:
```bash
# Docker
docker run --gpus all -p 8080:80 \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id Qwen/Qwen3-0.6B
```

### 6. Caching Prompts

**Cache tokenized prompts** n·∫øu prompts l·∫∑p l·∫°i:
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def tokenize_prompt(prompt):
    return self.tokenizer(prompt, return_tensors="pt")
```

## üìà Expected Speedup

| Optimization | Semantic Summary | LLM Inference | Memory Impact |
|--------------|------------------|---------------|--------------|
| Increase Batch Size | 2-4x | 3-5x | +50-100% |
| 8-bit Quantization | 1.5-2x | 1.5-2x | -50% |
| 4-bit Quantization | 2-3x | 2-3x | -75% |
| torch.compile() | 1.2-1.5x | 1.2-1.5x | 0% |
| Flash Attention 2 | 1.5-2x | N/A | -20% |
| vLLM | N/A | 5-10x | +20% |
| Batch Inference | N/A | 3-5x | +30% |

## ‚öôÔ∏è Implementation Priority

### Quick Wins (D·ªÖ implement, hi·ªáu qu·∫£ cao):
1. ‚úÖ TƒÉng batch size cho semantic summary
2. ‚úÖ 4-bit quantization cho c·∫£ hai
3. ‚úÖ torch.compile() cho c·∫£ hai
4. ‚úÖ Batch inference cho LLM

### Medium Effort (C·∫ßn thay ƒë·ªïi code nhi·ªÅu h∆°n):
1. Flash Attention 2 cho Qwen3-VL
2. Parallel processing cho semantic summary
3. Caching mechanisms

### Advanced (C·∫ßn setup ph·ª©c t·∫°p):
1. vLLM cho LLM inference
2. TGI server
3. Multi-GPU inference

## üîß Configuration

Th√™m v√†o `config.py`:
```python
parser.add_argument('--semantic_summary_batch_size', type=int, default=4,
                   help='Batch size for semantic summary generation')
parser.add_argument('--llm_batch_size', type=int, default=1,
                   help='Batch size for LLM inference')
parser.add_argument('--use_quantization', action='store_true',
                   help='Use 4-bit quantization for models')
parser.add_argument('--use_torch_compile', action='store_true',
                   help='Use torch.compile() for faster inference')
parser.add_argument('--use_flash_attention', action='store_true',
                   help='Use Flash Attention 2 (if supported)')
```

## üìù Notes

- **Memory vs Speed**: Quantization gi·∫£m memory nh∆∞ng c√≥ th·ªÉ gi·∫£m accuracy nh·∫π
- **GPU Required**: H·∫ßu h·∫øt optimizations c·∫ßn GPU
- **Compatibility**: M·ªôt s·ªë optimizations kh√¥ng work v·ªõi t·∫•t c·∫£ models
- **Testing**: Lu√¥n test accuracy sau khi apply optimizations

