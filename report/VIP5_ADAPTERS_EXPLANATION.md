# VIP5 Adapters - Giáº£i thÃ­ch vÃ  TÃ¡c dá»¥ng

## âœ… Tráº¡ng thÃ¡i hiá»‡n táº¡i

**Adapters Ä‘Ã£ cÃ³ trong project!** 

Folder `rerank/models/adapters/` Ä‘Ã£ Ä‘Æ°á»£c copy tá»« VIP5 repository vÃ  bao gá»“m:
- `adapter_controller.py`: Controller quáº£n lÃ½ adapters
- `adapter_modeling.py`: CÃ¡c loáº¡i adapter layers
- `adapter_configuration.py`: Configuration cho adapters
- `adapter_hypernetwork.py`: Hypernetwork adapters
- `adapter_utils.py`: Utilities
- `config.py`: Config classes
- `low_rank_layer.py`: Low-rank adapters
- `hypercomplex/`: Hypercomplex adapters

## ğŸ¯ TÃ¡c dá»¥ng cá»§a Adapters trong VIP5

### 1. **Parameter-Efficient Fine-Tuning (PEFT)**

Adapters cho phÃ©p fine-tune VIP5 model vá»›i **ráº¥t Ã­t parameters**:

- **KhÃ´ng cáº§n fine-tune toÃ n bá»™ model**: Chá»‰ train má»™t sá»‘ lÆ°á»£ng nhá» parameters trong adapter layers
- **Giáº£m memory vÃ  computation**: Chá»‰ cáº§n lÆ°u vÃ  update adapter weights, khÃ´ng pháº£i toÃ n bá»™ model
- **Nhanh hÆ¡n**: Training vÃ  inference nhanh hÆ¡n so vá»›i full fine-tuning

### 2. **Multi-Task Learning**

VIP5 sá»­ dá»¥ng adapters Ä‘á»ƒ há»— trá»£ **nhiá»u tasks khÃ¡c nhau**:

- **Sequential recommendation**: Dá»± Ä‘oÃ¡n item tiáº¿p theo
- **Direct recommendation**: ÄÃ¡nh giÃ¡ user-item preference
- **Explanation generation**: Táº¡o explanation cho recommendations

Má»—i task cÃ³ thá»ƒ cÃ³ adapter riÃªng, cho phÃ©p model há»c task-specific features mÃ  khÃ´ng lÃ m áº£nh hÆ°á»Ÿng Ä‘áº¿n base model.

### 3. **Task-Specific Adaptation**

Adapters cho phÃ©p model **adapt** cho tá»«ng task cá»¥ thá»ƒ:

```python
# Trong VIP5 forward pass
if self.ff_adapter is not None:
    forwarded_states = self.ff_adapter(forwarded_states, task)  # task = "sequential", "direct", etc.
```

Model tá»± Ä‘á»™ng chá»n adapter phÃ¹ há»£p vá»›i task hiá»‡n táº¡i.

## ğŸ”§ CÃ¡ch Adapters hoáº¡t Ä‘á»™ng trong VIP5

### Architecture

Adapter lÃ  má»™t **bottleneck layer** vá»›i cáº¥u trÃºc:

```
Input (d_model) 
  â†“
Down-sampling (d_model â†’ d_model/reduction_factor)
  â†“
Activation (GELU)
  â†“
Up-sampling (d_model/reduction_factor â†’ d_model)
  â†“
Output (d_model)
```

**VÃ­ dá»¥**: Vá»›i `d_model=512` vÃ  `reduction_factor=16`:
- Down-sampling: 512 â†’ 32
- Up-sampling: 32 â†’ 512
- **Chá»‰ train 512Ã—32 + 32Ã—512 = 32,768 parameters** thay vÃ¬ toÃ n bá»™ layer

### Vá»‹ trÃ­ trong VIP5

Adapters Ä‘Æ°á»£c thÃªm vÃ o **3 vá»‹ trÃ­ chÃ­nh**:

1. **Feed-Forward Layer** (`T5LayerFF`):
   ```python
   # Trong T5LayerFF.forward()
   forwarded_states = self.DenseReluDense(forwarded_states)
   if self.ff_adapter is not None:
       forwarded_states = self.ff_adapter(forwarded_states, task)  # â† Adapter á»Ÿ Ä‘Ã¢y
   ```

2. **Self-Attention Layer** (`T5LayerSelfAttention`):
   ```python
   # Trong T5LayerSelfAttention.forward()
   y = attention_output[0]
   if self.attn_adapter is not None:
       y = self.attn_adapter(y, task)  # â† Adapter á»Ÿ Ä‘Ã¢y
   ```

3. **Cross-Attention Layer** (`T5LayerCrossAttention`):
   ```python
   # Trong T5LayerCrossAttention.forward()
   y = attention_output[0]
   if self.enc_attn_adapter is not None:
       y = self.enc_attn_adapter(y, task)  # â† Adapter á»Ÿ Ä‘Ã¢y
   ```

4. **LM Head** (`VIP5`):
   ```python
   # Trong VIP5.__init__()
   if config.use_lm_head_adapter:
       self.output_adapter = OutputParallelAdapterLayer(...)  # â† Adapter cho output
   ```

### AdapterController

`AdapterController` quáº£n lÃ½ nhiá»u adapters cho nhiá»u tasks:

```python
class AdapterController(nn.Module):
    def __init__(self, config):
        self.adapters = nn.ModuleDict()
        # Táº¡o adapter cho má»—i task
        for task in tasks:
            self.adapters[task] = Adapter(config)
    
    def forward(self, inputs, task):
        # Chá»n adapter phÃ¹ há»£p vá»›i task
        adapter = self.get_adapter(task)
        return adapter(inputs)
```

## ğŸ“Š So sÃ¡nh: CÃ³ Adapter vs KhÃ´ng cÃ³ Adapter

### KhÃ´ng cÃ³ Adapter (`use_adapter=False`):
- Fine-tune toÃ n bá»™ model
- Cáº§n nhiá»u memory vÃ  computation
- Cháº­m hÆ¡n
- KhÃ³ multi-task learning

### CÃ³ Adapter (`use_adapter=True`):
- Chá»‰ fine-tune adapter layers (~1-5% parameters)
- Tiáº¿t kiá»‡m memory vÃ  computation
- Nhanh hÆ¡n
- Dá»… dÃ ng multi-task learning
- CÃ³ thá»ƒ share base model cho nhiá»u tasks

## ğŸ¨ CÃ¡c loáº¡i Adapters trong VIP5

### 1. **Standard Adapter** (Adapter)
- Bottleneck architecture: down â†’ activation â†’ up
- Reduction factor: 16 (default)
- Parameters: ~1/16 cá»§a full layer

### 2. **Low-Rank Adapter** (LowRankAdapter)
- Sá»­ dá»¥ng low-rank matrices
- CÃ²n Ã­t parameters hÆ¡n
- PhÃ¹ há»£p cho resource-constrained environments

### 3. **HyperComplex Adapter** (HyperComplexAdapter)
- Sá»­ dá»¥ng hypercomplex multiplication
- Parameters: 1/n so vá»›i standard adapter (n = hypercomplex_division)
- Hiá»‡u quáº£ hÆ¡n vá» memory

### 4. **Output Adapter** (OutputAdapter)
- DÃ¹ng cho LM head
- Output dimension cÃ³ thá»ƒ khÃ¡c input dimension

## ğŸ’¡ Lá»£i Ã­ch cá»¥ thá»ƒ cho VIP5

1. **Efficient Training**:
   - Chá»‰ train adapters thay vÃ¬ toÃ n bá»™ T5 model
   - Giáº£m training time vÃ  memory usage

2. **Task Specialization**:
   - Má»—i task (sequential, direct, explanation) cÃ³ adapter riÃªng
   - Model cÃ³ thá»ƒ há»c task-specific patterns

3. **Transfer Learning**:
   - Base T5 model Ä‘Æ°á»£c giá»¯ nguyÃªn (pretrained weights)
   - Chá»‰ adapters Ä‘Æ°á»£c fine-tune cho recommendation tasks

4. **Scalability**:
   - Dá»… dÃ ng thÃªm tasks má»›i báº±ng cÃ¡ch thÃªm adapter má»›i
   - KhÃ´ng cáº§n retrain toÃ n bá»™ model

## ğŸ” Kiá»ƒm tra Adapters trong Code

### Trong `vip5_modeling.py`:

```python
# T5LayerFF - Feed-forward adapter
if config.use_adapter:
    self.ff_adapter = AdapterController(config.adapter_config)

# T5LayerSelfAttention - Self-attention adapter  
if config.use_adapter:
    self.attn_adapter = AdapterController(config.adapter_config)

# T5LayerCrossAttention - Cross-attention adapter
if config.use_adapter and config.add_adapter_cross_attn:
    self.enc_attn_adapter = AdapterController(config.adapter_config)

# VIP5 - Output adapter
if config.use_lm_head_adapter:
    self.output_adapter = OutputParallelAdapterLayer(...)
```

### Sá»­ dá»¥ng trong Forward Pass:

```python
# Feed-forward
forwarded_states = self.DenseReluDense(forwarded_states)
if self.ff_adapter is not None:
    forwarded_states = self.ff_adapter(forwarded_states, task)  # task-aware

# Self-attention
y = attention_output[0]
if self.attn_adapter is not None:
    y = self.attn_adapter(y, task)  # task-aware
```

## âš™ï¸ Configuration

Äá»ƒ sá»­ dá»¥ng adapters trong VIP5:

```python
from rerank.models.adapters.config import AdapterConfig

adapter_config = AdapterConfig(
    reduction_factor=16,  # Giáº£m 16 láº§n parameters
    non_linearity="gelu_new",
    tasks=["sequential", "direct", "explanation"],  # CÃ¡c tasks
    use_single_adapter=False,  # Má»—i task cÃ³ adapter riÃªng
)

# Trong VIP5Reranker
reranker = VIP5Reranker(
    use_adapter=True,  # Enable adapters
    adapter_config=adapter_config,
    add_adapter_cross_attn=True,  # ThÃªm adapter cho cross-attention
    use_lm_head_adapter=True,  # Adapter cho LM head
)
```

## ğŸ“ˆ Performance Impact

Vá»›i adapters:
- **Parameters to train**: ~1-5% cá»§a full model
- **Memory usage**: Giáº£m ~80-90%
- **Training speed**: Nhanh hÆ¡n 2-5x
- **Inference speed**: Gáº§n nhÆ° khÃ´ng Ä‘á»•i (chá»‰ thÃªm má»™t vÃ i operations)

## ğŸ¯ Káº¿t luáº­n

Adapters trong VIP5 lÃ  má»™t **parameter-efficient fine-tuning technique** cho phÃ©p:

1. âœ… Fine-tune model vá»›i Ã­t parameters
2. âœ… Há»— trá»£ multi-task learning
3. âœ… Task-specific adaptation
4. âœ… Tiáº¿t kiá»‡m memory vÃ  computation
5. âœ… Dá»… dÃ ng scale vÃ  thÃªm tasks má»›i

**Adapters Ä‘Ã£ cÃ³ sáºµn trong project** vÃ  sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c sá»­ dá»¥ng náº¿u `use_adapter=True` trong config!

