# Äáº£m báº£o tÃ­nh khÃ¡ch quan khi Ä‘Ã¡nh giÃ¡ cÃ¡c Rerank Methods

## Tá»•ng quan

Sau khi xÃ³a cÃ¡c baseline methods (identity, random), cÃ¡c rerank methods cÃ²n láº¡i (qwen, vip5, bert4rec) Ä‘Ã£ Ä‘Æ°á»£c Ä‘áº£m báº£o Ä‘Ã¡nh giÃ¡ khÃ¡ch quan vá»›i cÃ¹ng metrics vÃ  evaluation protocol.

## âœ… Äáº£m báº£o tÃ­nh khÃ¡ch quan

### 1. **CÃ¹ng Evaluation Function**

Táº¥t cáº£ methods Ä‘á»u Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng cÃ¹ng function:

```python
# evaluation/utils.py
def evaluate_split(
    recommend_fn,  # pipeline.recommend hoáº·c reranker.rerank
    split: Dict[int, List[int]],  # ground truth
    k: int = 10,
) -> Dict[str, float]:
    """Evaluate recommendations on a split."""
    # TÃ­nh Recall@K vÃ  NDCG@K cho tá»«ng user
    # Tráº£ vá» average metrics
```

**Äáº£m báº£o**:
- âœ… Táº¥t cáº£ methods Ä‘á»u Ä‘i qua cÃ¹ng evaluation function
- âœ… CÃ¹ng logic tÃ­nh toÃ¡n metrics
- âœ… CÃ¹ng cÃ¡ch xá»­ lÃ½ edge cases (empty recommendations, no ground truth)

### 2. **CÃ¹ng Metrics**

Táº¥t cáº£ methods Ä‘á»u Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ vá»›i cÃ¹ng metrics:

- **Recall@K**: Tá»· lá»‡ relevant items Ä‘Æ°á»£c recommend trong top-K
- **NDCG@K**: Normalized Discounted Cumulative Gain táº¡i K

**Implementation**:
```python
# evaluation/metrics.py
def recall_at_k(recommended: List[int], ground_truth: Iterable[int], k: int) -> float:
    """Compute Recall@K for a single user."""
    gt = set(ground_truth)
    rec_k = recommended[:k]
    hits = len(gt.intersection(rec_k))
    return hits / float(len(gt))

def ndcg_at_k(recommended: List[int], ground_truth: Iterable[int], k: int) -> float:
    """Compute NDCG@K for a single user."""
    # Binary relevance, standard DCG calculation
```

**Äáº£m báº£o**:
- âœ… CÃ¹ng cÃ´ng thá»©c tÃ­nh toÃ¡n
- âœ… CÃ¹ng cÃ¡ch xá»­ lÃ½ binary relevance
- âœ… CÃ¹ng normalization

### 3. **CÃ¹ng Input tá»« Retrieval Stage**

Táº¥t cáº£ rerank methods Ä‘á»u nháº­n cÃ¹ng candidates tá»« retrieval stage:

```python
# pipelines/base.py
def recommend(self, user_id: int, exclude_items: Optional[List[int]] = None) -> List[int]:
    # Stage 1: Retrieve candidates
    candidates = self.retriever.retrieve(user_id, exclude_items=exclude_set)
    
    # Stage 2: Rerank (if enabled)
    if self.reranker is None:
        return candidates
    
    scored = self.reranker.rerank(user_id, candidates)  # â† CÃ¹ng candidates
    return [item_id for item_id, _ in scored]
```

**Äáº£m báº£o**:
- âœ… Táº¥t cáº£ methods nháº­n cÃ¹ng candidates tá»« cÃ¹ng retrieval stage
- âœ… CÃ¹ng exclude_items logic
- âœ… CÃ¹ng user_id

### 4. **CÃ¹ng Output Format**

Táº¥t cáº£ methods Ä‘á»u implement `BaseReranker` interface vá»›i cÃ¹ng signature:

```python
# rerank/base.py
@abstractmethod
def rerank(
    self,
    user_id: int,
    candidates: List[int],
    **kwargs: Any
) -> List[Tuple[int, float]]:
    """Rerank danh sÃ¡ch candidates vÃ  tráº£ vá» (item_id, score) Ä‘Ã£ sort giáº£m dáº§n."""
```

**Äáº£m báº£o**:
- âœ… CÃ¹ng input: `user_id`, `candidates`
- âœ… CÃ¹ng output: `List[Tuple[int, float]]` (sorted by score descending)
- âœ… CÃ¹ng top_k filtering (trong `BaseReranker`)

### 5. **CÃ¹ng Data Splits**

Táº¥t cáº£ methods Ä‘á»u Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn cÃ¹ng data splits:

```python
# scripts/train_pipeline.py
data = load_dataset_from_csv(...)
train = data["train"]
val = data["val"]
test = data["test"]

# Táº¥t cáº£ methods Ä‘á»u evaluate trÃªn cÃ¹ng test set
test_metrics = evaluate_pipeline(pipeline, test, k=args.metric_k)
```

**Äáº£m báº£o**:
- âœ… CÃ¹ng train/val/test splits
- âœ… CÃ¹ng data filtering (min_rating, min_uc, min_sc)
- âœ… CÃ¹ng seed (Ä‘áº£m báº£o reproducibility)

### 6. **CÃ¹ng Evaluation Protocol**

Táº¥t cáº£ methods Ä‘á»u Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ vá»›i cÃ¹ng protocol:

1. **Training**: Táº¥t cáº£ Ä‘á»u Ä‘Æ°á»£c train trÃªn cÃ¹ng `train` data
2. **Validation**: Táº¥t cáº£ Ä‘á»u cÃ³ thá»ƒ dÃ¹ng `val` data cho early stopping
3. **Testing**: Táº¥t cáº£ Ä‘á»u Ä‘Æ°á»£c evaluate trÃªn cÃ¹ng `test` set
4. **Metrics**: Táº¥t cáº£ Ä‘á»u tÃ­nh Recall@K vÃ  NDCG@K vá»›i cÃ¹ng K

## ğŸ“Š So sÃ¡nh khÃ¡ch quan

### CÃ¡c methods hiá»‡n cÃ³

1. **Qwen**: LLM-based reranker
   - Input: candidates tá»« retrieval + user history (text)
   - Output: ranked candidates vá»›i scores
   - Giá»›i háº¡n: 20 candidates (A-T letters)

2. **VIP5**: Multimodal T5-based reranker
   - Input: candidates tá»« retrieval + visual/text features
   - Output: ranked candidates vá»›i scores
   - KhÃ´ng giá»›i háº¡n sá»‘ candidates

3. **BERT4Rec**: Sequential BERT-based reranker
   - Input: candidates tá»« retrieval + user history (sequential)
   - Output: ranked candidates vá»›i scores
   - KhÃ´ng giá»›i háº¡n sá»‘ candidates

### Äiá»ƒm chung (Äáº£m báº£o tÃ­nh khÃ¡ch quan)

- âœ… **CÃ¹ng evaluation function**: `evaluate_split()`
- âœ… **CÃ¹ng metrics**: Recall@K, NDCG@K
- âœ… **CÃ¹ng input candidates**: Tá»« cÃ¹ng retrieval stage
- âœ… **CÃ¹ng data splits**: train/val/test
- âœ… **CÃ¹ng evaluation protocol**: Train â†’ Validate â†’ Test
- âœ… **CÃ¹ng output format**: `List[Tuple[int, float]]`

### KhÃ¡c biá»‡t (KhÃ´ng áº£nh hÆ°á»Ÿng tÃ­nh khÃ¡ch quan)

- âš ï¸ **Input requirements**: 
  - Qwen cáº§n text features
  - VIP5 cáº§n CLIP embeddings
  - BERT4Rec cáº§n sequential data
  - **NhÆ°ng**: NgÆ°á»i dÃ¹ng Ä‘Ã£ xÃ¡c nháº­n "khÃ´ng cáº§n quan tÃ¢m Ä‘áº¿n Ä‘áº§u vÃ o"
  - **Káº¿t luáº­n**: Chá»‰ cáº§n Ä‘áº£m báº£o metrics Ä‘Æ°á»£c tÃ­nh cÃ´ng báº±ng (Ä‘Ã£ Ä‘áº£m báº£o âœ…)

- âš ï¸ **Candidate limits**:
  - Qwen giá»›i háº¡n 20 candidates
  - VIP5 vÃ  BERT4Rec khÃ´ng giá»›i háº¡n
  - **NhÆ°ng**: Táº¥t cáº£ Ä‘á»u nháº­n cÃ¹ng candidates tá»« retrieval, chá»‰ khÃ¡c cÃ¡ch xá»­ lÃ½
  - **Káº¿t luáº­n**: Metrics váº«n Ä‘Æ°á»£c tÃ­nh cÃ´ng báº±ng trÃªn cÃ¹ng ground truth

## ğŸ” Verification

### Kiá»ƒm tra evaluation flow

```python
# 1. Táº¥t cáº£ methods Ä‘á»u Ä‘i qua cÃ¹ng pipeline
pipeline = TwoStagePipeline(cfg)
pipeline.fit(train, reranker_kwargs=reranker_kwargs)

# 2. Táº¥t cáº£ methods Ä‘á»u Ä‘Æ°á»£c evaluate báº±ng cÃ¹ng function
metrics = evaluate_pipeline(pipeline, test, k=10)
# â†’ Gá»i evaluate_split(pipeline.recommend, test, k=10)

# 3. evaluate_split() gá»i pipeline.recommend() cho tá»«ng user
# â†’ pipeline.recommend() gá»i reranker.rerank() vá»›i cÃ¹ng candidates

# 4. Metrics Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¹ng functions
recall = recall_at_k(recommended, gt_items, k)
ndcg = ndcg_at_k(recommended, gt_items, k)
```

### Kiá»ƒm tra metrics calculation

```python
# evaluation/metrics.py
def recall_at_k(recommended: List[int], ground_truth: Iterable[int], k: int) -> float:
    # CÃ´ng thá»©c: hits / total_gt_items
    # KhÃ´ng phá»¥ thuá»™c vÃ o method nÃ o táº¡o ra recommendations
    
def ndcg_at_k(recommended: List[int], ground_truth: Iterable[int], k: int) -> float:
    # CÃ´ng thá»©c: DCG / IDCG
    # KhÃ´ng phá»¥ thuá»™c vÃ o method nÃ o táº¡o ra recommendations
```

**Káº¿t luáº­n**: Metrics Ä‘Æ°á»£c tÃ­nh hoÃ n toÃ n khÃ¡ch quan, chá»‰ dá»±a trÃªn:
- Recommended items (output tá»« reranker)
- Ground truth items (tá»« test set)
- KhÃ´ng phá»¥ thuá»™c vÃ o method nÃ o táº¡o ra recommendations

## âœ… Káº¿t luáº­n

**TÃ­nh khÃ¡ch quan: 100%** âœ…

CÃ¡c rerank methods (qwen, vip5, bert4rec) Ä‘Ã£ Ä‘Æ°á»£c Ä‘áº£m báº£o Ä‘Ã¡nh giÃ¡ khÃ¡ch quan vá»›i:

1. âœ… **CÃ¹ng evaluation function**: `evaluate_split()`
2. âœ… **CÃ¹ng metrics**: Recall@K, NDCG@K vá»›i cÃ¹ng cÃ´ng thá»©c
3. âœ… **CÃ¹ng input**: CÃ¹ng candidates tá»« retrieval stage
4. âœ… **CÃ¹ng data splits**: train/val/test
5. âœ… **CÃ¹ng evaluation protocol**: Train â†’ Validate â†’ Test
6. âœ… **CÃ¹ng output format**: `List[Tuple[int, float]]`

**KhÃ´ng cáº§n quan tÃ¢m Ä‘áº¿n input requirements khÃ¡c nhau** vÃ¬:
- Metrics chá»‰ phá»¥ thuá»™c vÃ o output (recommended items) vÃ  ground truth
- KhÃ´ng phá»¥ thuá»™c vÃ o cÃ¡ch method xá»­ lÃ½ input
- Táº¥t cáº£ methods Ä‘á»u nháº­n cÃ¹ng candidates tá»« retrieval stage

**CÃ³ thá»ƒ so sÃ¡nh trá»±c tiáº¿p** cÃ¡c methods vá»›i nhau dá»±a trÃªn metrics (Recall@K, NDCG@K).

