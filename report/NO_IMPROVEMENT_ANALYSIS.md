# PhÃ¢n tÃ­ch: Táº¡i sao káº¿t quáº£ khÃ´ng cáº£i thiá»‡n sau 4 epochs?

## ğŸ“Š Váº¥n Ä‘á»

**ÄÃ£ thá»­ training vá»›i 4 epochs nhÆ°ng káº¿t quáº£ khÃ´ng cáº£i thiá»‡n** (Recall@20 váº«n ~0.4)

Äiá»u nÃ y cho tháº¥y cÃ³ váº¥n Ä‘á» khÃ¡c ngoÃ i epochs. Cáº§n kiá»ƒm tra:

---

## ğŸ” CÃ¡c NguyÃªn nhÃ¢n CÃ³ thá»ƒ

### **1. Training Loss khÃ´ng giáº£m** ğŸ”´

**Kiá»ƒm tra**:
- Training loss cÃ³ giáº£m khÃ´ng?
- Náº¿u loss khÃ´ng giáº£m â†’ model khÃ´ng há»c Ä‘Æ°á»£c gÃ¬

**Expected behavior**:
- Initial loss: ~3.9 (random vá»›i 50 candidates: -log(1/50) â‰ˆ 3.9)
- Sau 1 epoch: ~2.5-3.5 (náº¿u há»c Ä‘Æ°á»£c má»™t chÃºt)
- Sau 4 epochs: ~1.5-2.5 (náº¿u há»c tá»‘t)

**Náº¿u loss khÃ´ng giáº£m**:
- âŒ Learning rate quÃ¡ tháº¥p
- âŒ Model khÃ´ng Ä‘Æ°á»£c train (checkpoint khÃ´ng Ä‘Æ°á»£c save/load)
- âŒ Training data format sai
- âŒ Loss masking khÃ´ng Ä‘Ãºng

**CÃ¡ch kiá»ƒm tra**:
```python
# ThÃªm logging trong training
training_args = SFTConfig(
    ...
    logging_steps=10,  # âœ… Log má»—i 10 steps
    report_to="tensorboard",  # Hoáº·c "wandb"
)
```

---

### **2. Model khÃ´ng Predict Ä‘Ãºng Letter Tokens** ğŸ”´

**Váº¥n Ä‘á»**:
- Model cÃ³ thá»ƒ khÃ´ng tÃ¬m tháº¥y letter tokens (A, B, C, ...)
- Fallback vá» uniform distribution â†’ recall = random

**Code kiá»ƒm tra** (`rerank/models/llm.py:386-393`):
```python
# Debug: Check if we found letter tokens
if len(letter_tokens) < num_candidates:
    print(f"[WARNING] Only found {len(letter_tokens)}/{num_candidates} letter tokens!")
    # Náº¿u khÃ´ng tÃ¬m tháº¥y â†’ fallback to uniform
    if len(letter_tokens) == 0:
        return np.ones(num_candidates) / num_candidates  # âŒ Uniform!
```

**CÃ¡ch kiá»ƒm tra**:
1. **ThÃªm debug output**:
```python
# Trong predict_probs()
print(f"[DEBUG] Found {len(letter_tokens)}/{num_candidates} letter tokens")
print(f"[DEBUG] Letter tokens: {[l for _, l, _ in letter_tokens[:5]]}")
print(f"[DEBUG] Probabilities: {prob_array[:5]}")
```

2. **Kiá»ƒm tra tokenizer**:
```python
# Test tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
for letter in ["A", "B", "C", "D", "E"]:
    token_id = tokenizer.convert_tokens_to_ids(letter)
    print(f"Letter {letter}: token_id={token_id}, unk={tokenizer.unk_token_id}")
```

**Náº¿u khÃ´ng tÃ¬m tháº¥y letter tokens**:
- âŒ Tokenizer khÃ´ng há»— trá»£ single letter tokens
- âŒ Cáº§n dÃ¹ng strategy khÃ¡c (space prefix, encoding)

---

### **3. Model Predict Uniform Distribution** ğŸ”´

**Váº¥n Ä‘á»**:
- Model cÃ³ thá»ƒ predict gáº§n nhÆ° uniform distribution
- Táº¥t cáº£ candidates cÃ³ probability â‰ˆ 1/num_candidates
- â†’ Recall = random

**CÃ¡ch kiá»ƒm tra**:
```python
# Trong rerank()
probs = self.llm_model.predict_probs(prompt, num_candidates=len(candidates))
print(f"[DEBUG] Probabilities: {probs}")
print(f"[DEBUG] Max prob: {np.max(probs)}, Min prob: {np.min(probs)}")
print(f"[DEBUG] Std: {np.std(probs)}")

# Náº¿u std ráº¥t nhá» â†’ uniform distribution
if np.std(probs) < 0.01:
    print("[WARNING] Probabilities are nearly uniform!")
```

**NguyÃªn nhÃ¢n**:
- âŒ Model chÆ°a há»c Ä‘Æ°á»£c pattern (loss khÃ´ng giáº£m)
- âŒ Training data khÃ´ng Ä‘á»§ quality
- âŒ Model quÃ¡ nhá» (Qwen3-0.6B cÃ³ thá»ƒ khÃ´ng Ä‘á»§)

---

### **4. Evaluation Setup cÃ³ váº¥n Ä‘á»** ğŸŸ¡

**Váº¥n Ä‘á»**:
- GT items cÃ³ thá»ƒ khÃ´ng cÃ³ trong candidates
- Candidates cÃ³ thá»ƒ khÃ´ng Ä‘Æ°á»£c shuffle Ä‘Ãºng
- Evaluation cÃ³ thá»ƒ khÃ´ng Ä‘Ãºng

**Code kiá»ƒm tra** (`rerank/methods/qwen_reranker_unified.py:1333-1374`):
```python
def _evaluate_split(self, split: Dict[int, List[int]], k: int) -> float:
    recalls = []
    for user_id, gt_items in split.items():
        # ... get candidates ...
        
        # âœ… Äáº£m báº£o GT item cÃ³ trong candidates
        if not any(item in candidates for item in gt_items):
            candidates[0] = gt_items[0]  # âœ… Force GT vÃ o candidates
        
        # Rerank
        reranked = self.rerank(user_id, candidates)
        
        # Compute recall
        top_k_items = [item_id for item_id, _ in reranked[:k]]
        hits = len(set(top_k_items) & set(gt_items))
        recalls.append(hits / len(gt_items))
```

**CÃ¡ch kiá»ƒm tra**:
```python
# ThÃªm debug trong evaluation
print(f"[DEBUG] User {user_id}: GT={gt_items}, Candidates={candidates[:5]}")
print(f"[DEBUG] Reranked top-5: {[item_id for item_id, _ in reranked[:5]]}")
print(f"[DEBUG] Hits: {hits}, Recall: {hits / len(gt_items)}")
```

---

### **5. Training Data Quality** ğŸŸ¡

**Váº¥n Ä‘á»**:
- Training data cÃ³ thá»ƒ khÃ´ng Ä‘á»§ quality
- History cÃ³ thá»ƒ quÃ¡ ngáº¯n
- Candidates cÃ³ thá»ƒ khÃ´ng Ä‘a dáº¡ng

**CÃ¡ch kiá»ƒm tra**:
```python
# Kiá»ƒm tra training data
print(f"Training samples: {len(train_data_for_llm)}")
print(f"Sample prompt length: {len(train_data_for_llm[0]['messages'][1]['content'])}")
print(f"Sample target: {train_data_for_llm[0]['messages'][2]['content']}")

# Kiá»ƒm tra distribution cá»§a target letters
from collections import Counter
targets = [sample['messages'][2]['content'] for sample in train_data_for_llm]
target_counts = Counter(targets)
print(f"Target distribution: {target_counts}")
# Náº¿u quÃ¡ imbalanced â†’ cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng training
```

---

### **6. Model Size cÃ³ thá»ƒ quÃ¡ nhá»** ğŸŸ¡

**Váº¥n Ä‘á»**:
- Qwen3-0.6B cÃ³ thá»ƒ quÃ¡ nhá» cho task nÃ y
- LlamaRec dÃ¹ng Llama 2-7B (lá»›n hÆ¡n 10x)

**So sÃ¡nh**:
- **LlamaRec**: Llama 2-7B (7B parameters)
- **Project hiá»‡n táº¡i**: Qwen3-0.6B (0.6B parameters)

**Giáº£i phÃ¡p**:
- Thá»­ model lá»›n hÆ¡n: Qwen3-1.7B, Qwen3-4B
- Hoáº·c tÄƒng LoRA rank: r=16, alpha=32

---

### **7. Learning Rate cÃ³ thá»ƒ chÆ°a tá»‘i Æ°u** ğŸŸ¡

**Váº¥n Ä‘á»**:
- Learning rate 1e-4 cÃ³ thá»ƒ quÃ¡ tháº¥p hoáº·c quÃ¡ cao
- Cáº§n Ä‘iá»u chá»‰nh dá»±a trÃªn training loss

**CÃ¡ch kiá»ƒm tra**:
- Náº¿u loss khÃ´ng giáº£m â†’ tÄƒng learning rate (2e-4, 5e-4)
- Náº¿u loss oscillate â†’ giáº£m learning rate (5e-5)

---

## ğŸ”§ Debugging Steps

### **Step 1: Kiá»ƒm tra Training Loss**

```python
# ThÃªm vÃ o training
training_args = SFTConfig(
    ...
    logging_steps=1,  # Log má»—i step Ä‘á»ƒ debug
    report_to="none",  # Hoáº·c "tensorboard"
)

# Sau training, check logs
# Náº¿u loss khÃ´ng giáº£m â†’ cÃ³ váº¥n Ä‘á» vá»›i training
```

---

### **Step 2: Kiá»ƒm tra Letter Token Extraction**

```python
# ThÃªm debug trong predict_probs()
def predict_probs(self, prompt, num_candidates=None):
    # ... existing code ...
    
    # âœ… DEBUG: Check letter tokens
    print(f"[DEBUG] Looking for {num_candidates} letter tokens")
    print(f"[DEBUG] Found {len(letter_tokens)} letter tokens")
    if len(letter_tokens) < num_candidates:
        print(f"[WARNING] Missing {num_candidates - len(letter_tokens)} letter tokens!")
        print(f"[DEBUG] Found letters: {[l for _, l, _ in letter_tokens]}")
    
    # âœ… DEBUG: Check probabilities
    print(f"[DEBUG] Probabilities: max={np.max(prob_array):.4f}, min={np.min(prob_array):.4f}, std={np.std(prob_array):.4f}")
    
    return prob_array
```

---

### **Step 3: Kiá»ƒm tra Model Prediction**

```python
# Test prediction trÃªn má»™t sample
prompt = """You are a recommendation ranking assistant.

Choose exactly ONE item the user is most likely to interact with next.

User history:
- item1
- item2

Candidate items:
A. candidate1
B. candidate2
C. candidate3

Answer with only one letter (A-C)."""

probs = model.predict_probs(prompt, num_candidates=3)
print(f"Probabilities: {probs}")
print(f"Predicted letter: {LETTERS[np.argmax(probs)]}")

# Náº¿u probabilities gáº§n uniform â†’ model chÆ°a há»c Ä‘Æ°á»£c gÃ¬
```

---

### **Step 4: Kiá»ƒm tra Evaluation**

```python
# ThÃªm debug trong _evaluate_split()
def _evaluate_split(self, split: Dict[int, List[int]], k: int) -> float:
    recalls = []
    for i, (user_id, gt_items) in enumerate(split.items()):
        # ... existing code ...
        
        # âœ… DEBUG: Check first few samples
        if i < 3:
            print(f"\n[DEBUG] User {user_id}:")
            print(f"  GT items: {gt_items}")
            print(f"  Candidates: {candidates[:10]}")
            print(f"  GT in candidates: {any(item in candidates for item in gt_items)}")
            
            reranked = self.rerank(user_id, candidates)
            print(f"  Reranked top-5: {[item_id for item_id, _ in reranked[:5]]}")
            
            top_k_items = [item_id for item_id, _ in reranked[:k]]
            hits = len(set(top_k_items) & set(gt_items))
            recall = hits / len(gt_items)
            print(f"  Hits: {hits}, Recall@{k}: {recall:.4f}")
    
    return float(np.mean(recalls))
```

---

## ğŸ¯ Action Plan

### **Priority 1: Debug Training Loss** ğŸ”´

1. **ThÃªm logging**:
```python
training_args = SFTConfig(
    logging_steps=1,  # Log má»—i step
    ...
)
```

2. **Kiá»ƒm tra loss cÃ³ giáº£m khÃ´ng**:
- Náº¿u khÃ´ng giáº£m â†’ cÃ³ váº¥n Ä‘á» vá»›i training
- Náº¿u giáº£m nhÆ°ng cháº­m â†’ tÄƒng learning rate

---

### **Priority 2: Debug Letter Token Extraction** ğŸ”´

1. **ThÃªm debug output** trong `predict_probs()`
2. **Kiá»ƒm tra xem letter tokens cÃ³ Ä‘Æ°á»£c tÃ¬m tháº¥y khÃ´ng**
3. **Náº¿u khÃ´ng tÃ¬m tháº¥y â†’ sá»­a token extraction**

---

### **Priority 3: Debug Model Prediction** ğŸŸ¡

1. **Test prediction trÃªn sample cá»¥ thá»ƒ**
2. **Kiá»ƒm tra probabilities cÃ³ uniform khÃ´ng**
3. **Náº¿u uniform â†’ model chÆ°a há»c Ä‘Æ°á»£c gÃ¬**

---

### **Priority 4: Kiá»ƒm tra Evaluation** ğŸŸ¡

1. **ThÃªm debug trong evaluation**
2. **Kiá»ƒm tra GT items cÃ³ trong candidates khÃ´ng**
3. **Kiá»ƒm tra reranking cÃ³ Ä‘Ãºng khÃ´ng**

---

## ğŸ“Š Expected Results sau khi Debug

### **Náº¿u Training Loss giáº£m nhÆ°ng Recall khÃ´ng cáº£i thiá»‡n**:

- âŒ Váº¥n Ä‘á» vá»›i letter token extraction
- âŒ Váº¥n Ä‘á» vá»›i evaluation setup
- âŒ Model predict uniform distribution

### **Náº¿u Training Loss khÃ´ng giáº£m**:

- âŒ Learning rate quÃ¡ tháº¥p/cao
- âŒ Training data format sai
- âŒ Model khÃ´ng Ä‘Æ°á»£c train Ä‘Ãºng

### **Náº¿u Letter Tokens khÃ´ng Ä‘Æ°á»£c tÃ¬m tháº¥y**:

- âŒ Tokenizer khÃ´ng há»— trá»£ single letters
- âŒ Cáº§n sá»­a token extraction strategy

---

## âœ… TÃ³m táº¯t

**Váº¥n Ä‘á»**: Training 4 epochs nhÆ°ng káº¿t quáº£ khÃ´ng cáº£i thiá»‡n

**CÃ¡c nguyÃªn nhÃ¢n cÃ³ thá»ƒ**:
1. ğŸ”´ Training loss khÃ´ng giáº£m
2. ğŸ”´ Letter tokens khÃ´ng Ä‘Æ°á»£c tÃ¬m tháº¥y
3. ğŸ”´ Model predict uniform distribution
4. ğŸŸ¡ Evaluation setup cÃ³ váº¥n Ä‘á»
5. ğŸŸ¡ Training data quality
6. ğŸŸ¡ Model size quÃ¡ nhá»
7. ğŸŸ¡ Learning rate chÆ°a tá»‘i Æ°u

**Next Steps**:
1. âœ… Debug training loss
2. âœ… Debug letter token extraction
3. âœ… Debug model prediction
4. âœ… Debug evaluation setup

