# So sÃ¡nh Logic Training vÃ  Rerank vá»›i LlamaRec

## ğŸ“Š Tá»•ng quan

So sÃ¡nh chi tiáº¿t logic training vÃ  rerank cá»§a project hiá»‡n táº¡i vá»›i LlamaRec Ä‘á»ƒ tÃ¬m váº¥n Ä‘á».

---

## ğŸ” 1. Training Data Preparation

### **LlamaRec Approach**

```python
# LlamaRec training sample format
{
    "messages": [
        {
            "role": "user",
            "content": """
Candidate items:
A. candidate1
B. candidate2
C. candidate3
D. candidate4

Answer with only one letter (A-D).
"""
        },
        {
            "role": "assistant",
            "content": "D"  # âœ… Letter index
        }
    ]
}
```

**Key Points**:
- âœ… DÃ¹ng **letter labels** (A, B, C, D, ...)
- âœ… Target = letter cá»§a ground-truth item
- âœ… Prompt format vá»›i letter labels

---

### **Project hiá»‡n táº¡i - Kiá»ƒm tra Code**

#### **1. Text-only mode** (`rerank/methods/qwen_reranker_unified.py`)

**Code hiá»‡n táº¡i** (line 300-389):
```python
# For caption/semantic_summary modes
if self.mode in ["caption", "semantic_summary"]:
    train_samples = self._prepare_training_samples(train_data)
    # ...
    target = LETTERS[target_idx]  # âœ… Letter index (A, B, C, ...)
    train_data_for_llm.append({
        "messages": [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": target}  # âœ… Letter
        ]
    })
```

**Status**: âœ… **ÄÃšNG** - DÃ¹ng letter labels

**NhÆ°ng**: Cáº§n kiá»ƒm tra `text_only` mode cÃ³ dÃ¹ng `train_data_for_llm` tá»« kwargs khÃ´ng?

---

#### **2. Training Sample Preparation** (`_prepare_training_samples`)

**Code hiá»‡n táº¡i** (`rerank/methods/qwen_reranker_unified.py:652-738`):
```python
def _prepare_training_samples(
    self,
    train_data: Dict[int, List[int]],
) -> List[Dict]:
    """Prepare training samples for multimodal modes."""
    samples = []
    
    for user_id, items in train_data.items():
        if len(items) < 2:
            continue
        
        # Randomly select split point
        split_point = random.randint(1, len(items) - 1)
        history = items[:split_point]
        target_item = items[split_point]
        
        # Get candidates: target + negatives
        all_items = set()
        for user_items in train_data.values():
            all_items.update(user_items)
        
        # Exclude history and target
        candidate_pool = [item for item in all_items 
                         if item not in history and item != target_item]
        
        # Sample negatives
        num_negatives = min(self.top_k - 1, len(candidate_pool))
        negatives = random.sample(candidate_pool, num_negatives)
        
        # Candidates = [target] + negatives (shuffled)
        candidates = [target_item] + negatives
        random.shuffle(candidates)
        
        # Find target index
        target_idx = candidates.index(target_item)
        
        samples.append({
            "user_id": user_id,
            "history": history,
            "candidates": candidates,
            "target_item": target_item,
            "target_idx": target_idx,  # âœ… 0-indexed
        })
    
    return samples
```

**PhÃ¢n tÃ­ch**:
- âœ… **ÄÃšNG**: Random split point
- âœ… **ÄÃšNG**: History = items[:split_point]
- âœ… **ÄÃšNG**: Target = items[split_point]
- âœ… **ÄÃšNG**: Candidates = [target] + negatives (shuffled)
- âœ… **ÄÃšNG**: Target index = 0-indexed

**Status**: âœ… **Logic Ä‘Ãºng vá»›i LlamaRec**

---

## ğŸ” 2. Training Process

### **LlamaRec Approach**

```python
# 1. Load model vá»›i LoRA
model = get_peft_model(model, LoraConfig(...))

# 2. Prepare dataset vá»›i letter labels
train_dataset = prepare_dataset(train_data)

# 3. Train vá»›i next-token prediction
trainer = Trainer(...)
trainer.train()

# 4. Mask prompt tokens (chá»‰ tÃ­nh loss á»Ÿ response)
# LlamaRec sá»­ dá»¥ng mask Ä‘á»ƒ chá»‰ tÃ­nh loss á»Ÿ pháº§n assistant response
```

---

### **Project hiá»‡n táº¡i**

**Code** (`rerank/models/llm.py:164-263`):
```python
def train(self, batch_size=None):
    # 1. Load dataset
    hf_train_dataset = Dataset.from_list(self.train_data)
    
    # 2. Format messages to text using chat template
    hf_train_dataset = hf_train_dataset.map(
        formatting_prompts_func,
        batched=True,
    )
    
    # 3. Setup training args
    training_args = SFTConfig(
        learning_rate=learning_rate,  # âœ… From config
        num_train_epochs=num_epochs,   # âœ… From config
        ...
    )
    
    # 4. Create trainer
    trainer = SFTTrainer(...)
    
    # 5. âœ… Mask prompt tokens (chá»‰ tÃ­nh loss á»Ÿ response)
    trainer = train_on_responses_only(
        trainer,
        instruction_part="<|im_start|>user\n",
        response_part="<|im_start|>assistant\n",
    )
    
    # 6. Train
    trainer.train()
```

**PhÃ¢n tÃ­ch**:
- âœ… **ÄÃšNG**: Next-token prediction
- âœ… **ÄÃšNG**: Mask prompt tokens vá»›i `train_on_responses_only`
- âœ… **ÄÃšNG**: Chat template format
- âœ… **ÄÃšNG**: LoRA fine-tuning

**Status**: âœ… **Logic Ä‘Ãºng vá»›i LlamaRec**

---

## ğŸ” 3. Rerank/Inference Process

### **LlamaRec Approach**

```python
# 1. Build prompt vá»›i letter labels
prompt = build_prompt(user_history, candidates)  # A, B, C, ...

# 2. Forward pass
outputs = model(**inputs)
logits = outputs.logits[:, -1]  # [vocab_size]

# 3. Extract letter token IDs
letter_tokens = []
for letter in ["A", "B", "C", ...]:
    token_id = tokenizer.convert_tokens_to_ids(letter)
    letter_tokens.append((idx, letter, token_id))

# 4. Extract probabilities
token_ids = [tid for _, _, tid in letter_tokens]
probs = F.softmax(logits[:, token_ids], dim=-1)

# 5. Map to candidates
prob_array = np.zeros(num_candidates)
for idx, (cand_idx, letter, token_id) in enumerate(letter_tokens):
    prob_array[cand_idx] = probs[0, idx].item()

# 6. Rank by probability
ranked = sorted(zip(candidates, prob_array), key=lambda x: x[1], reverse=True)
```

---

### **Project hiá»‡n táº¡i**

**Code** (`rerank/models/llm.py:290-414`):
```python
def predict_probs(self, prompt, num_candidates=None):
    # 1. Convert to chat template format
    messages = [{"role": "user", "content": prompt}]
    text = self.tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # 2. Tokenize
    inputs = self.tokenizer(text, return_tensors="pt", ...).to(self.model.device)
    
    # 3. Forward pass
    with torch.no_grad():
        outputs = self.model(**inputs)
    
    # 4. Extract logits
    logits = outputs.logits[:, -1]  # [vocab_size] âœ… ÄÃšNG
    
    # 5. Extract letter token IDs
    letter_tokens = []
    for i in range(num_candidates):
        letter = LETTERS[i]  # âœ… Letter (A, B, C, ...)
        
        # Strategy 1: Direct letter token
        token_id = self.tokenizer.convert_tokens_to_ids(letter)
        if token_id != self.tokenizer.unk_token_id:
            letter_tokens.append((i, letter, token_id))
            continue
        
        # Strategy 2: With space prefix
        token_id = self.tokenizer.convert_tokens_to_ids(" " + letter)
        if token_id != self.tokenizer.unk_token_id:
            letter_tokens.append((i, letter, token_id))
            continue
        
        # Strategy 3: Encoding
        encoded = self.tokenizer.encode(letter, add_special_tokens=False)
        if len(encoded) > 0:
            letter_tokens.append((i, letter, encoded[0]))
    
    # 6. Extract probabilities
    token_ids = [tid for _, _, tid in letter_tokens]
    probs = F.softmax(logits[:, token_ids], dim=-1)  # âœ… ÄÃšNG
    
    # 7. Map to candidates
    prob_array = np.zeros(num_candidates)
    for idx, (cand_idx, letter, token_id) in enumerate(letter_tokens):
        if cand_idx < num_candidates:
            prob_array[cand_idx] = probs[0, idx].item()
    
    # 8. Normalize
    if prob_array.sum() > 0:
        prob_array = prob_array / prob_array.sum()
    
    return prob_array
```

**PhÃ¢n tÃ­ch**:
- âœ… **ÄÃšNG**: Extract logits cá»§a token cuá»‘i cÃ¹ng
- âœ… **ÄÃšNG**: Extract letter token IDs (vá»›i fallback strategies)
- âœ… **ÄÃšNG**: Softmax trÃªn letter tokens
- âœ… **ÄÃšNG**: Map vá» candidate indices
- âœ… **ÄÃšNG**: Normalize probabilities

**Status**: âœ… **Logic Ä‘Ãºng vá»›i LlamaRec**

---

## ğŸ” 4. Prompt Format

### **LlamaRec**

```
You are a recommendation ranking assistant.

Choose exactly ONE item the user is most likely to interact with next.

User history:
- item1
- item2

Candidate items:
A. candidate1
B. candidate2
C. candidate3

Answer with only one letter (A-C).
```

---

### **Project hiá»‡n táº¡i**

**Code** (`rerank/models/llm.py:19-69`):
```python
def build_prompt_from_candidates(user_history, candidate_ids, item_id2text, max_candidates=None):
    # ...
    # Use letters (A-Z, a-z) for up to 52 candidates (LlamaRec style)
    cand_text = "\n".join(
        [f"{LETTERS[i]}. {c}" for i, c in enumerate(candidates)]  # âœ… Letter labels
    )
    
    # Answer format with letters
    if num_candidates <= 26:
        answer_format = f"Answer with only one letter (A-{LETTERS[num_candidates-1]})."  # âœ…
    else:
        answer_format = f"Answer with only one letter (A-Z, a-{LETTERS[num_candidates-1]})."  # âœ…
    
    prompt = f"""
You are a recommendation ranking assistant.

Choose exactly ONE item the user is most likely to interact with next.

User history:
{history_text}

Candidate items:
{cand_text}

{answer_format}
"""
    return prompt
```

**PhÃ¢n tÃ­ch**:
- âœ… **ÄÃšNG**: DÃ¹ng letter labels (A, B, C, ...)
- âœ… **ÄÃšNG**: Answer format vá»›i letters
- âœ… **ÄÃšNG**: Format giá»‘ng LlamaRec

**Status**: âœ… **Logic Ä‘Ãºng vá»›i LlamaRec**

---

## âš ï¸ Váº¥n Ä‘á» Logic PhÃ¡t hiá»‡n

### **1. Text-only mode training data** âš ï¸

**Váº¥n Ä‘á»**: 
- Code hiá»‡n táº¡i cÃ³ 2 paths cho training:
  1. `train_data_for_llm` tá»« kwargs (pre-prepared)
  2. Tá»± prepare tá»« `_prepare_training_samples` (cho caption/semantic_summary)

**Kiá»ƒm tra**: 
- Náº¿u `text_only` mode dÃ¹ng `train_data_for_llm` tá»« kwargs, cáº§n Ä‘áº£m báº£o format Ä‘Ãºng
- Náº¿u khÃ´ng cÃ³ `train_data_for_llm`, code sáº½ khÃ´ng train (chá»‰ load model)

**Code** (`rerank/methods/qwen_reranker_unified.py:296-403`):
```python
train_data_for_llm = kwargs.get("train_data_for_llm")

if train_data_for_llm is not None:
    # Train vá»›i pre-prepared data
    self.llm_model = LLMModel(train_data=train_data_for_llm, ...)
    self.llm_model.train(...)
else:
    # Chá»‰ load model, khÃ´ng train
    self.llm_model = LLMModel(train_data=None, ...)
    self.llm_model.load_model(...)
```

**Váº¥n Ä‘á» tiá»m áº©n**:
- Náº¿u `train_data_for_llm` khÃ´ng Ä‘Æ°á»£c cung cáº¥p cho `text_only` mode â†’ model khÃ´ng Ä‘Æ°á»£c train
- Cáº§n kiá»ƒm tra xem `train_data_for_llm` cÃ³ Ä‘Æ°á»£c prepare Ä‘Ãºng format khÃ´ng

---

### **2. Chat template format consistency** âš ï¸

**Váº¥n Ä‘á»**:
- Training: DÃ¹ng chat template format (`apply_chat_template`)
- Inference: CÅ©ng dÃ¹ng chat template format (`apply_chat_template`)

**PhÃ¢n tÃ­ch**:
- âœ… **ÄÃšNG**: Consistency giá»¯a training vÃ  inference
- âœ… **ÄÃšNG**: DÃ¹ng `add_generation_prompt=True` cho inference

**Status**: âœ… **Logic Ä‘Ãºng**

---

### **3. Letter token extraction fallback** âœ…

**Code** (`rerank/models/llm.py:363-384`):
```python
# Strategy 1: Direct letter token
token_id = self.tokenizer.convert_tokens_to_ids(letter)

# Strategy 2: With space prefix
token_id = self.tokenizer.convert_tokens_to_ids(" " + letter)

# Strategy 3: Encoding
encoded = self.tokenizer.encode(letter, add_special_tokens=False)
```

**PhÃ¢n tÃ­ch**:
- âœ… **ÄÃšNG**: CÃ³ fallback strategies Ä‘á»ƒ handle different tokenizers
- âœ… **ÄÃšNG**: LlamaRec cÃ³ thá»ƒ khÃ´ng cÃ³ fallback nÃ y, nhÆ°ng Ä‘Ã¢y lÃ  improvement

**Status**: âœ… **Logic Ä‘Ãºng (tháº­m chÃ­ tá»‘t hÆ¡n LlamaRec)**

---

## ğŸ“Š TÃ³m táº¯t So sÃ¡nh

| Aspect | LlamaRec | Project hiá»‡n táº¡i | Status |
|--------|----------|------------------|--------|
| **Training Data Prep** | Letter labels | âœ… Letter labels | âœ… ÄÃšNG |
| **Training Objective** | Next-token prediction | âœ… Next-token prediction | âœ… ÄÃšNG |
| **Loss Masking** | Mask prompt tokens | âœ… `train_on_responses_only` | âœ… ÄÃšNG |
| **Prompt Format** | Letter labels | âœ… Letter labels | âœ… ÄÃšNG |
| **Logits Extraction** | `logits[:, -1]` | âœ… `logits[:, -1]` | âœ… ÄÃšNG |
| **Token Extraction** | Letter tokens | âœ… Letter tokens (vá»›i fallback) | âœ… ÄÃšNG |
| **Probability Mapping** | Map letters â†’ candidates | âœ… Map letters â†’ candidates | âœ… ÄÃšNG |
| **Reranking** | Sort by probability | âœ… Sort by probability | âœ… ÄÃšNG |

---

## âš ï¸ Váº¥n Ä‘á» Logic Tiá»m áº©n

### **1. Text-only mode training data source** ğŸ”´

**Váº¥n Ä‘á»**:
- `text_only` mode phá»¥ thuá»™c vÃ o `train_data_for_llm` tá»« kwargs
- Náº¿u khÃ´ng cÃ³ â†’ model khÃ´ng Ä‘Æ°á»£c train

**Giáº£i phÃ¡p**:
```python
# ThÃªm auto-prepare cho text_only mode náº¿u khÃ´ng cÃ³ train_data_for_llm
if self.mode == "text_only" and train_data_for_llm is None:
    train_samples = self._prepare_training_samples(train_data)
    # Convert to LLM training format vá»›i letter labels
    train_data_for_llm = []
    for sample in train_samples:
        # ... build prompt vÃ  target vá»›i letter labels ...
```

---

### **2. Epochs quÃ¡ Ã­t** ğŸ”´

**Váº¥n Ä‘á»**:
- `rerank_epochs = 1` (default) â†’ quÃ¡ Ã­t Ä‘á»ƒ model há»c Ä‘Æ°á»£c pattern

**Giáº£i phÃ¡p**:
```python
# config.py
parser.add_argument('--rerank_epochs', type=int, default=5,  # âœ… TÄƒng lÃªn 5
```

---

### **3. LoRA config cÃ³ thá»ƒ chÆ°a tá»‘i Æ°u** ğŸŸ¡

**Váº¥n Ä‘á»**:
- `r=8, alpha=16` cÃ³ thá»ƒ nhá» cho better performance

**Giáº£i phÃ¡p**:
```python
# Thá»­ tÄƒng lÃªn r=16, alpha=32
```

---

## âœ… Káº¿t luáº­n

### **Logic Training vÃ  Rerank**: âœ… **ÄÃšNG vá»›i LlamaRec**

**Táº¥t cáº£ cÃ¡c aspects chÃ­nh Ä‘á»u Ä‘Ãºng**:
- âœ… Training data preparation vá»›i letter labels
- âœ… Training process vá»›i next-token prediction vÃ  loss masking
- âœ… Rerank process vá»›i verbalizer approach
- âœ… Prompt format vá»›i letter labels

### **Váº¥n Ä‘á» chÃ­nh**: 

1. **Epochs quÃ¡ Ã­t** (1 epoch) â†’ Model chÆ°a há»c Ä‘Æ°á»£c pattern
2. **Text-only mode** cÃ³ thá»ƒ khÃ´ng train náº¿u khÃ´ng cÃ³ `train_data_for_llm`

### **Recommendation**:

1. âœ… **TÄƒng epochs lÃªn 5-10**
2. âœ… **Kiá»ƒm tra text_only mode cÃ³ train khÃ´ng**
3. âœ… **Optional: TÄƒng LoRA rank náº¿u cáº§n**

**Logic code Ä‘Ã£ Ä‘Ãºng, váº¥n Ä‘á» lÃ  hyperparameters (epochs quÃ¡ Ã­t)!**

