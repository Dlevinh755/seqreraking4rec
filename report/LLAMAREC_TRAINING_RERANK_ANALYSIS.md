# Ph√¢n t√≠ch C√°ch LlamaRec Train, L·∫•y Logits v√† Rerank

## üìö T·ªïng quan LlamaRec

**Repository**: [LlamaRec](https://github.com/Yueeeeeeee/LlamaRec)

**Paper**: "LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking" (PGAI@CIKM 2023)

**Approach**: 
- **Stage 1**: Sequential recommender (LRURec) ƒë·ªÉ retrieve candidates
- **Stage 2**: LLM (Llama 2) ƒë·ªÉ rank candidates
- **Method**: Verbalizer-based approach - transform output logits th√†nh probability distributions

---

## üîç 1. Training Process

### **LlamaRec Training Approach**

Theo paper v√† README:

#### **Training Objective**:
- **Next-token prediction** (gi·ªëng LLM chu·∫©n)
- **Label = letter index** c·ªßa ground-truth item (A, B, C, D, ...)
- **Loss function**: Cross-entropy loss tr√™n token label
- **Ch·ªâ t√≠nh loss ·ªü ph·∫ßn Response** (token index + EOS)

#### **Training Data Format**:

```python
# LlamaRec training sample format
{
    "messages": [
        {
            "role": "user",
            "content": """
You are a recommendation ranking assistant.

Choose exactly ONE item the user is most likely to interact with next.

User history:
- item1
- item2
- item3

Candidate items:
A. candidate1
B. candidate2
C. candidate3
D. candidate4

Answer with only one letter (A-D).
"""
        },
        {
            "role": "assistant",
            "content": "D"  # ‚úÖ Letter index (not number)
        }
    ]
}
```

#### **Training Process**:

```python
# Pseudocode t·ª´ LlamaRec
# 1. Load base model (Llama 2)
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

# 2. Setup LoRA (parameter-efficient fine-tuning)
model = get_peft_model(model, LoraConfig(...))

# 3. Prepare training data v·ªõi letter labels
train_dataset = prepare_dataset(train_data)  # Format v·ªõi letter labels (A, B, C, ...)

# 4. Train v·ªõi next-token prediction
trainer = Trainer(
    model=model,
    train_dataset=train_dataset,
    args=training_args,
)

# 5. Mask prompt tokens (ch·ªâ t√≠nh loss ·ªü response)
# LlamaRec s·ª≠ d·ª•ng mask ƒë·ªÉ ch·ªâ t√≠nh loss ·ªü ph·∫ßn assistant response
trainer.train()
```

**Key Points**:
- ‚úÖ D√πng **letter labels** (A, B, C, ...) thay v√¨ numbers (1, 2, 3, ...)
- ‚úÖ **Mask prompt tokens** - ch·ªâ t√≠nh loss ·ªü response
- ‚úÖ **Next-token prediction** - model predict letter token

---

## üîç 2. Logits Extraction (Verbalizer Approach)

### **LlamaRec Verbalizer Approach**

Theo paper, LlamaRec s·ª≠ d·ª•ng **verbalizer-based approach**:

> "Instead of generating next-item titles, we adopt a verbalizer-based approach that transforms output logits into probability distributions over the candidate items."

#### **Process**:

```python
# LlamaRec logits extraction (pseudocode)
def extract_logits_and_rerank(model, tokenizer, prompt, candidates):
    """
    Extract logits using verbalizer approach.
    
    Args:
        model: Fine-tuned Llama 2 model
        tokenizer: Tokenizer
        prompt: Input prompt with candidates labeled A, B, C, ...
        candidates: List of candidate items
    
    Returns:
        probabilities: [num_candidates] - probability distribution
    """
    # 1. Tokenize prompt
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # 2. Forward pass
    with torch.no_grad():
        outputs = model(**inputs)
    
    # 3. Extract logits c·ªßa token cu·ªëi c√πng (next token prediction)
    logits = outputs.logits[:, -1]  # [vocab_size]
    
    # 4. Extract token IDs cho letters (A, B, C, ...)
    letter_tokens = []
    for i, letter in enumerate(["A", "B", "C", "D", ...]):  # Up to num_candidates
        token_id = tokenizer.convert_tokens_to_ids(letter)
        if token_id != tokenizer.unk_token_id:
            letter_tokens.append((i, letter, token_id))
    
    # 5. Extract probabilities cho letter tokens
    token_ids = [tid for _, _, tid in letter_tokens]
    probs = F.softmax(logits[:, token_ids], dim=-1)  # [1, num_letters]
    
    # 6. Map v·ªÅ candidate indices
    prob_array = np.zeros(len(candidates))
    for idx, (cand_idx, letter, token_id) in enumerate(letter_tokens):
        if cand_idx < len(candidates):
            prob_array[cand_idx] = probs[0, idx].item()
    
    # 7. Normalize
    if prob_array.sum() > 0:
        prob_array = prob_array / prob_array.sum()
    else:
        prob_array = np.ones(len(candidates)) / len(candidates)
    
    return prob_array
```

**Key Points**:
- ‚úÖ **Next-token prediction**: `logits = outputs.logits[:, -1]`
- ‚úÖ **Letter tokens**: Extract token IDs cho letters (A, B, C, ...)
- ‚úÖ **Softmax**: Convert logits ‚Üí probabilities
- ‚úÖ **Map to candidates**: Map letter probabilities ‚Üí candidate indices

---

## üîç 3. Reranking Process

### **LlamaRec Reranking**

#### **Two-Stage Process**:

```python
# Stage 1: Retrieval (LRURec)
def retrieve_candidates(user_history, retriever_model, top_k=50):
    """
    Retrieve top-K candidates using sequential recommender.
    """
    candidates = retriever_model.retrieve(user_history, top_k=top_k)
    return candidates

# Stage 2: Ranking (Llama 2)
def rerank_candidates(user_history, candidates, ranker_model, tokenizer):
    """
    Rerank candidates using LLM.
    """
    # 1. Build prompt v·ªõi candidates labeled A, B, C, ...
    prompt = build_prompt(user_history, candidates)
    
    # 2. Extract probabilities using verbalizer approach
    probs = extract_logits_and_rerank(ranker_model, tokenizer, prompt, candidates)
    
    # 3. Sort candidates by probability
    ranked_indices = np.argsort(probs)[::-1]  # Descending order
    ranked_candidates = [candidates[i] for i in ranked_indices]
    ranked_scores = [probs[i] for i in ranked_indices]
    
    return list(zip(ranked_candidates, ranked_scores))
```

#### **Prompt Format**:

```
You are a recommendation ranking assistant.

Choose exactly ONE item the user is most likely to interact with next.

User history:
- item1
- item2
- item3

Candidate items:
A. candidate1
B. candidate2
C. candidate3
D. candidate4
...

Answer with only one letter (A-{last_letter}).
```

**Key Points**:
- ‚úÖ **Two-stage**: Retrieval ‚Üí Ranking
- ‚úÖ **Letter labels**: Candidates labeled A, B, C, ...
- ‚úÖ **Single token output**: Model ch·ªâ predict m·ªôt letter
- ‚úÖ **Probability distribution**: Transform logits ‚Üí probabilities ‚Üí ranking

---

## üìä So s√°nh v·ªõi Project hi·ªán t·∫°i

### **1. Training**

| Aspect | LlamaRec | Project hi·ªán t·∫°i |
|--------|----------|------------------|
| **Base Model** | Llama 2-7B | Qwen3-0.6B, Qwen3-2BVL |
| **Fine-tuning** | LoRA | LoRA (r=8, alpha=16) |
| **Label Format** | ‚úÖ Letters (A, B, C, ...) | ‚úÖ Letters (A, B, C, ...) - **ƒê√£ s·ª≠a** |
| **Loss Masking** | ‚úÖ Mask prompt tokens | ‚úÖ `train_on_responses_only` |
| **Training Objective** | ‚úÖ Next-token prediction | ‚úÖ Next-token prediction |

**Status**: ‚úÖ **ƒê√£ align v·ªõi LlamaRec** (sau khi s·ª≠a sang letter labels)

---

### **2. Logits Extraction**

| Aspect | LlamaRec | Project hi·ªán t·∫°i |
|--------|----------|------------------|
| **Method** | ‚úÖ Verbalizer approach | ‚úÖ Verbalizer approach |
| **Logits Source** | ‚úÖ `logits[:, -1]` | ‚úÖ `logits[:, -1]` |
| **Token Type** | ‚úÖ Letter tokens (A, B, C, ...) | ‚úÖ Letter tokens (A, B, C, ...) |
| **Extraction** | ‚úÖ Extract letter token IDs | ‚úÖ Extract letter token IDs |
| **Softmax** | ‚úÖ Apply on letter tokens | ‚úÖ Apply on letter tokens |
| **Mapping** | ‚úÖ Map letters ‚Üí candidates | ‚úÖ Map letters ‚Üí candidates |

**Status**: ‚úÖ **Gi·ªëng LlamaRec**

**Code hi·ªán t·∫°i** (`rerank/models/llm.py:363-410`):
```python
# Get token IDs for letters A-Z, a-z (LlamaRec style)
letter_tokens = []
for i in range(num_candidates):
    letter = LETTERS[i]  # "A", "B", "C", ...
    
    # Strategy 1: Try direct letter token
    token_id = self.tokenizer.convert_tokens_to_ids(letter)
    if token_id != self.tokenizer.unk_token_id:
        letter_tokens.append((i, letter, token_id))
        continue
    
    # Strategy 2: Try with space prefix
    token_id = self.tokenizer.convert_tokens_to_ids(" " + letter)
    if token_id != self.tokenizer.unk_token_id:
        letter_tokens.append((i, letter, token_id))
        continue
    
    # Strategy 3: Try encoding and taking first token
    encoded = self.tokenizer.encode(letter, add_special_tokens=False)
    if len(encoded) > 0 and encoded[0] != self.tokenizer.unk_token_id:
        letter_tokens.append((i, letter, encoded[0]))
        continue

# Extract probabilities for letter tokens
token_ids = [tid for _, _, tid in letter_tokens]
probs = F.softmax(logits[:, token_ids], dim=-1)

# Map back to candidate indices
prob_array = np.zeros(num_candidates)
for idx, (cand_idx, letter, token_id) in enumerate(letter_tokens):
    if cand_idx < num_candidates:
        prob_array[cand_idx] = probs[0, idx].item()
```

**K·∫øt lu·∫≠n**: ‚úÖ **Code hi·ªán t·∫°i ƒë√£ implement ƒë√∫ng verbalizer approach c·ªßa LlamaRec**

---

### **3. Reranking**

| Aspect | LlamaRec | Project hi·ªán t·∫°i |
|--------|----------|------------------|
| **Two-stage** | ‚úÖ Retrieval + Ranking | ‚úÖ Retrieval + Ranking |
| **Prompt Format** | ‚úÖ Letter labels (A, B, C, ...) | ‚úÖ Letter labels (A, B, C, ...) |
| **Output** | ‚úÖ Single letter token | ‚úÖ Single letter token |
| **Ranking** | ‚úÖ Sort by probability | ‚úÖ Sort by probability |

**Status**: ‚úÖ **Gi·ªëng LlamaRec**

**Code hi·ªán t·∫°i** (`rerank/methods/qwen_reranker_unified.py:426-500`):
```python
def rerank(self, user_id: int, candidates: List[int], **kwargs: Any) -> List[Tuple[int, float]]:
    """Rerank candidates using LLM."""
    # 1. Build prompt v·ªõi letter labels
    prompt = build_prompt_from_candidates(
        user_history, 
        candidates, 
        item_id2text,
        max_candidates=self.max_candidates
    )
    
    # 2. Extract probabilities
    probs = self.llm_model.predict_probs(prompt, num_candidates=len(candidates))
    
    # 3. Sort by probability
    ranked = sorted(
        zip(candidates, probs),
        key=lambda x: x[1],
        reverse=True
    )
    
    return ranked
```

**K·∫øt lu·∫≠n**: ‚úÖ **Code hi·ªán t·∫°i ƒë√£ implement ƒë√∫ng reranking process c·ªßa LlamaRec**

---

## ‚úÖ T√≥m t·∫Øt

### **Training**:
- ‚úÖ **ƒê√£ align**: D√πng letter labels, next-token prediction, mask prompt tokens
- ‚úÖ **Code**: `rerank/models/llm.py:164-263`

### **Logits Extraction**:
- ‚úÖ **ƒê√£ implement ƒë√∫ng**: Verbalizer approach v·ªõi letter tokens
- ‚úÖ **Code**: `rerank/models/llm.py:363-410`

### **Reranking**:
- ‚úÖ **ƒê√£ implement ƒë√∫ng**: Two-stage, letter labels, probability-based ranking
- ‚úÖ **Code**: `rerank/methods/qwen_reranker_unified.py:426-500`

---

## üéØ K·∫øt lu·∫≠n

**Project hi·ªán t·∫°i ƒë√£ implement ƒë√∫ng c√°ch LlamaRec train, l·∫•y logits v√† rerank:**

1. ‚úÖ **Training**: Next-token prediction v·ªõi letter labels, mask prompt tokens
2. ‚úÖ **Logits Extraction**: Verbalizer approach - extract letter token logits
3. ‚úÖ **Reranking**: Two-stage process, probability-based ranking

**Kh√°c bi·ªát duy nh·∫•t**:
- **Base Model**: LlamaRec d√πng Llama 2-7B, project hi·ªán t·∫°i d√πng Qwen3-0.6B/Qwen3-2BVL
- **LoRA Config**: Kh√°c nhau v·ªÅ rank v√† alpha (nh∆∞ng kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn approach)

**Recommendation**: ‚úÖ **Code hi·ªán t·∫°i ƒë√£ ƒë√∫ng theo LlamaRec approach**

---

## üìö References

- **LlamaRec Repository**: [https://github.com/Yueeeeeeee/LlamaRec](https://github.com/Yueeeeeeee/LlamaRec)
- **Paper**: "LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking" (PGAI@CIKM 2023)
- **Verbalizer Approach**: Transform output logits into probability distributions over candidate items

