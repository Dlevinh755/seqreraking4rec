# Qwen3VLReranker Training Report

## T·ªïng quan

Qwen3VLReranker h·ªó tr·ª£ training cho **t·∫•t c·∫£ 4 modes**:
1. `raw_image`: S·ª≠ d·ª•ng raw images tr·ª±c ti·∫øp
2. `caption`: S·ª≠ d·ª•ng image captions
3. `semantic_summary`: S·ª≠ d·ª•ng semantic summaries v·ªõi Qwen3-VL
4. `semantic_summary_small`: S·ª≠ d·ª•ng semantic summaries v·ªõi model nh·ªè h∆°n (Qwen3-0.6B)

## Training Architecture

### 1. Text Model Training (`semantic_summary_small`)

**Model**: `unsloth/Qwen3-0.6B-unsloth-bnb-4bit`
- **Framework**: Unsloth v·ªõi LoRA adapters
- **Memory**: ~4-8GB GPU
- **Training**: Parameter-efficient fine-tuning (ch·ªâ train adapters)

**Process**:
1. Load model v·ªõi Unsloth `FastLanguageModel`
2. Setup LoRA adapters (r=8, alpha=16)
3. Prepare training data: (prompt, target_number)
4. Train v·ªõi transformers `Trainer`
5. Validation v√† early stopping

### 2. VL Model Training (`raw_image`, `caption`, `semantic_summary`)

**Model**: `unsloth/Qwen3-VL-2B-Instruct`
- **Framework**: Transformers Trainer
- **Memory**: ~8-16GB GPU
- **Training**: Full model fine-tuning (ho·∫∑c c√≥ th·ªÉ d√πng LoRA n·∫øu c·∫ßn)

**Process**:
1. Load Qwen3-VL model v·ªõi `Qwen3VLForConditionalGeneration`
2. Prepare training data v·ªõi proper format
3. Custom data collator ƒë·ªÉ handle multimodal inputs
4. Train v·ªõi transformers `Trainer`
5. Validation v√† early stopping

**L∆∞u √Ω cho `raw_image` mode**:
- Training s·ª≠ d·ª•ng **raw images tr·ª±c ti·∫øp** (gi·ªëng nh∆∞ inference)
- Images ƒë∆∞·ª£c load t·ª´ `item_meta[item_id]["image_path"]` v√† ƒë∆∞a v√†o messages
- Format: messages v·ªõi content l√† list ch·ª©a `{"type": "image", "image": PIL.Image}` v√† `{"type": "text", "text": "..."}`

## Data Requirements

### 1. `raw_image` Mode

**Required**:
- `item_meta[item_id]["image_path"]`: ƒê∆∞·ªùng d·∫´n ƒë·∫øn image file (b·∫Øt bu·ªôc cho c·∫£ training v√† inference)
- `item_meta[item_id]["text"]`: Item text/description

**Data Preparation**:
```bash
# Download images (no caption generation needed for raw_image mode)
python data_prepare.py \
    --dataset_code beauty \
    --min_rating 3 \
    --min_uc 20 \
    --min_sc 20 \
    --use_image
```

**Note**: `raw_image` mode s·ª≠ d·ª•ng tr·ª±c ti·∫øp raw images cho c·∫£ training v√† inference, kh√¥ng c·∫ßn captions.

### 2. `caption` Mode

**Required**:
- `item_meta[item_id]["caption"]`: Image caption
- `item_meta[item_id]["text"]`: Item text/description

**Data Preparation**:
```bash
# Generate captions
python data_prepare.py \
    --dataset_code beauty \
    --min_rating 3 \
    --min_uc 20 \
    --min_sc 20 \
    --use_image \
    --generate_caption
```

### 3. `semantic_summary` Mode

**Required**:
- `item_meta[item_id]["semantic_summary"]`: Semantic summary
- `item_meta[item_id]["text"]`: Item text/description

**Data Preparation**:
```bash
# Generate semantic summaries
python data_prepare.py \
    --dataset_code beauty \
    --min_rating 3 \
    --min_uc 20 \
    --min_sc 20 \
    --use_image \
    --generate_semantic_summary
```

### 4. `semantic_summary_small` Mode

**Required**:
- `item_meta[item_id]["semantic_summary"]`: Semantic summary
- `item_meta[item_id]["text"]`: Item text/description

**Data Preparation**:
```bash
# Generate semantic summaries
python data_prepare.py \
    --dataset_code beauty \
    --min_rating 3 \
    --min_uc 20 \
    --min_sc 20 \
    --use_image \
    --generate_semantic_summary
```

## Training Setup

### Hyperparameters

T·∫•t c·∫£ modes s·ª≠ d·ª•ng c√πng hyperparameters t·ª´ config:

```python
# From config.py
--rerank_epochs: int = 10          # S·ªë epochs
--rerank_batch_size: int = 32      # Batch size
--rerank_lr: float = 1e-4          # Learning rate
--rerank_patience: int = 5         # Early stopping patience
```

### Training Data Format

**Input**: `train_data: Dict[int, List[int]]`
- Key: user_id (1-indexed)
- Value: List of item_ids in chronological order

**Training Samples**:
- History: `items[0:end_pos]` (randomly selected split point)
- Target: `items[end_pos]` (next item)
- Candidates: `[target_item] + 19 random negatives`
- Target label: Index c·ªßa target trong candidates (1-indexed)

**Example**:
```python
# User history: [1, 2, 3, 4, 5]
# Split at position 3
# History: [1, 2, 3]
# Target: 4
# Candidates: [4, 10, 15, 20, ...] (shuffled)
# Target label: "2" (if target is at position 2 after shuffle)
```

### Training Process

#### Step 1: Prepare Training Samples

```python
def _prepare_training_samples(train_data):
    # For each user:
    # 1. Randomly select split point
    # 2. History = items[:split_point]
    # 3. Target = items[split_point]
    # 4. Candidates = [target] + 19 negatives
    # 5. Shuffle candidates
    # 6. Find target index
    return samples
```

#### Step 2: Build Training Prompts

**Format cho text-only modes** (`caption`, `semantic_summary`, `semantic_summary_small`):
```
You are a recommendation ranking assistant.

Choose exactly ONE item the user is most likely to interact with next.

User history:
- item_1 (Image: caption_1)
- item_2 (Image: caption_2)
...

Candidate items:
1. item_10 (Image: caption_10)
2. item_4 (Image: caption_4)  # Target
3. item_15 (Image: caption_15)
...

Answer with only one number (1-20).
```

**Format cho `raw_image` mode**:
Messages v·ªõi content l√† list ch·ª©a images v√† text:
```python
messages = [{
    "role": "user",
    "content": [
        {"type": "text", "text": "You are a recommendation ranking assistant.\n\nChoose exactly ONE item..."},
        {"type": "image", "image": PIL.Image},  # Candidate 1 image
        {"type": "text", "text": "1. item_10"},
        {"type": "image", "image": PIL.Image},  # Candidate 2 image
        {"type": "text", "text": "2. item_4"},  # Target
        ...
        {"type": "text", "text": "\nAnswer with only one number (1-20)."}
    ]
}]
```

**Target**: `"2"` (1-indexed position of target item)

#### Step 3: Training Loop

**Text Model** (`semantic_summary_small`):
```python
# 1. Tokenize prompts + targets
# 2. Create labels (same as input_ids for causal LM)
# 3. Train with Unsloth Trainer
# 4. Validation after each epoch
# 5. Early stopping based on val_recall
```

**VL Model** (`raw_image`, `caption`, `semantic_summary`):
```python
# 1. Apply chat template to prompts
# 2. Tokenize targets
# 3. Create labels (-100 for input, target tokens for output)
# 4. Custom collate function for batching
# 5. Train with transformers Trainer
# 6. Validation after each epoch
# 7. Early stopping based on val_recall
```

## Training Examples

### Example 1: Train `semantic_summary_small` Mode

```bash
# 1. Prepare data with semantic summaries
python data_prepare.py \
    --dataset_code beauty \
    --min_rating 3 \
    --min_uc 20 \
    --min_sc 20 \
    --use_image \
    --generate_semantic_summary

# 2. Train reranker
python scripts/train_rerank_standalone.py \
    --rerank_method qwen3vl \
    --qwen3vl_mode semantic_summary_small \
    --mode ground_truth \
    --rerank_top_k 50 \
    --rerank_epochs 10 \
    --rerank_batch_size 32 \
    --rerank_lr 1e-4 \
    --rerank_patience 5
```

### Example 2: Train `caption` Mode

```bash
# 1. Prepare data with captions
python data_prepare.py \
    --dataset_code beauty \
    --min_rating 3 \
    --min_uc 20 \
    --min_sc 20 \
    --use_image \
    --generate_caption

# 2. Train reranker
python scripts/train_rerank_standalone.py \
    --rerank_method qwen3vl \
    --qwen3vl_mode caption \
    --mode ground_truth \
    --rerank_top_k 50 \
    --rerank_epochs 10 \
    --rerank_batch_size 16 \
    --rerank_lr 1e-4 \
    --rerank_patience 5
```

### Example 3: Train `raw_image` Mode

```bash
# 1. Prepare data with images (no captions needed)
python data_prepare.py \
    --dataset_code beauty \
    --min_rating 3 \
    --min_uc 20 \
    --min_sc 20 \
    --use_image

# 2. Train reranker
# Note: Training uses raw images directly (same as inference)
python scripts/train_rerank_standalone.py \
    --rerank_method qwen3vl \
    --qwen3vl_mode raw_image \
    --mode ground_truth \
    --rerank_top_k 50 \
    --rerank_epochs 10 \
    --rerank_batch_size 8 \
    --rerank_lr 1e-4 \
    --rerank_patience 5
```

**Note**: `raw_image` mode s·ª≠ d·ª•ng raw images tr·ª±c ti·∫øp cho c·∫£ training v√† inference. Images ƒë∆∞·ª£c load t·ª´ `image_path` v√† ƒë∆∞a v√†o messages v·ªõi format `{"type": "image", "image": PIL.Image}`.

### Example 4: Train `semantic_summary` Mode

```bash
# 1. Prepare data with semantic summaries
python data_prepare.py \
    --dataset_code beauty \
    --min_rating 3 \
    --min_uc 20 \
    --min_sc 20 \
    --use_image \
    --generate_semantic_summary

# 2. Train reranker
python scripts/train_rerank_standalone.py \
    --rerank_method qwen3vl \
    --qwen3vl_mode semantic_summary \
    --mode ground_truth \
    --rerank_top_k 50 \
    --rerank_epochs 10 \
    --rerank_batch_size 16 \
    --rerank_lr 1e-4 \
    --rerank_patience 5
```

## Training Configuration

### Batch Size Recommendations

| Mode | Recommended Batch Size | Memory Usage |
|------|------------------------|--------------|
| `semantic_summary_small` | 32 | ~4-8GB |
| `caption` | 16 | ~8-12GB |
| `semantic_summary` | 16 | ~8-12GB |
| `raw_image` | 8 | ~12-16GB |

**Note**: Batch sizes c√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh d·ª±a tr√™n GPU memory available.

### Learning Rate

- **Default**: `1e-4`
- **Range**: `5e-5` ƒë·∫øn `2e-4`
- **Recommendation**: 
  - Text model: `1e-4` ho·∫∑c `2e-4`
  - VL model: `1e-4` ho·∫∑c `5e-5`

### Epochs v√† Patience

- **Default epochs**: 10
- **Default patience**: 5
- **Recommendation**: 
  - N·∫øu dataset nh·ªè: epochs=20, patience=10
  - N·∫øu dataset l·ªõn: epochs=5, patience=3

## Validation Process

### Validation Metrics

- **Metric**: Recall@K (K = min(10, top_k))
- **Process**:
  1. For each user in validation set
  2. Get user history from training data
  3. Rerank all items (or sample 100 items for efficiency)
  4. Compute Recall@K
  5. Average across all users

### Early Stopping

- **Criterion**: Validation Recall@K
- **Patience**: Number of epochs without improvement
- **Best model**: Model v·ªõi highest validation Recall@K

## Training Output

### Console Output

```
[Qwen3VLReranker] Training epoch 1/10...
Training loss: 2.3456
[Qwen3VLReranker] Epoch 1/10 - val_Recall@10: 0.1234

[Qwen3VLReranker] Training epoch 2/10...
Training loss: 1.9876
[Qwen3VLReranker] Epoch 2/10 - val_Recall@10: 0.1456 [BEST]

...

[Qwen3VLReranker] Training epoch 6/10...
Training loss: 1.2345
[Qwen3VLReranker] Epoch 6/10 - val_Recall@10: 0.1456
Early stopping at epoch 6

Loaded best model with val_Recall@10: 0.1456
```

### Saved Checkpoints

- **Text model**: `./qwen3vl_rerank/checkpoint-{step}/`
- **VL model**: `./qwen3vl_rerank_vl/checkpoint-{step}/`

## Troubleshooting

### 1. Out of Memory

**Problem**: GPU out of memory during training

**Solutions**:
- Gi·∫£m `--rerank_batch_size`
- TƒÉng `gradient_accumulation_steps` (trong code)
- S·ª≠ d·ª•ng `semantic_summary_small` mode (nh·∫π h∆°n)
- Gi·∫£m `max_history` trong Qwen3VLReranker

### 2. Training Loss kh√¥ng gi·∫£m

**Problem**: Loss kh√¥ng gi·∫£m ho·∫∑c tƒÉng

**Solutions**:
- Gi·∫£m learning rate (`--rerank_lr 5e-5`)
- TƒÉng batch size (n·∫øu c√≥ memory)
- Ki·ªÉm tra data quality (captions/semantic summaries c√≥ ƒë√∫ng kh√¥ng)
- Ki·ªÉm tra training samples c√≥ ƒë√∫ng format kh√¥ng

### 3. Validation Recall th·∫•p

**Problem**: Validation Recall@K th·∫•p ho·∫∑c kh√¥ng c·∫£i thi·ªán

**Solutions**:
- TƒÉng s·ªë epochs
- TƒÉng patience
- Ki·ªÉm tra validation data c√≥ ƒë√∫ng kh√¥ng
- Ki·ªÉm tra model c√≥ ƒë∆∞·ª£c load ƒë√∫ng kh√¥ng

### 4. Missing Dependencies

**Problem**: Import errors

**Solutions**:
```bash
# Install transformers from source (required for Qwen3-VL)
pip install git+https://github.com/huggingface/transformers

# Install unsloth
pip install unsloth[colab-new]

# Install other dependencies
pip install -r requirements.txt
```

## Best Practices

### 1. Data Preparation

- **Lu√¥n generate captions/semantic summaries tr∆∞·ªõc khi train**
- **Ki·ªÉm tra data quality**: ƒê·∫£m b·∫£o captions/semantic summaries c√≥ √Ω nghƒ©a
- **Filter items**: Ch·ªâ train v·ªõi items c√≥ ƒë·∫ßy ƒë·ªß features

### 2. Hyperparameter Tuning

- **B·∫Øt ƒë·∫ßu v·ªõi defaults**: epochs=10, batch_size=32, lr=1e-4
- **Tune batch size tr∆∞·ªõc**: TƒÉng/gi·∫£m d·ª±a tr√™n GPU memory
- **Tune learning rate**: N·∫øu loss kh√¥ng gi·∫£m, th·ª≠ gi·∫£m lr
- **Tune epochs/patience**: D·ª±a tr√™n validation performance

### 3. Training Strategy

- **Ground truth mode**: D√πng ƒë·ªÉ train v√† evaluate rerank quality
- **Retrieval mode**: D√πng ƒë·ªÉ train v·ªõi candidates t·ª´ retrieval stage
- **Validation**: Lu√¥n enable validation ƒë·ªÉ monitor training

### 4. Model Selection

- **Memory constrained**: D√πng `semantic_summary_small` mode
- **Best quality**: D√πng `raw_image` ho·∫∑c `semantic_summary` mode
- **Fast training**: D√πng `caption` mode (text-only, nhanh h∆°n)

## Comparison Table

| Mode | Model | Training | Memory | Speed | Quality |
|------|-------|----------|--------|-------|---------|
| `semantic_summary_small` | Qwen3-0.6B | ‚úÖ Unsloth LoRA | ~4-8GB | ‚ö° Fast | ‚≠ê‚≠ê‚≠ê |
| `caption` | Qwen3-VL-2B | ‚úÖ Transformers | ~8-12GB | üê¢ Medium | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `semantic_summary` | Qwen3-VL-2B | ‚úÖ Transformers | ~8-12GB | üê¢ Medium | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `raw_image` | Qwen3-VL-2B | ‚úÖ Transformers* | ~12-16GB | üêå Slow | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

*Note: `raw_image` training uses captions, but inference uses raw images.

## Summary

Qwen3VLReranker h·ªó tr·ª£ training cho **t·∫•t c·∫£ 4 modes**:

1. ‚úÖ **semantic_summary_small**: Text model v·ªõi Unsloth (nh·∫π nh·∫•t, nhanh nh·∫•t)
2. ‚úÖ **caption**: VL model v·ªõi text-only training (c√¢n b·∫±ng)
3. ‚úÖ **semantic_summary**: VL model v·ªõi semantic summaries (ch·∫•t l∆∞·ª£ng cao)
4. ‚úÖ **raw_image**: VL model v·ªõi caption-based training (ch·∫•t l∆∞·ª£ng cao nh·∫•t, inference d√πng raw images)

T·∫•t c·∫£ modes ƒë·ªÅu:
- Support validation v√† early stopping
- S·ª≠ d·ª•ng hyperparameters t·ª´ config
- Fair comparison v·ªõi c√°c rerank methods kh√°c
- ƒê∆∞·ª£c fine-tune tr√™n dataset hi·ªán t·∫°i

