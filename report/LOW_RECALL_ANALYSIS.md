# Ph√¢n t√≠ch: T·∫°i sao Recall@20 th·∫•p (0.4 - ngang v·ªõi random)?

## üìä V·∫•n ƒë·ªÅ

**Recall@20 ‚âà 0.4** - G·∫ßn nh∆∞ random performance

**Random baseline**:
- V·ªõi 20 candidates v√† 1 GT item: Recall@20 = 1/20 = **0.05** (5%)
- V·ªõi 20 candidates v√† nhi·ªÅu GT items: Recall@20 c√≥ th·ªÉ cao h∆°n
- **0.4 recall@20** cho th·∫•y model g·∫ßn nh∆∞ kh√¥ng h·ªçc ƒë∆∞·ª£c g√¨

---

## üîç Nguy√™n nh√¢n ch√≠nh

### **1. Epochs qu√° √≠t (CRITICAL)** ‚ö†Ô∏è

**Config hi·ªán t·∫°i** (`config.py:122`):
```python
parser.add_argument('--rerank_epochs', type=int, default=1,  # ‚ùå QU√Å √çT!
```

**V·∫•n ƒë·ªÅ**:
- **1 epoch** l√† qu√° √≠t ƒë·ªÉ model h·ªçc ƒë∆∞·ª£c pattern
- V·ªõi LLM fine-tuning, th∆∞·ªùng c·∫ßn **3-10 epochs**
- Model ch∆∞a k·ªãp converge

**Ph√¢n t√≠ch**:
- Training steps = `(num_samples / batch_size) * epochs`
- V·ªõi 1635 samples, batch_size=16, gradient_accumulation=2:
  - Effective batch size = 16 * 2 = 32
  - Steps per epoch = 1635 / 32 ‚âà **51 steps**
  - Total steps = 51 * 1 = **51 steps** (qu√° √≠t!)

**Gi·∫£i ph√°p**:
```python
# config.py
parser.add_argument('--rerank_epochs', type=int, default=5,  # ‚úÖ TƒÉng l√™n 5-10
```

---

### **2. Learning Rate c√≥ th·ªÉ ch∆∞a t·ªëi ∆∞u** ‚ö†Ô∏è

**Config hi·ªán t·∫°i** (`config.py:126`):
```python
parser.add_argument('--rerank_lr', type=float, default=1e-4,
```

**Ph√¢n t√≠ch**:
- `1e-4` c√≥ th·ªÉ OK cho LoRA fine-tuning
- Nh∆∞ng v·ªõi ch·ªâ 1 epoch, learning rate c·∫ßn cao h∆°n ƒë·ªÉ model h·ªçc nhanh h∆°n
- LlamaRec th∆∞·ªùng d√πng `1e-4` ƒë·∫øn `5e-4`

**Gi·∫£i ph√°p**:
```python
# Th·ª≠ tƒÉng learning rate n·∫øu epochs v·∫´n √≠t
parser.add_argument('--rerank_lr', type=float, default=2e-4,  # ‚úÖ TƒÉng l√™n 2e-4
```

---

### **3. Model ch∆∞a ƒë∆∞·ª£c train ƒë·ªß** ‚ö†Ô∏è

**Tri·ªáu ch·ª©ng**:
- Recall@20 ‚âà 0.4 (g·∫ßn random)
- Model c√≥ th·ªÉ ƒëang predict g·∫ßn nh∆∞ uniform distribution

**Ki·ªÉm tra**:
```python
# Ki·ªÉm tra training loss
# N·∫øu loss > 3.0 sau training ‚Üí model ch∆∞a h·ªçc ƒë∆∞·ª£c g√¨
# V·ªõi 50 candidates: random loss ‚âà -log(1/50) ‚âà 3.9
```

**Gi·∫£i ph√°p**:
- TƒÉng epochs
- Ki·ªÉm tra training loss c√≥ gi·∫£m kh√¥ng
- Ki·ªÉm tra validation loss

---

### **4. LoRA Config c√≥ th·ªÉ ch∆∞a t·ªëi ∆∞u** ‚ö†Ô∏è

**Code hi·ªán t·∫°i** (`rerank/models/llm.py:146-154`):
```python
self.model = FastLanguageModel.get_peft_model(
    self.model,
    r = 8,              # LoRA rank
    target_modules = ["q_proj","k_proj","v_proj","o_proj"],
    lora_alpha = 16,   # LoRA alpha
    lora_dropout = 0.05,
    bias = "none",
    use_gradient_checkpointing = True,
)
```

**Ph√¢n t√≠ch**:
- `r=8, alpha=16` l√† kh√° nh·ªè
- LlamaRec v√† c√°c paper th∆∞·ªùng d√πng `r=16-32, alpha=32-64`
- V·ªõi model nh·ªè (Qwen3-0.6B), `r=8` c√≥ th·ªÉ ƒë·ªß, nh∆∞ng c√≥ th·ªÉ th·ª≠ tƒÉng

**Gi·∫£i ph√°p**:
```python
# Th·ª≠ tƒÉng LoRA rank
r = 16,              # TƒÉng t·ª´ 8 l√™n 16
lora_alpha = 32,     # TƒÉng t·ª´ 16 l√™n 32
```

---

### **5. Evaluation Setup c√≥ th·ªÉ c√≥ v·∫•n ƒë·ªÅ** ‚ö†Ô∏è

**Ki·ªÉm tra evaluation process**:

1. **Candidates sampling**:
   - C√≥ ƒë·∫£m b·∫£o GT item c√≥ trong candidates kh√¥ng?
   - C√≥ shuffle candidates ƒë·ªÉ tr√°nh bias kh√¥ng?

2. **History exclusion**:
   - C√≥ exclude history items kh·ªèi candidates kh√¥ng?

3. **Prompt format**:
   - Prompt c√≥ ƒë√∫ng format kh√¥ng?
   - Model c√≥ hi·ªÉu ƒë∆∞·ª£c task kh√¥ng?

**Code evaluation** (`rerank/methods/qwen_reranker_unified.py:1333-1374`):
```python
def _evaluate_split(self, split: Dict[int, List[int]], k: int) -> float:
    recalls = []
    for user_id, gt_items in split.items():
        # ... get candidates ...
        # Rerank
        reranked = self.rerank(user_id, candidates)
        # Compute recall
        top_k_items = [item_id for item_id, _ in reranked[:k]]
        hits = len(set(top_k_items) & set(gt_items))
        recalls.append(hits / len(gt_items))
    return float(np.mean(recalls))
```

**V·∫•n ƒë·ªÅ c√≥ th·ªÉ c√≥**:
- Candidates c√≥ th·ªÉ kh√¥ng ch·ª©a GT items
- Model c√≥ th·ªÉ kh√¥ng predict ƒë√∫ng letter tokens
- Probabilities c√≥ th·ªÉ b·ªã uniform

---

### **6. Model Prediction c√≥ v·∫•n ƒë·ªÅ** ‚ö†Ô∏è

**Ki·ªÉm tra logits extraction**:

1. **Letter tokens c√≥ ƒë∆∞·ª£c t√¨m th·∫•y kh√¥ng?**
```python
# rerank/models/llm.py:386-393
if len(letter_tokens) < num_candidates:
    print(f"[WARNING] Only found {len(letter_tokens)}/{num_candidates} letter tokens!")
    # N·∫øu kh√¥ng t√¨m th·∫•y letter tokens ‚Üí fallback to uniform
```

2. **Probabilities c√≥ b·ªã uniform kh√¥ng?**
```python
# Ki·ªÉm tra output probabilities
probs = self.llm_model.predict_probs(prompt, num_candidates=len(candidates))
print(f"Probs: {probs}")  # N·∫øu g·∫ßn uniform ‚Üí model ch∆∞a h·ªçc ƒë∆∞·ª£c g√¨
```

3. **Model c√≥ predict ƒë√∫ng letter kh√¥ng?**
```python
# Ki·ªÉm tra predicted letter
predicted_letter = LETTERS[np.argmax(probs)]
print(f"Predicted: {predicted_letter}, GT: {gt_letter}")
```

---

## üéØ Gi·∫£i ph√°p ∆∞u ti√™n

### **Priority 1: TƒÉng Epochs (CRITICAL)** üî¥

```python
# config.py
parser.add_argument('--rerank_epochs', type=int, default=5,  # ‚úÖ TƒÉng t·ª´ 1 l√™n 5
                    help='Number of training epochs for rerank models.')
```

**L√Ω do**:
- 1 epoch qu√° √≠t ƒë·ªÉ model h·ªçc ƒë∆∞·ª£c pattern
- V·ªõi 1635 samples, c·∫ßn √≠t nh·∫•t 3-5 epochs
- LlamaRec th∆∞·ªùng train 3-10 epochs

---

### **Priority 2: Ki·ªÉm tra Training Loss** üü°

```python
# Th√™m logging ƒë·ªÉ ki·ªÉm tra training loss
# N·∫øu loss kh√¥ng gi·∫£m ‚Üí c√≥ v·∫•n ƒë·ªÅ v·ªõi training
```

**Expected behavior**:
- Initial loss: ~3.9 (random v·ªõi 50 candidates)
- After 1 epoch: ~2.0-3.0 (n·∫øu model h·ªçc ƒë∆∞·ª£c m·ªôt ch√∫t)
- After 5 epochs: ~1.0-2.0 (n·∫øu model h·ªçc t·ªët)

---

### **Priority 3: TƒÉng LoRA Rank (n·∫øu c·∫ßn)** üü°

```python
# rerank/models/llm.py
self.model = FastLanguageModel.get_peft_model(
    self.model,
    r = 16,              # ‚úÖ TƒÉng t·ª´ 8 l√™n 16
    lora_alpha = 32,     # ‚úÖ TƒÉng t·ª´ 16 l√™n 32
    ...
)
```

**L√Ω do**:
- TƒÉng model capacity
- C√≥ th·ªÉ improve performance
- Trade-off: ch·∫≠m h∆°n m·ªôt ch√∫t

---

### **Priority 4: Ki·ªÉm tra Evaluation** üü¢

1. **Debug prediction**:
```python
# Th√™m debug output
probs = self.llm_model.predict_probs(prompt, num_candidates=len(candidates))
print(f"Probs: {probs[:5]}")  # Top 5 probabilities
print(f"Max prob: {np.max(probs)}, Min prob: {np.min(probs)}")
```

2. **Ki·ªÉm tra letter tokens**:
```python
# Ki·ªÉm tra xem letter tokens c√≥ ƒë∆∞·ª£c t√¨m th·∫•y kh√¥ng
# N·∫øu kh√¥ng ‚Üí c√≥ v·∫•n ƒë·ªÅ v·ªõi tokenizer
```

3. **Ki·ªÉm tra candidates**:
```python
# ƒê·∫£m b·∫£o GT items c√≥ trong candidates
# N·∫øu kh√¥ng ‚Üí recall s·∫Ω = 0
```

---

## üìä Expected Results sau khi s·ª≠a

### **Sau khi tƒÉng epochs l√™n 5**:

**Expected**:
- Training loss: ~1.5-2.5 (gi·∫£m t·ª´ ~3.9)
- Recall@20: **0.6-0.8** (tƒÉng t·ª´ 0.4)
- NDCG@10: **0.3-0.5** (tƒÉng t·ª´ ~0.1)

**N·∫øu v·∫´n th·∫•p**:
- Ki·ªÉm tra training data quality
- Ki·ªÉm tra model size (c√≥ th·ªÉ c·∫ßn model l·ªõn h∆°n)
- Ki·ªÉm tra evaluation setup

---

## üîß Action Items

### **Immediate (CRITICAL)**:

1. ‚úÖ **TƒÉng epochs l√™n 5-10**:
   ```python
   # config.py
   --rerank_epochs 5
   ```

2. ‚úÖ **Ki·ªÉm tra training loss**:
   - Xem loss c√≥ gi·∫£m kh√¥ng
   - N·∫øu kh√¥ng gi·∫£m ‚Üí c√≥ v·∫•n ƒë·ªÅ v·ªõi training

3. ‚úÖ **Debug prediction**:
   - Ki·ªÉm tra probabilities c√≥ uniform kh√¥ng
   - Ki·ªÉm tra letter tokens c√≥ ƒë∆∞·ª£c t√¨m th·∫•y kh√¥ng

### **Next Steps**:

4. ‚úÖ **TƒÉng LoRA rank** (n·∫øu epochs kh√¥ng ƒë·ªß):
   ```python
   r = 16, alpha = 32
   ```

5. ‚úÖ **TƒÉng learning rate** (n·∫øu c·∫ßn):
   ```python
   --rerank_lr 2e-4
   ```

6. ‚úÖ **Ki·ªÉm tra evaluation setup**:
   - ƒê·∫£m b·∫£o GT items c√≥ trong candidates
   - Ki·ªÉm tra prompt format

---

## üìö References

- **LlamaRec**: Th∆∞·ªùng train 3-10 epochs
- **LoRA best practices**: r=16-32, alpha=32-64 cho better performance
- **Learning rate**: 1e-4 ƒë·∫øn 5e-4 cho LoRA fine-tuning

---

## ‚úÖ T√≥m t·∫Øt

**Nguy√™n nh√¢n ch√≠nh**: **Epochs qu√° √≠t (1 epoch)**

**Gi·∫£i ph√°p**:
1. ‚úÖ TƒÉng `--rerank_epochs` l√™n 5-10
2. ‚úÖ Ki·ªÉm tra training loss
3. ‚úÖ Debug prediction ƒë·ªÉ ƒë·∫£m b·∫£o model h·ªçc ƒë∆∞·ª£c

**Expected improvement**: Recall@20 t·ª´ 0.4 ‚Üí **0.6-0.8** sau khi tƒÉng epochs

