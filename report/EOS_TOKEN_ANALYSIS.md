# PhÃ¢n tÃ­ch EOS Token trong Prompt

## âœ… Káº¿t quáº£ kiá»ƒm tra

### 1. **Training (LLM.train())**

**File**: `rerank/models/llm.py:186-200`

**CÃ¡ch hoáº¡t Ä‘á»™ng**:
```python
text = self.tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=False
)
```

**Káº¿t quáº£**:
- `apply_chat_template` tá»± Ä‘á»™ng format messages vá»›i chat template cá»§a Qwen
- Output format: `'\n<user_content><|im_end|>\n<|im_start|>assistant\n<response><|im_end|>\n'`
- **EOS token (`<|im_end|>`) Ä‘Æ°á»£c tá»± Ä‘á»™ng thÃªm vÃ o sau response**

**VÃ­ dá»¥**:
- Input: `messages = [{'role': 'user', 'content': 'test'}, {'role': 'assistant', 'content': 'A'}]`
- Output: `'\ntest<|im_end|>\n<|im_start|>assistant\nA<|im_end|>\n'`
- âœ… **EOS token cÃ³ trong training data**

### 2. **Training vá»›i `train_on_responses_only`**

**File**: `rerank/models/llm.py:244-248`

**CÃ¡ch hoáº¡t Ä‘á»™ng**:
```python
trainer = train_on_responses_only(
    trainer,
    instruction_part="<|im_start|>user\n",
    response_part="<|im_start|>assistant\n",
)
```

**Káº¿t quáº£**:
- `train_on_responses_only` mask prompt tokens, chá»‰ tÃ­nh loss trÃªn response tokens
- Response part: `"<|im_start|>assistant\n"` - Ä‘Ã¢y lÃ  prefix cho response
- **EOS token (`<|im_end|>`) Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o bá»Ÿi `apply_chat_template` trÆ°á»›c Ä‘Ã³**
- âœ… **Loss Ä‘Æ°á»£c tÃ­nh trÃªn response tokens (bao gá»“m cáº£ EOS token)**

### 3. **Inference (`predict_probs()`)**

**File**: `rerank/models/llm.py:296-300`

**CÃ¡ch hoáº¡t Ä‘á»™ng**:
```python
inputs = self.tokenizer(
    prompt, 
    return_tensors="pt",
    truncation=True,
    max_length=max_length,
)
```

**Káº¿t quáº£**:
- `tokenizer()` vá»›i default `add_special_tokens=True` (khÃ´ng explicit)
- Prompt lÃ  plain text (khÃ´ng pháº£i chat template format)
- **EOS token KHÃ”NG Ä‘Æ°á»£c thÃªm vÃ o prompt** (vÃ¬ prompt khÃ´ng pháº£i chat format)
- Model sáº½ predict next token sau prompt (khÃ´ng cÃ³ EOS)

### 4. **So sÃ¡nh vá»›i LlamaRec**

**LlamaRec yÃªu cáº§u**:
- Training: Loss chá»‰ tÃ­nh trÃªn response tokens (bao gá»“m EOS)
- Inference: Model predict next token (letter) sau prompt

**Code hiá»‡n táº¡i**:
- âœ… Training: EOS token cÃ³ trong response, loss chá»‰ tÃ­nh trÃªn response (bao gá»“m EOS)
- âœ… Inference: Model predict next token (letter) sau prompt (khÃ´ng cÃ³ EOS trong prompt)

## ğŸ“ Káº¿t luáº­n

### âœ… **EOS token ÄÆ¯á»¢C Sá»¬ Dá»¤NG trong Training**
- `apply_chat_template` tá»± Ä‘á»™ng thÃªm `<|im_end|>` sau response
- `train_on_responses_only` tÃ­nh loss trÃªn response tokens (bao gá»“m EOS)
- ÄÃºng vá»›i LlamaRec: loss chá»‰ tÃ­nh trÃªn response (token label + EOS)

### âš ï¸ **EOS token KHÃ”NG cÃ³ trong Inference Prompt**
- Prompt lÃ  plain text, khÃ´ng pháº£i chat template format
- `tokenizer()` khÃ´ng thÃªm EOS vÃ o prompt (vÃ¬ khÃ´ng pháº£i chat format)
- Model predict next token (letter) sau prompt
- **Äiá»u nÃ y lÃ  ÄÃšNG** - model cáº§n predict letter, khÃ´ng cáº§n EOS trong prompt

## ğŸ” Kiá»ƒm tra thÃªm

Cáº§n kiá»ƒm tra xem:
1. Model cÃ³ tá»± Ä‘á»™ng thÃªm EOS khi generate khÃ´ng?
2. `predict_probs()` cÃ³ cáº§n thÃªm EOS vÃ o prompt khÃ´ng?

**Káº¿t luáº­n**: Code hiá»‡n táº¡i Ä‘Ã£ Ä‘Ãºng - EOS token Ä‘Æ°á»£c sá»­ dá»¥ng trong training (tá»± Ä‘á»™ng bá»Ÿi `apply_chat_template`), nhÆ°ng khÃ´ng cáº§n trong inference prompt (vÃ¬ model chá»‰ cáº§n predict letter).

