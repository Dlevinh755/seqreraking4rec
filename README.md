# Sequential Reranking for Recommendation

H·ªá th·ªëng recommendation hai giai ƒëo·∫°n (Two-Stage): Retrieval (Stage 1) + Reranking (Stage 2).

## üìã T·ªïng quan

Project n√†y implement m·ªôt pipeline recommendation hai giai ƒëo·∫°n:
- **Stage 1 (Retrieval)**: Generate candidates t·ª´ to√†n b·ªô item pool
- **Stage 2 (Reranking)**: Re-rank candidates t·ª´ Stage 1 ƒë·ªÉ t·∫°o final recommendations

### Features
- ‚úÖ 4 Retrieval methods: LRURec, MMGCN, VBPR, BM3
- ‚úÖ 5 Rerank methods: Qwen, Qwen3-VL (4 modes), VIP5, BERT4Rec
- ‚úÖ Multimodal support: Images, text, captions, semantic summaries
- ‚úÖ Training v√† evaluation ƒë·ªôc l·∫≠p cho t·ª´ng stage
- ‚úÖ Evaluation metrics: Recall@K, NDCG@K, Hit@K t·∫°i @5, @10, @20
- ‚úÖ Image preprocessing: T·ª± ƒë·ªông resize v·ªÅ 448px
- ‚úÖ Per-epoch validation v·ªõi early stopping

## üì¶ C√†i ƒë·∫∑t

### 1. C√†i ƒë·∫∑t dependencies

```bash
# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c∆° b·∫£n
pip install -r requirements.txt

# C√†i ƒë·∫∑t transformers t·ª´ source (c·∫ßn cho Qwen3-VL)
pip install git+https://github.com/huggingface/transformers
```

### 2. C·∫•u h√¨nh dataset

Ch·ªânh s·ª≠a `config.py` ho·∫∑c s·ª≠ d·ª•ng command-line arguments ƒë·ªÉ c·∫•u h√¨nh dataset v√† hyperparameters.

## üöÄ Ch·∫°y Project

### B∆∞·ªõc 1: Prepare Data

```bash
# Basic data preparation
python data_prepare.py \
    --dataset_code beauty \
    --min_rating 3 \
    --min_uc 20 \
    --min_sc 20

# V·ªõi image filtering v√† CLIP embeddings
python data_prepare.py \
    --dataset_code beauty \
    --min_rating 3 \
    --min_uc 20 \
    --min_sc 20 \
    --use_image \
    --use_text

# V·ªõi caption generation (BLIP2)
python data_prepare.py \
    --dataset_code beauty \
    --min_rating 3 \
    --min_uc 20 \
    --min_sc 20 \
    --use_image \
    --generate_caption

# V·ªõi semantic summary generation (Qwen3-VL)
python data_prepare.py \
    --dataset_code beauty \
    --min_rating 3 \
    --min_uc 20 \
    --min_sc 20 \
    --use_image \
    --generate_semantic_summary
```

### B∆∞·ªõc 2: Train Retrieval (Stage 1)

#### 2.1. Neural LRURec
**Requirements**: Kh√¥ng c·∫ßn g√¨ ƒë·∫∑c bi·ªát, ch·ªâ c·∫ßn dataset ƒë√£ ƒë∆∞·ª£c prepare.

```bash
python scripts/train_retrieval.py --retrieval_method lrurec
```

#### 2.2. MMGCN (Multimodal Graph Convolutional Network)
**Requirements**: 
- Dataset v·ªõi images ho·∫∑c text (ch·∫°y `data_prepare.py` v·ªõi `--use_image` ho·∫∑c `--use_text`)
- CLIP embeddings s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông extract

```bash
# Prepare data v·ªõi images/text tr∆∞·ªõc
python data_prepare.py \
    --dataset_code beauty \
    --use_image \
    --use_text

# Train MMGCN
python scripts/train_retrieval.py --retrieval_method mmgcn
```

#### 2.3. VBPR (Visual Bayesian Personalized Ranking)
**Requirements**: 
- Dataset v·ªõi images (ch·∫°y `data_prepare.py` v·ªõi `--use_image`)
- CLIP image embeddings s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông extract

```bash
# Prepare data v·ªõi images tr∆∞·ªõc
python data_prepare.py \
    --dataset_code beauty \
    --use_image

# Train VBPR
python scripts/train_retrieval.py --retrieval_method vbpr
```

#### 2.4. BM3 (Bootstrap Latent Representations for Multi-modal Recommendation)
**Requirements**: 
- Dataset v·ªõi images v√† text (ch·∫°y `data_prepare.py` v·ªõi `--use_image` v√† `--use_text`)
- CLIP embeddings s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông extract

```bash
# Prepare data v·ªõi images v√† text tr∆∞·ªõc
python data_prepare.py \
    --dataset_code beauty \
    --use_image \
    --use_text

# Train BM3
python scripts/train_retrieval.py --retrieval_method bm3
```

### B∆∞·ªõc 3: Train Rerank (Stage 2) - Standalone

#### 3.1. Qwen LLM Reranker

**Requirements**: Dataset v·ªõi text data (`item_text` column)

**Ground Truth Mode** (kh√¥ng c·∫ßn retrieval model):
```bash
python scripts/train_rerank_standalone.py \
    --rerank_method qwen \
    --mode ground_truth \
    --rerank_top_k 50
```

**Retrieval Mode** (c·∫ßn retrieval model ƒë√£ train):
```bash
python scripts/train_rerank_standalone.py \
    --rerank_method qwen \
    --mode retrieval \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_top_k 50
```

#### 3.2. Qwen3-VL Reranker

**Requirements**: 
- `raw_image` mode: Dataset v·ªõi images
- `caption` mode: Dataset v·ªõi images + captions (ch·∫°y `data_prepare.py` v·ªõi `--generate_caption`)
- `semantic_summary` mode: Dataset v·ªõi images + semantic summaries (ch·∫°y `data_prepare.py` v·ªõi `--generate_semantic_summary`)
- `semantic_summary_small` mode: Dataset v·ªõi images + semantic summaries

**3.2.1. Raw Image Mode**
```bash
# Set mode trong config.py ho·∫∑c command line
# config.py: --qwen3vl_mode raw_image

python scripts/train_rerank_standalone.py \
    --rerank_method qwen3vl \
    --mode ground_truth \
    --rerank_top_k 50
```

**3.2.2. Caption Mode**
```bash
# Prepare data v·ªõi captions tr∆∞·ªõc
python data_prepare.py \
    --dataset_code beauty \
    --use_image \
    --generate_caption

# Set mode trong config.py
# config.py: --qwen3vl_mode caption

python scripts/train_rerank_standalone.py \
    --rerank_method qwen3vl \
    --mode ground_truth \
    --rerank_top_k 50
```

**3.2.3. Semantic Summary Mode**
```bash
# Prepare data v·ªõi semantic summaries tr∆∞·ªõc
python data_prepare.py \
    --dataset_code beauty \
    --use_image \
    --generate_semantic_summary

# Set mode trong config.py
# config.py: --qwen3vl_mode semantic_summary

python scripts/train_rerank_standalone.py \
    --rerank_method qwen3vl \
    --mode ground_truth \
    --rerank_top_k 50
```

**3.2.4. Semantic Summary Small Mode**
```bash
# Prepare data v·ªõi semantic summaries tr∆∞·ªõc (gi·ªëng semantic_summary mode)
python data_prepare.py \
    --dataset_code beauty \
    --use_image \
    --generate_semantic_summary

# Set mode trong config.py
# config.py: --qwen3vl_mode semantic_summary_small

python scripts/train_rerank_standalone.py \
    --rerank_method qwen3vl \
    --mode ground_truth \
    --rerank_top_k 50
```

**Retrieval Mode** (cho t·∫•t c·∫£ Qwen3-VL modes):
```bash
# Set qwen3vl_mode trong config.py tr∆∞·ªõc
python scripts/train_rerank_standalone.py \
    --rerank_method qwen3vl \
    --mode retrieval \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_top_k 50
```

#### 3.3. VIP5 Reranker

**Requirements**: Dataset v·ªõi images + CLIP embeddings

**Ground Truth Mode**:
```bash
python scripts/train_rerank_standalone.py \
    --rerank_method vip5 \
    --mode ground_truth \
    --rerank_top_k 50
```

**Retrieval Mode**:
```bash
python scripts/train_rerank_standalone.py \
    --rerank_method vip5 \
    --mode retrieval \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_top_k 50
```

#### 3.4. BERT4Rec Reranker

**Requirements**: Dataset v·ªõi sequential data (kh√¥ng c·∫ßn images/text)

**Ground Truth Mode**:
```bash
python scripts/train_rerank_standalone.py \
    --rerank_method bert4rec \
    --mode ground_truth \
    --rerank_top_k 50
```

**Retrieval Mode**:
```bash
python scripts/train_rerank_standalone.py \
    --rerank_method bert4rec \
    --mode retrieval \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_top_k 50
```

### B∆∞·ªõc 4: Train Pipeline (Stage 1 + Stage 2) - End-to-End

#### 4.1. Qwen LLM Reranker

**Retrieval Mode**:
```bash
python scripts/train_pipeline.py \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method qwen \
    --rerank_top_k 50 \
    --rerank_mode retrieval
```

**Ground Truth Mode**:
```bash
python scripts/train_pipeline.py \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method qwen \
    --rerank_top_k 50 \
    --rerank_mode ground_truth
```

#### 4.2. Qwen3-VL Reranker

**4.2.1. Raw Image Mode**
```bash
# Set mode trong config.py: --qwen3vl_mode raw_image
python scripts/train_pipeline.py \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method qwen3vl \
    --rerank_top_k 50 \
    --rerank_mode retrieval
```

**4.2.2. Caption Mode**
```bash
# Prepare data v·ªõi captions tr∆∞·ªõc
python data_prepare.py \
    --dataset_code beauty \
    --use_image \
    --generate_caption

# Set mode trong config.py: --qwen3vl_mode caption
python scripts/train_pipeline.py \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method qwen3vl \
    --rerank_top_k 50 \
    --rerank_mode retrieval
```

**4.2.3. Semantic Summary Mode**
```bash
# Prepare data v·ªõi semantic summaries tr∆∞·ªõc
python data_prepare.py \
    --dataset_code beauty \
    --use_image \
    --generate_semantic_summary

# Set mode trong config.py: --qwen3vl_mode semantic_summary
python scripts/train_pipeline.py \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method qwen3vl \
    --rerank_top_k 50 \
    --rerank_mode retrieval
```

**4.2.4. Semantic Summary Small Mode**
```bash
# Prepare data v·ªõi semantic summaries tr∆∞·ªõc
python data_prepare.py \
    --dataset_code beauty \
    --use_image \
    --generate_semantic_summary

# Set mode trong config.py: --qwen3vl_mode semantic_summary_small
python scripts/train_pipeline.py \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method qwen3vl \
    --rerank_top_k 50 \
    --rerank_mode retrieval
```

#### 4.3. VIP5 Reranker

```bash
python scripts/train_pipeline.py \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method vip5 \
    --rerank_top_k 50 \
    --rerank_mode retrieval
```

#### 4.4. BERT4Rec Reranker

```bash
python scripts/train_pipeline.py \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method bert4rec \
    --rerank_top_k 50 \
    --rerank_mode retrieval
```

### B∆∞·ªõc 5: Offline Evaluation

T·∫•t c·∫£ evaluation t·ª± ƒë·ªông t√≠nh metrics cho @5, @10, @20 v·ªõi Recall, NDCG, v√† Hit Rate tr√™n c·∫£ **val** v√† **test** sets.

```bash
# Evaluate retrieval only
python evaluation/offline_eval.py \
    --mode retrieval \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --split val  # or --split test

# Evaluate full pipeline v·ªõi Qwen reranker
python evaluation/offline_eval.py \
    --mode full \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method qwen \
    --rerank_top_k 50 \
    --rerank_mode retrieval \
    --split test

# Evaluate full pipeline v·ªõi Qwen3-VL (raw_image mode)
python evaluation/offline_eval.py \
    --mode full \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method qwen3vl \
    --qwen3vl_mode raw_image \
    --rerank_top_k 50 \
    --rerank_mode retrieval \
    --split val

# Evaluate full pipeline v·ªõi Qwen3-VL (caption mode)
python evaluation/offline_eval.py \
    --mode full \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method qwen3vl \
    --qwen3vl_mode caption \
    --rerank_top_k 50 \
    --rerank_mode retrieval \
    --split test

# Evaluate full pipeline v·ªõi Qwen3-VL (semantic_summary mode)
python evaluation/offline_eval.py \
    --mode full \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method qwen3vl \
    --qwen3vl_mode semantic_summary \
    --rerank_top_k 50 \
    --rerank_mode retrieval \
    --split val

# Evaluate full pipeline v·ªõi Qwen3-VL (semantic_summary_small mode)
python evaluation/offline_eval.py \
    --mode full \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method qwen3vl \
    --qwen3vl_mode semantic_summary_small \
    --rerank_top_k 50 \
    --rerank_mode retrieval \
    --split test

# Evaluate rerank only (ground truth + negatives)
python evaluation/offline_eval.py \
    --mode rerank_only \
    --retrieval_method lrurec \
    --retrieval_top_k 200 \
    --rerank_method qwen \
    --rerank_top_k 50 \
    --split val
```

**Output format**: T·∫•t c·∫£ metrics ƒë∆∞·ª£c hi·ªÉn th·ªã d·∫°ng b·∫£ng v·ªõi @5, @10, @20:
```
Metric       @5        @10        @20
Recall     0.1234    0.2345    0.3456
Ndcg       0.0567    0.0890    0.1234
Hit        0.4500    0.6700    0.8900
```

## üìù C√°c Methods Available

### Retrieval Methods (Stage 1)

| Method | Description | Requirements | Training Command |
|--------|-------------|--------------|------------------|
| `lrurec` | Neural LRU-based sequential recommender | Dataset c∆° b·∫£n | `python scripts/train_retrieval.py --retrieval_method lrurec` |
| `mmgcn` | Multimodal Graph Convolutional Network | Images/text + CLIP embeddings | `python scripts/train_retrieval.py --retrieval_method mmgcn` |
| `vbpr` | Visual Bayesian Personalized Ranking | Images + CLIP image embeddings | `python scripts/train_retrieval.py --retrieval_method vbpr` |
| `bm3` | Bootstrap Latent Representations for Multi-modal Recommendation | Images + text + CLIP embeddings | `python scripts/train_retrieval.py --retrieval_method bm3` |

### Rerank Methods (Stage 2)

| Method | Description | Requirements | Training Command (Standalone) |
|--------|-------------|--------------|-------------------------------|
| `qwen` | Qwen LLM-based reranker (text-only) | Text data | `python scripts/train_rerank_standalone.py --rerank_method qwen --mode ground_truth` |
| `qwen3vl` | Qwen3-VL reranker v·ªõi 4 modes | T√πy mode (xem b·∫£ng d∆∞·ªõi) | `python scripts/train_rerank_standalone.py --rerank_method qwen3vl --mode ground_truth` |
| `vip5` | VIP5 multimodal T5-based reranker | Images + CLIP embeddings | `python scripts/train_rerank_standalone.py --rerank_method vip5 --mode ground_truth` |
| `bert4rec` | BERT4Rec sequential reranker | Sequential data | `python scripts/train_rerank_standalone.py --rerank_method bert4rec --mode ground_truth` |

### Qwen3-VL Modes

| Mode | Description | Requirements | Data Preparation |
|------|-------------|--------------|-----------------|
| `raw_image` | Use raw images directly in prompt | Images | `--use_image` |
| `caption` | Use image captions | Images + captions | `--use_image --generate_caption` |
| `semantic_summary` | Use semantic summaries with Qwen3-VL | Images + semantic summaries | `--use_image --generate_semantic_summary` |
| `semantic_summary_small` | Use semantic summaries with smaller model | Images + semantic summaries | `--use_image --generate_semantic_summary` |

### Rerank Modes

| Mode | Description | Use Case |
|------|-------------|----------|
| `retrieval` | Use candidates from Stage 1 | Full pipeline evaluation |
| `ground_truth` | Use ground truth + 19 random negatives | Rerank quality evaluation (independent of retrieval) |

### Training Modes

| Mode | Script | Description | Requirements |
|------|--------|-------------|--------------|
| **End-to-end** | `train_pipeline.py` | Train c·∫£ retrieval v√† rerank c√πng l√∫c | C·∫£ hai stages |
| **Standalone rerank** | `train_rerank_standalone.py` | Train rerank ri√™ng l·∫ª | Rerank method + (optional) retrieval model |
| **Standalone retrieval** | `train_retrieval.py` | Train retrieval ri√™ng l·∫ª | Retrieval method |

## ‚öôÔ∏è Configuration

C√°c hyperparameters c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh trong `config.py`:

### Retrieval Hyperparameters
- `--retrieval_epochs`: S·ªë epochs cho retrieval training (default: 10)
- `--retrieval_lr`: Learning rate cho retrieval methods (default: 1e-3)
- `--batch_size_retrieval`: Batch size cho retrieval training (default: 128)
- `--retrieval_patience`: Early stopping patience (default: 5)

### Rerank Hyperparameters
- `--rerank_epochs`: S·ªë epochs cho rerank training (default: 10)
- `--rerank_lr`: Learning rate cho rerank methods (default: 1e-4)
- `--rerank_batch_size`: Batch size cho rerank training (default: 32)
- `--rerank_patience`: Early stopping patience (default: 5)

### Reranker-Specific Config
- `--qwen_max_candidates`: Max candidates cho Qwen reranker (None = d√πng t·∫•t c·∫£ t·ª´ retrieval)
- `--qwen3vl_mode`: Prompt mode cho Qwen3-VL reranker (raw_image, caption, semantic_summary, semantic_summary_small)

### Performance Optimization Config
- `--semantic_summary_batch_size`: Batch size cho semantic summary generation (default: 4, c√≥ th·ªÉ tƒÉng l√™n 8, 16, 32 n·∫øu GPU memory cho ph√©p)
- `--use_quantization`: S·ª≠ d·ª•ng 4-bit quantization cho models (ti·∫øt ki·ªám memory, tƒÉng t·ªëc)
- `--use_torch_compile`: S·ª≠ d·ª•ng torch.compile() ƒë·ªÉ compile models (tƒÉng t·ªëc inference, c·∫ßn PyTorch 2.0+)

## üìä Output

### Preprocessed Data
`data/preprocessed/{dataset_code}_min_rating{min_rating}-min_uc{min_uc}-min_sc{min_sc}/`
- `dataset_single_export.csv`: Dataset v·ªõi captions v√† semantic summaries
- `clip_embeddings.pt`: CLIP embeddings (n·∫øu c√≥)
- `blip2_captions.pt`: BLIP2 captions cache (n·∫øu c√≥)
- `qwen3vl_semantic_summaries.pt`: Qwen3-VL semantic summaries cache (n·∫øu c√≥)

### Retrieval Results
`experiments/retrieval/{method}/{dataset_code}/seed{seed}/`
- `retrieved.csv`: Retrieved candidates
- `retrieved_metrics.json`: Evaluation metrics v·ªõi @5, @10, @20

### Evaluation Results
T·∫•t c·∫£ evaluation t·ª± ƒë·ªông t√≠nh v√† hi·ªÉn th·ªã metrics cho @5, @10, @20:
- **Recall@K**: T·ª∑ l·ªá relevant items ƒë∆∞·ª£c retrieve trong top-K
- **NDCG@K**: Normalized Discounted Cumulative Gain t·∫°i K
- **Hit@K**: T·ª∑ l·ªá users c√≥ √≠t nh·∫•t 1 relevant item trong top-K

Output format:
```
Metric       @5        @10        @20
Recall     0.1234    0.2345    0.3456
Ndcg       0.0567    0.0890    0.1234
Hit        0.4500    0.6700    0.8900
```

## üí° Tips

1. **Training ƒë·ªôc l·∫≠p**: 
   - S·ª≠ d·ª•ng `train_rerank_standalone.py` ƒë·ªÉ train rerank ri√™ng l·∫ª, kh√¥ng c·∫ßn train retrieval
   - Ground truth mode kh√¥ng c·∫ßn retrieval model
   - Retrieval mode c·∫ßn load retrieval model ƒë√£ train s·∫µn

2. **Qwen reranker**: 
   - S·ªë l∆∞·ª£ng candidates t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh theo `retrieval_top_k`
   - C√≥ th·ªÉ gi·ªõi h·∫°n b·∫±ng `--qwen_max_candidates` trong config.py

3. **CLIP embeddings**: 
   - C·∫ßn ch·∫°y `data_prepare.py` v·ªõi `--use_image` ho·∫∑c `--use_text` tr∆∞·ªõc khi train MMGCN/VBPR/BM3

4. **Caption/Semantic Summary**: 
   - C·∫ßn ch·∫°y `data_prepare.py` v·ªõi `--generate_caption` ho·∫∑c `--generate_semantic_summary` ƒë·ªÉ generate
   - Captions c·∫ßn cho Qwen3-VL `caption` mode
   - Semantic summaries c·∫ßn cho Qwen3-VL `semantic_summary` v√† `semantic_summary_small` modes

5. **Image Resize**: 
   - T·∫•t c·∫£ images ƒë∆∞·ª£c t·ª± ƒë·ªông resize v·ªÅ max 448px tr√™n c·∫°nh d√†i h∆°n (gi·ªØ nguy√™n aspect ratio)
   - Gi√∫p ti·∫øt ki·ªám memory v√† tƒÉng t·ªëc x·ª≠ l√Ω
   - √Åp d·ª•ng cho c·∫£ training v√† inference

6. **Ground truth mode**: 
   - D√πng ƒë·ªÉ ƒë√°nh gi√° rerank quality ƒë·ªôc l·∫≠p v·ªõi retrieval quality
   - T·∫°o candidates = [ground_truth] + 19 random negatives

7. **Evaluation metrics**: 
   - T·∫•t c·∫£ evaluation t·ª± ƒë·ªông t√≠nh @5, @10, @20
   - Metrics: Recall, NDCG, Hit Rate
   - C√≥ th·ªÉ evaluate tr√™n c·∫£ val v√† test sets (d√πng `--split val` ho·∫∑c `--split test`)

8. **Qwen3-VL Training**: 
   - T·∫•t c·∫£ 4 modes ƒë·ªÅu h·ªó tr·ª£ training: `raw_image`, `caption`, `semantic_summary`, `semantic_summary_small`
   - `raw_image` mode s·ª≠ d·ª•ng raw images tr·ª±c ti·∫øp cho c·∫£ training v√† inference
   - Training s·ª≠ d·ª•ng per-epoch validation v·ªõi early stopping
   - Xem chi ti·∫øt trong `QWEN3VL_TRAINING_REPORT.md`

## ‚ö° Performance Optimization

### TƒÉng t·ªëc Semantic Summary Generation

```bash
# TƒÉng batch size (n·∫øu GPU memory cho ph√©p)
python data_prepare.py \
    --dataset_code beauty \
    --use_image \
    --generate_semantic_summary \
    --semantic_summary_batch_size 8  # TƒÉng t·ª´ 4 l√™n 8

# S·ª≠ d·ª•ng quantization ƒë·ªÉ ti·∫øt ki·ªám memory
python data_prepare.py \
    --dataset_code beauty \
    --use_image \
    --generate_semantic_summary \
    --use_quantization  # 4-bit quantization

# S·ª≠ d·ª•ng torch.compile() ƒë·ªÉ tƒÉng t·ªëc (PyTorch 2.0+)
python data_prepare.py \
    --dataset_code beauty \
    --use_image \
    --generate_semantic_summary \
    --use_torch_compile
```

### TƒÉng t·ªëc LLM Inference

```bash
# S·ª≠ d·ª•ng torch.compile() cho LLM inference
python scripts/train_pipeline.py \
    --rerank_method qwen \
    --use_torch_compile  # Compile model ƒë·ªÉ tƒÉng t·ªëc

# S·ª≠ d·ª•ng quantization (ƒë√£ c√≥ s·∫µn trong Unsloth)
# Model ƒë√£ ƒë∆∞·ª£c load v·ªõi 4-bit quantization m·∫∑c ƒë·ªãnh
```

**Expected Speedup**:
- `--semantic_summary_batch_size 8`: 2-4x faster
- `--use_quantization`: 1.5-2x faster, -50% memory
- `--use_torch_compile`: 1.2-1.5x faster

Xem chi ti·∫øt trong `OPTIMIZATION_GUIDE.md`.

## üîß Troubleshooting

- **Qwen3-VL kh√¥ng load ƒë∆∞·ª£c**: C·∫ßn c√†i transformers t·ª´ source:
  ```bash
  pip install git+https://github.com/huggingface/transformers
  ```

- **CLIP embeddings kh√¥ng t√¨m th·∫•y**: Ch·∫°y `data_prepare.py` v·ªõi `--use_image` ho·∫∑c `--use_text` tr∆∞·ªõc.

- **Out of memory**: 
  - Gi·∫£m `--batch_size_retrieval` ho·∫∑c `--rerank_batch_size` trong `config.py`
  - V·ªõi Qwen3-VL `raw_image` mode: gi·∫£m batch size xu·ªëng 4-8
  - Images ƒë√£ ƒë∆∞·ª£c t·ª± ƒë·ªông resize v·ªÅ 448px ƒë·ªÉ ti·∫øt ki·ªám memory
  - S·ª≠ d·ª•ng `--use_quantization` ƒë·ªÉ gi·∫£m memory usage

- **Qwen3-VL training ch·∫≠m**: 
  - S·ª≠ d·ª•ng `semantic_summary_small` mode (nh·∫π h∆°n, nhanh h∆°n)
  - Gi·∫£m batch size ho·∫∑c s·ªë l∆∞·ª£ng training samples
  - S·ª≠ d·ª•ng GPU v·ªõi ƒë·ªß memory (recommended: 12GB+ cho VL modes)
  - S·ª≠ d·ª•ng `--use_torch_compile` ƒë·ªÉ tƒÉng t·ªëc

- **Semantic summary generation ch·∫≠m / GPU utilization th·∫•p**:
  - **V·∫•n ƒë·ªÅ**: Code process t·ª´ng image m·ªôt, g√¢y CPU bottleneck v√† GPU idle time
  - **Gi·∫£i ph√°p ƒë√£ implement**:
    - ‚úÖ Parallel image loading (ThreadPoolExecutor) ƒë·ªÉ gi·∫£m I/O bottleneck
    - ‚úÖ Pre-loading next batch trong background (overlap I/O v·ªõi GPU computation)
    - ‚úÖ **Batch processing th·ª≠ nghi·ªám** (n·∫øu Qwen3-VL support, s·∫Ω t·ª± ƒë·ªông fallback n·∫øu kh√¥ng)
    - ‚úÖ Gi·∫£m `max_new_tokens` t·ª´ 128 xu·ªëng 64 (c√≥ th·ªÉ config)
    - ‚úÖ Pre-load all images option (nhanh h∆°n nh∆∞ng t·ªën RAM)
  - **Khuy·∫øn ngh·ªã ƒë·ªÉ tƒÉng t·ªëc**:
    ```bash
    # Option 1: TƒÉng batch size v√† gi·∫£m max tokens
    python data_prepare.py \
        --dataset_code beauty \
        --use_image \
        --generate_semantic_summary \
        --semantic_summary_batch_size 8 \
        --semantic_summary_max_tokens 64
    
    # Option 2: Pre-load all images (nhanh nh·∫•t nh∆∞ng t·ªën RAM)
    python data_prepare.py \
        --dataset_code beauty \
        --use_image \
        --generate_semantic_summary \
        --semantic_summary_batch_size 8 \
        --semantic_summary_max_tokens 64 \
        --preload_all_images
    
    # Option 3: K·∫øt h·ª£p t·∫•t c·∫£ optimizations
    python data_prepare.py \
        --dataset_code beauty \
        --use_image \
        --generate_semantic_summary \
        --semantic_summary_batch_size 16 \
        --semantic_summary_max_tokens 64 \
        --preload_all_images \
        --use_quantization \
        --use_torch_compile
    ```
  - **L∆∞u √Ω**: 
    - Qwen3-VL c√≥ th·ªÉ kh√¥ng support true batch processing cho multimodal inputs
    - Code s·∫Ω t·ª± ƒë·ªông th·ª≠ batch processing, n·∫øu fail s·∫Ω fallback v·ªÅ sequential
    - `--preload_all_images` t·ªën RAM nh∆∞ng lo·∫°i b·ªè ho√†n to√†n I/O bottleneck
    - Gi·∫£m `--semantic_summary_max_tokens` t·ª´ 64 xu·ªëng 32-48 n·∫øu c·∫ßn t·ªëc ƒë·ªô h∆°n n·ªØa (nh∆∞ng c√≥ th·ªÉ gi·∫£m ch·∫•t l∆∞·ª£ng)

- **LLM inference ch·∫≠m**:
  - S·ª≠ d·ª•ng `--use_torch_compile` ƒë·ªÉ compile model
  - Model ƒë√£ ƒë∆∞·ª£c load v·ªõi 4-bit quantization m·∫∑c ƒë·ªãnh (Unsloth)
  - C√≥ th·ªÉ batch multiple prompts n·∫øu c·∫ßn (xem OPTIMIZATION_GUIDE.md)

- **Evaluation kh√¥ng ch·∫°y ƒë∆∞·ª£c**: 
  - Ki·ªÉm tra xem ƒë√£ train model ch∆∞a
  - ƒê·∫£m b·∫£o dataset ƒë√£ ƒë∆∞·ª£c prepare v·ªõi ƒë√∫ng flags (--use_image, --generate_caption, etc.)
  - Ki·ªÉm tra `--qwen3vl_mode` c√≥ ƒë√∫ng v·ªõi mode ƒë√£ train kh√¥ng

