# Data Preprocessing Pipeline - Complete Guide

## ğŸ“š Má»¤C Lá»¤C

1. [Tá»•ng quan](#tá»•ng-quan)
2. [CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)
3. [Sá»­ dá»¥ng cÆ¡ báº£n](#sá»­-dá»¥ng-cÆ¡-báº£n)
4. [TÃ­nh nÄƒng lá»c dá»¯ liá»‡u](#tÃ­nh-nÄƒng-lá»c-dá»¯-liá»‡u)
5. [Download images](#download-images)
6. [Quy trÃ¬nh preprocessing](#quy-trÃ¬nh-preprocessing)
7. [Cáº¥u trÃºc dá»¯ liá»‡u](#cáº¥u-trÃºc-dá»¯-liá»‡u)
8. [Utilities](#utilities)

---

## ğŸ¯ Tá»”NG QUAN

Pipeline preprocessing cho recommendation system vá»›i kháº£ nÄƒng:
- âœ… Lá»c items theo text vÃ  image availability
- âœ… Download vÃ  verify images tá»± Ä‘á»™ng
- âœ… Parallel processing (20 threads) Ä‘á»ƒ tÄƒng tá»‘c
- âœ… Tá»‘i Æ°u thá»© tá»± lá»c Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian

### Datasets há»— trá»£:
- **Beauty**: Amazon Beauty products (text + image)
- **Games**: Video Games (text + image)
- **ML-100k**: MovieLens (chá»‰ text)

---

## ğŸ”§ CÃ€I Äáº¶T

### YÃªu cáº§u:
```bash
python >= 3.8
pytorch-lightning
pandas
numpy
tqdm
Pillow
pyyaml
```

### CÃ i Ä‘áº·t dependencies:
```bash
pip install torch pytorch-lightning pandas numpy tqdm Pillow pyyaml
```

---

## ğŸš€ Sá»¬ Dá»¤NG CÆ  Báº¢N

### 1. Preprocessing Ä‘Æ¡n giáº£n (khÃ´ng lá»c):
```bash
python data_prepare.py
```

### 2. Vá»›i filtering text:
```bash
python data_prepare.py --use_text
```

### 3. Vá»›i filtering text + download images:
```bash
python data_prepare.py --use_text --use_image
```

### 4. TÃ¹y chá»‰nh dataset:
```bash
python data_prepare.py --dataset_code games --min_uc 10 --min_sc 10
```

### CÃ¡c arguments:
- `--dataset_code`: beauty, games, ml-100k (default: beauty)
- `--min_rating`: Minimum rating Ä‘á»ƒ giá»¯ láº¡i (default: 3)
- `--min_uc`: Minimum ratings per user (default: 5)
- `--min_sc`: Minimum ratings per item (default: 5)
- `--use_text`: Lá»c items khÃ´ng cÃ³ text
- `--use_image`: Lá»c items khÃ´ng cÃ³ image + download images
- `--seed`: Random seed (default: 42)

---

## ğŸ¨ TÃNH NÄ‚NG Lá»ŒC Dá»® LIá»†U

### Metadata Structure:

```python
{
    'text': 'Title and description combined',  # None náº¿u khÃ´ng cÃ³
    'image': 'http://image-url.com/...',       # None náº¿u khÃ´ng cÃ³
    'image_path': 'data/.../images/item_1.jpg', # Local path (sau khi download)
    'title': 'Product title'
}
```

### Filtering Logic:

#### 1. Text Filtering (`--use_text`)
- Giá»¯ items cÃ³ text (title + description) há»£p lá»‡
- Loáº¡i bá» items cÃ³ text null hoáº·c rá»—ng

#### 2. Image Filtering (`--use_image`)
- Verify URL accessible
- Download image vÃ  verify format (PIL)
- Chá»‰ giá»¯ items download thÃ nh cÃ´ng
- **CHÃš Ã**: MovieLens khÃ´ng cÃ³ images!

### Káº¿t quáº£ Filtering (Beauty dataset):

| BÆ°á»›c | Sá»‘ items | Ghi chÃº |
|------|----------|---------|
| Total metadata | 259,204 | |
| After text filter | 258,992 | -212 items |
| After triplet filter | 12,101 | **-246,891 items!** |
| After image download | 11,800 | -300 items failed |

---

## ğŸ“¥ DOWNLOAD IMAGES

### Táº¡i sao nÃªn download images?

âœ… **Tá»‘c Ä‘á»™ training**: Äá»c local disk >> download internet  
âœ… **á»”n Ä‘á»‹nh**: KhÃ´ng phá»¥ thuá»™c vÃ o internet hoáº·c URL  
âœ… **Reproducibility**: Äáº£m báº£o cÃ¹ng dá»¯ liá»‡u má»—i láº§n  
âœ… **Tiáº¿t kiá»‡m bandwidth**: Chá»‰ download 1 láº§n  

### CÃ¡ch download Ä‘Æ°á»£c thá»±c hiá»‡n:

#### Vá»ªA CHECK Vá»ªA DOWNLOAD (1 request/image):
```python
download_and_verify_images_batch()
â”œâ”€ Download image tá»« URL
â”œâ”€ Verify báº±ng PIL (format, integrity)
â”œâ”€ LÆ°u vÃ o local disk
â””â”€ Return valid items
```

#### Parallel Processing:
- **20 threads** cháº¡y Ä‘á»“ng thá»i
- ~17 images/giÃ¢y (tÃ¹y network)
- 12,000 images â†’ **~12 phÃºt**

#### Äáº·t tÃªn file:
```
item_{item_id}_{url_hash}.{ext}
VÃ­ dá»¥: item_1_aeb6393c.jpg
```

### Cáº¥u trÃºc lÆ°u trá»¯:

```
data/
â””â”€â”€ preprocessed/
    â””â”€â”€ beauty_min_rating3-min_uc5-min_sc5/
        â”œâ”€â”€ dataset.pkl              # Metadata + mappings
        â””â”€â”€ images/                  # Downloaded images
            â”œâ”€â”€ item_1_aeb6393c.jpg
            â”œâ”€â”€ item_2_9179de12.jpg
            â””â”€â”€ ... (11,800 files)
```

---

## âš¡ QUY TRÃŒNH PREPROCESSING (Tá»I Æ¯U)

### Thá»© tá»± cÃ¡c bÆ°á»›c:

```
1. Load metadata (~259k items)
   â†“
2. Lá»c TEXT (nhanh - < 1s)
   â†“ ~258k items
3. Lá»c TRIPLETS (min_uc, min_sc)
   â†“ ~12k items â­ GIáº¢M 95%!
4. Densify Index (táº¡o mapping má»›i)
   â†“
5. DOWNLOAD IMAGES (chá»‰ 12k thay vÃ¬ 259k!)
   â†“ ~11.8k items
6. Lá»c láº¡i náº¿u download failed
   â†“
7. Split train/val/test & LÆ°u dataset
```

### Táº¡i sao thá»© tá»± nÃ y tá»‘i Æ°u?

âŒ **CÃ¡ch CÅ¨** (KHÃ”NG tá»‘i Æ°u):
```
Download 259k images â†’ Lá»c triplets â†’ CÃ²n 12k
â†’ ÄÃƒ DOWNLOAD THá»ªA 247k IMAGES! (lÃ£ng phÃ­ 95%)
Time: 5-10 giá»
```

âœ… **CÃ¡ch Má»šI** (Tá»‘i Æ°u):
```
Lá»c triplets â†’ CÃ²n 12k â†’ Download 12k images
â†’ TIáº¾T KIá»†M 95% THá»œI GIAN!
Time: 10-20 phÃºt
```

### Chi tiáº¿t tá»«ng bÆ°á»›c:

#### BÆ°á»›c 1: Load Metadata
```python
meta_raw = self.load_meta_dict()
# â†’ 259,204 items vá»›i text + image URL
```

#### BÆ°á»›c 2: Lá»c Text (nhanh)
```python
if self.args.use_text:
    valid_text_items = {id for id, meta in meta_raw.items() 
                       if meta['text']}
# â†’ 258,992 items
```

#### BÆ°á»›c 3: Lá»c Triplets (quan trá»ng!)
```python
df = self.filter_triplets(df)
# Lá»c users cÃ³ < min_uc ratings
# Lá»c items cÃ³ < min_sc ratings
# â†’ 12,101 items (GIáº¢M 95%!)
```

#### BÆ°á»›c 4: Densify Index
```python
df, umap, smap = self.densify_index(df)
remaining_items = set(smap.keys())
# â†’ XÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c items nÃ o cáº§n download
```

#### BÆ°á»›c 5: Download Images
```python
downloaded_images, valid_items = download_and_verify_images_batch(
    items_to_download,  # CHá»ˆ 12k items!
    image_folder,
    max_workers=20
)
# â†’ 11,800 images (300 failed)
```

---

## ğŸ“Š Cáº¤U TRÃšC Dá»® LIá»†U

### Dataset pickle file:

```python
dataset = {
    'train': {
        1: [321, 4001, 4344, 8730],  # user_id â†’ list of item_ids
        2: [4293, 8184, 6173],
        ...
    },
    'val': {
        1: [8785],  # Second-to-last item
        2: [2993],
        ...
    },
    'test': {
        1: [11063],  # Last item
        2: [2802],
        ...
    },
    'meta': {
        1: {  # NEW item_id (after densify)
            'text': 'Product description...',
            'image': 'http://original-url.com/...',
            'image_path': 'data/.../images/item_1_abc.jpg',
            'title': 'Product title'
        },
        ...
    },
    'umap': {
        'A1BKSLDI2V3D5K': 1,  # original_user_id â†’ new_user_id
        ...
    },
    'smap': {
        'B00ABC123': 1,  # original_item_id â†’ new_item_id
        ...
    }
}
```

### Sá»­ dá»¥ng trong training:

```python
import pickle
from PIL import Image
from torchvision import transforms

# Load dataset
with open('data/preprocessed/.../dataset.pkl', 'rb') as f:
    dataset = pickle.load(f)

# Get training data
user_items = dataset['train'][user_id]

# Load image
for item_id in user_items:
    img_path = dataset['meta'][item_id]['image_path']
    img = Image.open(img_path).convert('RGB')
    
    # Transform
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], 
                           [0.229, 0.224, 0.225])
    ])
    img_tensor = transform(img)
```

---

## ğŸ› ï¸ UTILITIES

Chi tiáº¿t Ä‘áº§y Ä‘á»§ táº¡i: **`tools/README.md`**

### 1. Xem cáº¥u trÃºc dataset:
```bash
python tools/inspect_pickle.py
```

Output:
- Sá»‘ users, items
- PhÃ¢n tÃ­ch metadata (text/image availability)
- Sample data

### 2. Kiá»ƒm tra filtering results:
```bash
python tools/test_filtering.py
```

Vá»›i arguments:
```bash
python tools/test_filtering.py --use_text --use_image
```

### 3. Kiá»ƒm tra downloaded images:
```bash
python tools/test_download_images.py
```

Output:
- Sá»‘ images downloaded
- Tá»•ng size
- Sample images vá»›i paths

### 4. XÃ³a preprocessed data cÅ©:
```bash
python tools/clean_preprocessed.py
```

Há»¯u Ã­ch khi:
- Thay Ä‘á»•i settings (min_uc, min_sc, etc.)
- Muá»‘n re-preprocess tá»« Ä‘áº§u

---

## ğŸ“ˆ HIá»†U NÄ‚NG & THá»NG KÃŠ

### Beauty Dataset (Thá»±c táº¿):

| Metric | Value |
|--------|-------|
| Total items in metadata | 259,204 |
| After text filter | 258,992 |
| After triplet filter | 12,101 |
| Downloaded images | 11,800 |
| Final users | 22,332 |
| Total size (images) | ~80 MB |
| Download time | 10-15 phÃºt |

### So sÃ¡nh hiá»‡u suáº¥t:

| PhÆ°Æ¡ng phÃ¡p | Items cáº§n download | Thá»i gian |
|-------------|-------------------|-----------|
| Download trÆ°á»›c | 259,204 | 5-10 giá» |
| **Download sau (Tá»‘i Æ°u)** | **12,101** | **10-20 phÃºt** |
| **Tiáº¿t kiá»‡m** | **95.3%** | **96%** |

---

## ğŸ” TROUBLESHOOTING

### Lá»—i: "ModuleNotFoundError: No module named 'PIL'"
```bash
pip install Pillow
```

### Lá»—i: Download quÃ¡ cháº­m
- Giáº£m `max_workers` xuá»‘ng 10
- TÄƒng `timeout` lÃªn 30s

### Dataset quÃ¡ nhá» sau filtering
- Giáº£m `min_uc` vÃ  `min_sc`
- Bá» `--use_image` náº¿u khÃ´ng cáº§n thiáº¿t

### Images bá»‹ corrupt
- HÃ m `download_image()` Ä‘Ã£ verify báº±ng PIL
- Náº¿u váº«n cÃ³ váº¥n Ä‘á», xÃ³a folder images vÃ  download láº¡i

---

## ğŸ“ NOTES

### Best Practices:

1. **LuÃ´n lá»c triplets TRÆ¯á»šC khi download images**
   - Tiáº¿t kiá»‡m 95% thá»i gian vÃ  bandwidth

2. **Sá»­ dá»¥ng `--use_text` náº¿u cáº§n text data**
   - Loáº¡i bá» items khÃ´ng cÃ³ description

3. **Chá»‰ dÃ¹ng `--use_image` khi thá»±c sá»± cáº§n**
   - Download máº¥t thá»i gian
   - MovieLens khÃ´ng cÃ³ images

4. **Giá»¯ láº¡i URL gá»‘c trong metadata**
   - Backup náº¿u cáº§n download láº¡i
   - Debug khi cÃ³ váº¥n Ä‘á»

5. **Pre-resize images náº¿u dataset lá»›n**
   - Faster training
   - Consistent input size

---

## ğŸ“ SUPPORT

Náº¿u cÃ³ váº¥n Ä‘á»:
1. Kiá»ƒm tra logs khi cháº¡y `data_prepare.py`
2. DÃ¹ng utility scripts Ä‘á»ƒ debug
3. Xem cÃ¡c file .md Ä‘á»ƒ hiá»ƒu chi tiáº¿t

---

**Version**: 2.0  
**Last Updated**: December 2025  
**Author**: Data Preprocessing Pipeline Team
