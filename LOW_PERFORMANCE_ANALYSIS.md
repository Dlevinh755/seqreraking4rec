# PhÃ¢n tÃ­ch: Táº¡i sao káº¿t quáº£ tháº¥p?

## ğŸ“Š Káº¿t quáº£ hiá»‡n táº¡i

- **Val Recall@1**: 0.0179 (1.79%) - Ráº¥t tháº¥p
- **Test Recall@1**: 0.0212 (2.12%) - Ráº¥t tháº¥p
- **Val NDCG@10**: 0.0875
- **Test NDCG@10**: 0.1019
- **Training Loss**: 4.25 (Ráº¥t cao, thÆ°á»ng nÃªn < 2)

## ğŸ” NguyÃªn nhÃ¢n chÃ­nh

### 1. **QuÃ¡ Ã­t Training Data vÃ  Epochs** âš ï¸ CRITICAL

**Váº¥n Ä‘á»**:
- **Training samples**: 614 (ráº¥t Ã­t)
- **Epochs**: 1 (quÃ¡ Ã­t)
- **Training steps**: 5 (614 / 128 batch size = ~5 steps)
- **Training loss**: 4.25 (ráº¥t cao, model chÆ°a há»c Ä‘Æ°á»£c gÃ¬)

**PhÃ¢n tÃ­ch**:
- Vá»›i 614 samples vÃ  1 epoch, model chá»‰ tháº¥y má»—i sample 1 láº§n
- 5 steps lÃ  quÃ¡ Ã­t Ä‘á»ƒ model há»c Ä‘Æ°á»£c pattern
- Loss 4.25 cho tháº¥y model gáº§n nhÆ° random (cross-entropy vá»›i 50 classes â‰ˆ -log(1/50) â‰ˆ 3.9)

**Giáº£i phÃ¡p**:
```python
# TÄƒng epochs trong config.py
--rerank_epochs 10  # Thay vÃ¬ 1

# Hoáº·c trong code
num_epochs = 10  # Thay vÃ¬ 1
```

### 2. **Learning Rate khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng tá»« Config** âš ï¸

**Váº¥n Ä‘á»**:
- Code hardcode `learning_rate=2e-5` trong `SFTConfig` (line 228)
- Config cÃ³ `--rerank_lr=1e-4` nhÆ°ng khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng
- Learning rate 2e-5 cÃ³ thá»ƒ quÃ¡ tháº¥p cho fine-tuning

**Code hiá»‡n táº¡i** (`rerank/models/llm.py:228`):
```python
training_args = SFTConfig(
    ...
    learning_rate=2e-5,  # âŒ Hardcoded, khÃ´ng dÃ¹ng tá»« config
    ...
)
```

**Giáº£i phÃ¡p**:
```python
# Láº¥y learning rate tá»« config
try:
    from config import arg
    lr = getattr(arg, 'rerank_lr', 2e-5)
except ImportError:
    lr = 2e-5

training_args = SFTConfig(
    ...
    learning_rate=lr,  # âœ… DÃ¹ng tá»« config
    ...
)
```

### 3. **Dataset quÃ¡ nhá»** âš ï¸

**Váº¥n Ä‘á»**:
- **Users**: 614 (ráº¥t Ã­t)
- **Items**: 474 (ráº¥t Ã­t)
- **Training samples**: 614 (1 sample/user)
- **Diversity**: Tháº¥p, model khÃ³ há»c pattern

**PhÃ¢n tÃ­ch**:
- Vá»›i dataset nhá», model dá»… overfit hoáº·c underfit
- Cáº§n nhiá»u epochs hÆ¡n Ä‘á»ƒ model há»c Ä‘Æ°á»£c pattern
- CÃ³ thá»ƒ cáº§n data augmentation hoáº·c transfer learning

### 4. **Model quÃ¡ nhá»** âš ï¸

**Váº¥n Ä‘á»**:
- **Model**: Qwen3-0.6B (600M parameters)
- **Task**: Rerank 50 candidates (khÃ¡ phá»©c táº¡p)
- **Capacity**: CÃ³ thá»ƒ khÃ´ng Ä‘á»§ Ä‘á»ƒ há»c pattern tá»‘t

**Giáº£i phÃ¡p**:
- Thá»­ model lá»›n hÆ¡n: `qwen3-1.6b` hoáº·c `qwen3-2b`
- Hoáº·c tÄƒng LoRA rank Ä‘á»ƒ tÄƒng capacity

### 5. **Training Loss quÃ¡ cao** âš ï¸

**Váº¥n Ä‘á»**:
- **Training loss**: 4.25 (ráº¥t cao)
- **Expected loss**: < 2.0 (cho 50 classes)
- **Random baseline**: ~3.9 (-log(1/50))

**PhÃ¢n tÃ­ch**:
- Loss 4.25 gáº§n vá»›i random (3.9)
- Model chÆ°a há»c Ä‘Æ°á»£c gÃ¬ há»¯u Ã­ch
- Cáº§n nhiá»u epochs Ä‘á»ƒ loss giáº£m xuá»‘ng

### 6. **Format má»›i (Letters) cÃ³ thá»ƒ chÆ°a Ä‘Æ°á»£c train Ä‘á»§** âš ï¸

**Váº¥n Ä‘á»**:
- Má»›i chuyá»ƒn tá»« numbers sang letters
- Model cáº§n há»c láº¡i cÃ¡ch predict letters
- Vá»›i 1 epoch, model chÆ°a ká»‹p há»c

**Giáº£i phÃ¡p**:
- TÄƒng epochs Ä‘á»ƒ model há»c Ä‘Æ°á»£c letter prediction
- Hoáº·c thá»­ láº¡i vá»›i numbers Ä‘á»ƒ so sÃ¡nh

### 7. **Chat Template Format má»›i** âš ï¸

**Váº¥n Ä‘á»**:
- Má»›i sá»­a Ä‘á»ƒ dÃ¹ng chat template format cho inference
- Training vÃ  inference giá» Ä‘Ã£ consistent
- NhÆ°ng model cáº§n thá»i gian Ä‘á»ƒ há»c format má»›i

**Giáº£i phÃ¡p**:
- TÄƒng epochs Ä‘á»ƒ model há»c Ä‘Æ°á»£c format má»›i
- Monitor training loss Ä‘á»ƒ Ä‘áº£m báº£o Ä‘ang giáº£m

## ğŸ¯ Giáº£i phÃ¡p Ä‘á» xuáº¥t

### **Priority 1: TÄƒng Epochs** (QUAN TRá»ŒNG NHáº¤T)

```bash
# Sá»­a config.py hoáº·c command line
--rerank_epochs 10  # Thay vÃ¬ 1
```

**LÃ½ do**:
- 1 epoch lÃ  quÃ¡ Ã­t, model chÆ°a ká»‹p há»c
- Vá»›i 614 samples, cáº§n Ã­t nháº¥t 5-10 epochs
- Loss 4.25 cho tháº¥y model chÆ°a converge

### **Priority 2: Sá»­a Learning Rate tá»« Config**

**Sá»­a code** (`rerank/models/llm.py:228`):
```python
# Láº¥y learning rate tá»« config
try:
    from config import arg
    lr = getattr(arg, 'rerank_lr', 2e-5)
except ImportError:
    lr = 2e-5

training_args = SFTConfig(
    ...
    learning_rate=lr,  # âœ… DÃ¹ng tá»« config (default: 1e-4)
    ...
)
```

**LÃ½ do**:
- Config cÃ³ `--rerank_lr=1e-4` nhÆ°ng khÃ´ng Ä‘Æ°á»£c dÃ¹ng
- 1e-4 thÆ°á»ng tá»‘t hÆ¡n 2e-5 cho fine-tuning
- Cáº§n consistency giá»¯a config vÃ  code

### **Priority 3: TÄƒng Batch Size (náº¿u GPU cho phÃ©p)**

```bash
--rerank_batch_size 32  # Thay vÃ¬ 16
```

**LÃ½ do**:
- Batch size lá»›n hÆ¡n â†’ training á»•n Ä‘á»‹nh hÆ¡n
- Gradient accumulation steps = 4, nÃªn effective batch = 128
- CÃ³ thá»ƒ tÄƒng lÃªn 64 náº¿u GPU memory cho phÃ©p

### **Priority 4: Monitor Training Loss**

**Kiá»ƒm tra**:
- Training loss cÃ³ giáº£m khÃ´ng?
- Náº¿u khÃ´ng giáº£m â†’ learning rate quÃ¡ tháº¥p hoáº·c model khÃ´ng há»c Ä‘Æ°á»£c
- Náº¿u giáº£m quÃ¡ nhanh â†’ cÃ³ thá»ƒ overfit

### **Priority 5: Thá»­ Model lá»›n hÆ¡n**

```bash
--qwen_model qwen3-1.6b  # Thay vÃ¬ qwen3-0.6b
```

**LÃ½ do**:
- Model lá»›n hÆ¡n cÃ³ capacity tá»‘t hÆ¡n
- CÃ³ thá»ƒ há»c Ä‘Æ°á»£c pattern phá»©c táº¡p hÆ¡n
- Trade-off: cháº­m hÆ¡n, tá»‘n memory hÆ¡n

## ğŸ“ˆ Expected Results sau khi sá»­a

**Vá»›i epochs=10, lr=1e-4**:
- Training loss: < 2.0 (sau 10 epochs)
- Val Recall@1: > 0.05 (5%)
- Test Recall@1: > 0.05 (5%)
- NDCG@10: > 0.15

**Vá»›i epochs=20, lr=1e-4, model lá»›n hÆ¡n**:
- Training loss: < 1.5
- Val Recall@1: > 0.10 (10%)
- Test Recall@1: > 0.10 (10%)
- NDCG@10: > 0.20

## ğŸ”§ Quick Fix

**Command line**:
```bash
python scripts/train_rerank_standalone.py \
    --rerank_method qwen3vl \
    --rerank_mode ground_truth \
    --rerank_epochs 10 \
    --rerank_lr 1e-4 \
    --rerank_batch_size 32
```

**Hoáº·c sá»­a config.py**:
```python
parser.add_argument('--rerank_epochs', type=int, default=10,  # Thay vÃ¬ 1
parser.add_argument('--rerank_lr', type=float, default=1e-4,  # ÄÃ£ Ä‘Ãºng
parser.add_argument('--rerank_batch_size', type=int, default=32,  # Thay vÃ¬ 16
```

## ğŸ“ Káº¿t luáº­n

**NguyÃªn nhÃ¢n chÃ­nh**: 
1. **QuÃ¡ Ã­t epochs (1)** â†’ Model chÆ°a ká»‹p há»c
2. **Learning rate khÃ´ng Ä‘Æ°á»£c dÃ¹ng tá»« config** â†’ CÃ³ thá»ƒ khÃ´ng tá»‘i Æ°u
3. **Training loss cao (4.25)** â†’ Model gáº§n nhÆ° random

**Giáº£i phÃ¡p**:
1. âœ… TÄƒng epochs lÃªn 10-20
2. âœ… Sá»­a code Ä‘á»ƒ dÃ¹ng learning rate tá»« config
3. âœ… Monitor training loss Ä‘á»ƒ Ä‘áº£m báº£o Ä‘ang giáº£m
4. âš ï¸ CÃ¢n nháº¯c model lá»›n hÆ¡n náº¿u váº«n tháº¥p

