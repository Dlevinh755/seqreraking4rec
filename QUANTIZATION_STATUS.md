# 4-bit Quantization Status Report

## ğŸ“Š Tá»•ng quan

BÃ¡o cÃ¡o nÃ y kiá»ƒm tra xem cÃ¡c models Ä‘Ã£ Ä‘Æ°á»£c load vá»›i 4-bit quantization chÆ°a.

## âœ… Models Ä‘Ã£ cÃ³ 4-bit quantization (Máº·c Ä‘á»‹nh) - Táº¤T Cáº¢ UNSLOTH MODELS

### 1. LLM Model (`rerank/models/llm.py`)
- **Status**: âœ… **ÄÃ£ cÃ³ 4-bit máº·c Ä‘á»‹nh** (Unsloth)
- **Location**: Line 110
- **Code**:
  ```python
  print(f"Loading LLM model with 4-bit quantization: {self.model_name}")
  self.model, self.tokenizer = FastLanguageModel.from_pretrained(
      model_name = self.model_name,
      max_seq_length = 2048,
      dtype = torch.float16,
      load_in_4bit = True,  # âœ… 4-bit máº·c Ä‘á»‹nh cho táº¥t cáº£ Unsloth models
  )
  ```
- **Model**: `Qwen/Qwen3-0.6B` (via Unsloth)
- **Usage**: Qwen reranker inference
- **Note**: Táº¥t cáº£ models load báº±ng Unsloth Ä‘á»u cÃ³ 4-bit máº·c Ä‘á»‹nh

### 2. Qwen3-VL Text Model - `semantic_summary_small` mode (`rerank/models/qwen3vl.py`)
- **Status**: âœ… **ÄÃ£ cÃ³ 4-bit máº·c Ä‘á»‹nh** (Unsloth)
- **Location**: Line 129
- **Code**:
  ```python
  print(f"Loading Qwen text model with 4-bit quantization: {self.model_name}")
  self.model, self.tokenizer = FastLanguageModel.from_pretrained(
      model_name=self.model_name,  # "unsloth/Qwen3-0.6B-unsloth-bnb-4bit"
      max_seq_length=2048,
      dtype=torch.float16,
      load_in_4bit=True,  # âœ… 4-bit máº·c Ä‘á»‹nh cho táº¥t cáº£ Unsloth models
  )
  ```
- **Model**: `unsloth/Qwen3-0.6B-unsloth-bnb-4bit`
- **Usage**: Qwen3-VL reranker vá»›i `semantic_summary_small` mode
- **Note**: Táº¥t cáº£ models load báº±ng Unsloth Ä‘á»u cÃ³ 4-bit máº·c Ä‘á»‹nh

## âŒ Models chÆ°a cÃ³ 4-bit quantization máº·c Ä‘á»‹nh

### 1. Qwen3-VL Model cho Semantic Summary Generation (`dataset/qwen3vl_semantic_summary.py`)
- **Status**: âŒ **ChÆ°a cÃ³ 4-bit máº·c Ä‘á»‹nh** (chá»‰ khi `--use_quantization` flag Ä‘Æ°á»£c set)
- **Location**: Lines 67-87
- **Current Code**:
  ```python
  quantization_config = None
  if use_quantization and device.type == "cuda":  # âŒ Chá»‰ khi flag Ä‘Æ°á»£c set
      try:
          from transformers import BitsAndBytesConfig
          quantization_config = BitsAndBytesConfig(
              load_in_4bit=True,
              ...
          )
  ```
- **Model**: `unsloth/Qwen3-VL-2B-Instruct`
- **Usage**: Generate semantic summaries trong `data_prepare.py`
- **Note**: Cáº§n set `--use_quantization` flag Ä‘á»ƒ enable 4-bit

### 2. Qwen3-VL Model cho Reranking - VL modes (`rerank/models/qwen3vl.py`)
- **Status**: âŒ **ChÆ°a cÃ³ 4-bit máº·c Ä‘á»‹nh**
- **Location**: Lines 96-119
- **Current Code**:
  ```python
  self.model = Qwen3VLForConditionalGeneration.from_pretrained(
      self.model_name,  # "unsloth/Qwen3-VL-2B-Instruct"
      dtype="auto" if self.device.type == "cuda" else torch.float32,
      device_map="auto" if self.device.type == "cuda" else None,
      trust_remote_code=True,
      # âŒ KhÃ´ng cÃ³ quantization_config
  )
  ```
- **Models**: 
  - `unsloth/Qwen3-VL-2B-Instruct` (cho `raw_image`, `caption`, `semantic_summary` modes)
- **Usage**: Qwen3-VL reranker vá»›i cÃ¡c VL modes

## ğŸ”§ Khuyáº¿n nghá»‹

### Option 1: Enable 4-bit máº·c Ä‘á»‹nh cho Semantic Summary Generation
- **LÃ½ do**: Semantic summary generation khÃ´ng cáº§n Ä‘á»™ chÃ­nh xÃ¡c cao, 4-bit sáº½ tiáº¿t kiá»‡m memory Ä‘Ã¡ng ká»ƒ
- **Action**: Set `use_quantization=True` máº·c Ä‘á»‹nh trong `_load_qwen3vl_model()`

### Option 2: Giá»¯ nguyÃªn (Optional vá»›i flag)
- **LÃ½ do**: Cho phÃ©p user lá»±a chá»n giá»¯a speed/memory vÃ  accuracy
- **Action**: Giá»¯ nguyÃªn, user cáº§n set `--use_quantization` flag

### Option 3: Enable 4-bit cho Qwen3-VL Reranker
- **LÃ½ do**: Reranking cáº§n Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n, nhÆ°ng 4-bit váº«n cÃ³ thá»ƒ acceptable
- **Action**: ThÃªm quantization support cho VL modes trong `rerank/models/qwen3vl.py`

## ğŸ“ Summary

| Model | Location | 4-bit Status | Notes |
|-------|----------|---------------|-------|
| **LLM (Qwen reranker)** | `rerank/models/llm.py` | âœ… **Máº·c Ä‘á»‹nh** | **Unsloth - 4-bit enabled** |
| **Qwen3-VL Text (small)** | `rerank/models/qwen3vl.py` | âœ… **Máº·c Ä‘á»‹nh** | **Unsloth - 4-bit enabled** |
| Qwen3-VL (semantic summary) | `dataset/qwen3vl_semantic_summary.py` | âš ï¸ Optional | Cáº§n `--use_quantization` flag (khÃ´ng dÃ¹ng Unsloth) |
| Qwen3-VL (reranker VL modes) | `rerank/models/qwen3vl.py` | âŒ ChÆ°a cÃ³ | KhÃ´ng dÃ¹ng Unsloth (dÃ¹ng transformers) |

## âœ… Káº¿t luáº­n

**Táº¤T Cáº¢ MODELS LOAD Báº°NG UNSLOTH ÄÃƒ CÃ“ 4-BIT QUANTIZATION Máº¶C Äá»ŠNH**

- âœ… `rerank/models/llm.py`: FastLanguageModel.from_pretrained() vá»›i `load_in_4bit=True`
- âœ… `rerank/models/qwen3vl.py`: FastLanguageModel.from_pretrained() vá»›i `load_in_4bit=True`

CÃ¡c models khÃ´ng dÃ¹ng Unsloth (Qwen3-VL vá»›i transformers) cÃ³ thá»ƒ enable 4-bit qua `--use_quantization` flag.

