# PhÃ¢n tÃ­ch VIP5 Training vÃ  Evaluation

## âœ… Nhá»¯ng gÃ¬ Ä‘ang hoáº¡t Ä‘á»™ng ÄÃšNG

### 1. **Training Loop** âœ…
- CÃ³ training loop Ä‘áº§y Ä‘á»§ vá»›i epochs, batches
- Loss calculation Ä‘Ãºng: reshape loss, mask padding tokens, apply loss weights
- CÃ³ validation vÃ  early stopping
- CÃ³ optimizer vÃ  gradient clipping
- Training format Ä‘Ãºng: Direct Task (B-5) template

### 2. **Loss Calculation** âœ…
- Loss Ä‘Æ°á»£c tÃ­nh Ä‘Ãºng: `reduce_loss=False` Ä‘á»ƒ láº¥y per-token loss
- Mask padding tokens: `target_mask = (target_ids != -100).float()`
- Per-sample loss: `(loss * target_mask).sum(dim=1) / target_mask.sum(dim=1)`
- Apply loss weights: `(per_sample_loss * loss_weights).mean()`
- Giá»‘ng vá»›i reference implementation trong `retrieval/vip5_temp/src/model.py`

### 3. **Evaluation Setup** âœ…
- CÃ³ `_evaluate_split()` method
- Load pre-generated candidates tá»« `evaluation.utils.load_rerank_candidates`
- Recall calculation Ä‘Ãºng: `hits / len(gt_items)` (cÃ´ng thá»©c chuáº©n)
- Batch processing Ä‘á»ƒ tá»‘i Æ°u tá»‘c Ä‘á»™

### 4. **Model Initialization** âœ…
- Load checkpoint náº¿u cÃ³
- Initialize tá»« T5 backbone náº¿u khÃ´ng cÃ³ checkpoint
- Adapter support (default: enabled)
- Visual features Ä‘Æ°á»£c load Ä‘Ãºng

---

## âš ï¸ Váº¥n Ä‘á» tiá»m áº©n cáº§n kiá»ƒm tra

### 1. **Scoring Logic trong Rerank() - CÃ“ THá»‚ SAI** âš ï¸

**Location**: `rerank/methods/vip5_reranker.py:655-679` vÃ  `1078-1102`

**Váº¥n Ä‘á»**:
- Code Ä‘ang láº¥y logit táº¡i position `first_token_idx` Ä‘á»ƒ predict token táº¡i position `first_token_idx`
- NhÆ°ng trong seq2seq T5, logit táº¡i position `t` dá»± Ä‘oÃ¡n token táº¡i position `t+1`
- Decoder input: `[pad_token, item_token1, item_token2, ...]`
- Logit táº¡i position 0 dá»± Ä‘oÃ¡n token táº¡i position 1
- Logit táº¡i position 1 dá»± Ä‘oÃ¡n token táº¡i position 2

**Code hiá»‡n táº¡i**:
```python
# Line 668-669
if first_token_idx < logits.size(1) - 1:
    score = float(logits[i, first_token_idx, item_token_id].item())
```

**Váº¥n Ä‘á»**: 
- `decoder_input_ids[i, first_token_idx]` lÃ  token táº¡i position `first_token_idx`
- NhÆ°ng `logits[i, first_token_idx, ...]` dá»± Ä‘oÃ¡n token táº¡i position `first_token_idx + 1`
- NÃªn láº¥y logit táº¡i position `first_token_idx` Ä‘á»ƒ predict token táº¡i position `first_token_idx + 1`

**Giáº£i phÃ¡p Ä‘á» xuáº¥t**:
```python
# Option 1: Láº¥y logit táº¡i position 0 Ä‘á»ƒ predict token táº¡i position 1 (first token cá»§a item)
# Decoder input: [pad_token, item_token1, item_token2, ...]
# Logit[0] predicts item_token1
first_token_idx = 0  # Always use position 0
item_token_id = decoder_input_ids[i, 1].item()  # Token táº¡i position 1
score = float(logits[i, 0, item_token_id].item())  # Logit táº¡i position 0

# Option 2: Sum logits cho táº¥t cáº£ tokens cá»§a item_id
# Láº¥y táº¥t cáº£ tokens cá»§a "item_{item_id}" vÃ  sum logits
item_tokens = decoder_input_ids[i][decoder_attention_mask[i] == 1]  # All non-padding tokens
scores = []
for pos in range(len(item_tokens) - 1):  # -1 vÃ¬ logit táº¡i pos predicts token táº¡i pos+1
    token_id = item_tokens[pos + 1].item()
    logit = logits[i, pos, token_id].item()
    scores.append(logit)
score = sum(scores) / len(scores)  # Average logit
```

### 2. **Decoder Input Format trong Inference** âš ï¸

**Váº¥n Ä‘á»**:
- Trong training, decoder_input_ids Ä‘Æ°á»£c tá»± Ä‘á»™ng shift tá»« labels (thÃªm pad_token_id á»Ÿ Ä‘áº§u)
- Trong inference (rerank), code Ä‘ang pass decoder_input_ids trá»±c tiáº¿p tá»« tokenizer
- Tokenizer cÃ³ thá»ƒ khÃ´ng thÃªm pad_token_id á»Ÿ Ä‘áº§u

**Kiá»ƒm tra**:
- T5 tokenizer thÆ°á»ng thÃªm pad_token_id á»Ÿ Ä‘áº§u khi tokenize vá»›i `add_special_tokens=True`
- NhÆ°ng code Ä‘ang dÃ¹ng `add_special_tokens=False` (line 627, 1055)
- Cáº§n Ä‘áº£m báº£o decoder_input_ids báº¯t Ä‘áº§u vá»›i pad_token_id

**Giáº£i phÃ¡p**:
```python
# Äáº£m báº£o decoder_input_ids báº¯t Ä‘áº§u vá»›i pad_token_id
decoder_input_ids = decoder_inputs_tokenized["input_ids"].to(self.device)
pad_token_id = self.tokenizer.pad_token_id
# Prepend pad_token_id náº¿u chÆ°a cÃ³
if decoder_input_ids[0, 0] != pad_token_id:
    pad_tokens = torch.full((decoder_input_ids.size(0), 1), pad_token_id, device=self.device)
    decoder_input_ids = torch.cat([pad_tokens, decoder_input_ids], dim=1)
```

### 3. **Training Sample Preparation** âœ… (CÃ³ thá»ƒ cáº£i thiá»‡n)

**Hiá»‡n táº¡i**:
- Target item lÃ  item cuá»‘i cÃ¹ng trong history: `target_item = items[-1]`
- Sample negatives tá»« táº¥t cáº£ items (trá»« user history)
- Shuffle candidates Ä‘á»ƒ trÃ¡nh bias

**CÃ³ thá»ƒ cáº£i thiá»‡n**:
- CÃ³ thá»ƒ thá»­ cÃ¡c strategies khÃ¡c: random target tá»« history, hoáº·c target tá»« middle
- NhÆ°ng hiá»‡n táº¡i Ä‘Ã£ Ä‘Ãºng vá»›i sequential recommendation

---

## ğŸ” Kiá»ƒm tra cáº§n thá»±c hiá»‡n

### 1. **Kiá»ƒm tra Decoder Input Format**
```python
# Trong rerank(), sau khi tokenize:
print(f"Decoder input IDs shape: {decoder_input_ids.shape}")
print(f"First token IDs: {decoder_input_ids[0, :5]}")
print(f"Pad token ID: {self.tokenizer.pad_token_id}")
# Äáº£m báº£o decoder_input_ids[0, 0] == pad_token_id
```

### 2. **Kiá»ƒm tra Scoring Logic**
```python
# So sÃ¡nh 2 cÃ¡ch tÃ­nh score:
# CÃ¡ch 1: Hiá»‡n táº¡i (cÃ³ thá»ƒ sai)
score1 = logits[i, first_token_idx, item_token_id]

# CÃ¡ch 2: Äá» xuáº¥t (sum logits cho táº¥t cáº£ tokens)
item_tokens = decoder_input_ids[i][decoder_attention_mask[i] == 1]
scores = []
for pos in range(len(item_tokens) - 1):
    token_id = item_tokens[pos + 1].item()
    logit = logits[i, pos, token_id].item()
    scores.append(logit)
score2 = sum(scores) / len(scores)

# So sÃ¡nh score1 vs score2
```

### 3. **Kiá»ƒm tra Training vs Inference Consistency**
- Training: decoder_input_ids Ä‘Æ°á»£c shift tá»« labels
- Inference: decoder_input_ids Ä‘Æ°á»£c tokenize trá»±c tiáº¿p
- Äáº£m báº£o format giá»‘ng nhau

---

## ğŸ“Š TÃ³m táº¯t

| Aspect | Status | Notes |
|--------|--------|-------|
| **Training Loop** | âœ… ÄÃºng | CÃ³ Ä‘áº§y Ä‘á»§ epochs, batches, validation |
| **Loss Calculation** | âœ… ÄÃºng | Giá»‘ng reference implementation |
| **Evaluation Setup** | âœ… ÄÃºng | Load candidates, tÃ­nh recall Ä‘Ãºng |
| **Scoring Logic** | âš ï¸ CÃ³ thá»ƒ sai | Cáº§n kiá»ƒm tra decoder input format vÃ  logit indexing |
| **Decoder Input Format** | âš ï¸ Cáº§n kiá»ƒm tra | Äáº£m báº£o cÃ³ pad_token_id á»Ÿ Ä‘áº§u |

---

## ğŸ¯ Äá» xuáº¥t sá»­a

### Priority 1: Sá»­a Scoring Logic
1. Kiá»ƒm tra decoder input format (cÃ³ pad_token_id á»Ÿ Ä‘áº§u khÃ´ng)
2. Sá»­a cÃ¡ch láº¥y logit: láº¥y logit táº¡i position `t` Ä‘á»ƒ predict token táº¡i position `t+1`
3. Hoáº·c sum logits cho táº¥t cáº£ tokens cá»§a item_id

### Priority 2: Äáº£m báº£o Consistency
1. Äáº£m báº£o decoder input format giá»‘ng nhau giá»¯a training vÃ  inference
2. Test vá»›i má»™t vÃ i samples Ä‘á»ƒ verify scoring logic

### Priority 3: ThÃªm Debug Logging
1. Log decoder input IDs format
2. Log scoring values Ä‘á»ƒ debug
3. So sÃ¡nh scores giá»¯a cÃ¡c candidates

---

## âœ… Káº¿t luáº­n

**VIP5 training vÃ  evaluation Ä‘ang hoáº¡t Ä‘á»™ng ÄÃšNG vá» cÆ¡ báº£n**, nhÆ°ng cÃ³ **má»™t váº¥n Ä‘á» tiá»m áº©n vá»›i scoring logic** trong `rerank()` method:

1. **Scoring logic cÃ³ thá»ƒ sai**: CÃ¡ch láº¥y logit Ä‘á»ƒ tÃ­nh score cÃ³ thá»ƒ khÃ´ng Ä‘Ãºng vá»›i seq2seq model
2. **Decoder input format cáº§n kiá»ƒm tra**: Äáº£m báº£o cÃ³ pad_token_id á»Ÿ Ä‘áº§u

**Khuyáº¿n nghá»‹**: 
- Kiá»ƒm tra vÃ  sá»­a scoring logic
- Test láº¡i vá»›i má»™t vÃ i samples Ä‘á»ƒ verify
- So sÃ¡nh performance trÆ°á»›c vÃ  sau khi sá»­a

