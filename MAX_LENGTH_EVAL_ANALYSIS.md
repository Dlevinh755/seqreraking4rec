# Ph√¢n t√≠ch Max Length trong Evaluation

## ‚úÖ C√°c model ƒë√£ s·ª≠ d·ª•ng max_length t·ª´ config

### 1. **Qwen3VL Model** ‚úÖ
- **Location**: `rerank/models/qwen3vl.py`
- **S·ª≠ d·ª•ng**: `qwen_max_seq_length` t·ª´ config
- **C√°c methods**:
  - `_predict_probs_raw_image()`: Line 424-430, s·ª≠ d·ª•ng `qwen_max_seq_length` t·ª´ config
  - `_predict_probs_caption()`: Line 523-533, s·ª≠ d·ª•ng `qwen_max_seq_length` t·ª´ config
  - `_predict_probs_semantic_summary_vl()`: Line 614-624, s·ª≠ d·ª•ng `qwen_max_seq_length` t·ª´ config
- **C√°ch s·ª≠ d·ª•ng**: 
  ```python
  base_max_len = getattr(arg, 'qwen_max_seq_length', 2048)
  max_len = base_max_len * 2 if self.mode == "raw_image" else base_max_len
  inputs = self.processor.apply_chat_template(..., max_length=max_len)
  ```

### 2. **Qwen Reranker (Unified)** ‚úÖ
- **Location**: `rerank/methods/qwen_reranker_unified.py`
- **S·ª≠ d·ª•ng**: `qwen_max_seq_length` t·ª´ config qua `_get_max_seq_length()`
- **C√°c ch·ªó s·ª≠ d·ª•ng**:
  - Token analysis: Line 736, 858, 1448
  - Training tokenization: Line 763, 897
- **C√°ch s·ª≠ d·ª•ng**:
  ```python
  max_length = _get_max_seq_length()  # From config
  tokenized = self.qwen3vl_model.tokenizer(..., max_length=max_length)
  ```

---

## ‚ùå C√°c model CH∆ØA s·ª≠ d·ª•ng max_length t·ª´ config

### 1. **LLM Model (Text-only)** ‚ùå

**Location**: `rerank/models/llm.py`

**V·∫•n ƒë·ªÅ**:
- `predict_probs()` method (line 266) g·ªçi `self.tokenizer(prompt, return_tensors="pt")` **KH√îNG c√≥ max_length parameter**
- Prompt c√≥ th·ªÉ b·ªã truncate theo default c·ªßa tokenizer (th∆∞·ªùng l√† 2048 ho·∫∑c model max), kh√¥ng theo config

**Code hi·ªán t·∫°i**:
```python
def predict_probs(self, prompt, num_candidates=None):
    inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
    # ‚ùå KH√îNG c√≥ max_length parameter!
```

**C·∫ßn s·ª≠a**:
```python
def predict_probs(self, prompt, num_candidates=None):
    # Get max_length from config
    try:
        from config import arg
        max_length = getattr(arg, 'qwen_max_seq_length', 2048)
    except ImportError:
        max_length = 2048
    
    inputs = self.tokenizer(
        prompt, 
        return_tensors="pt",
        truncation=True,  # ‚úÖ Truncate if too long
        max_length=max_length,  # ‚úÖ Use from config
    ).to(self.model.device)
```

### 2. **VIP5 Reranker** ‚ùå

**Location**: `rerank/methods/vip5_reranker.py`

**V·∫•n ƒë·ªÅ**:
- S·ª≠ d·ª•ng `self.max_text_length` (default: 128) nh∆∞ng **KH√îNG l·∫•y t·ª´ config**
- Kh√¥ng c√≥ config parameter cho VIP5 max_text_length

**Code hi·ªán t·∫°i**:
```python
def __init__(self, ..., max_text_length: int = 128, ...):
    self.max_text_length = max_text_length  # ‚ùå Hardcoded default, kh√¥ng t·ª´ config
```

**C·∫ßn s·ª≠a**:
1. Th√™m config parameter `vip5_max_text_length` v√†o `config.py`
2. S·ª≠a `__init__()` ƒë·ªÉ l·∫•y t·ª´ config n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p

---

## üìä T√≥m t·∫Øt

| Model | Method | S·ª≠ d·ª•ng max_length t·ª´ config? | Status |
|-------|--------|-------------------------------|--------|
| **Qwen3VL** | `_predict_probs_raw_image()` | ‚úÖ C√≥ | OK |
| **Qwen3VL** | `_predict_probs_caption()` | ‚úÖ C√≥ | OK |
| **Qwen3VL** | `_predict_probs_semantic_summary_vl()` | ‚úÖ C√≥ | OK |
| **Qwen Reranker** | Token analysis | ‚úÖ C√≥ | OK |
| **Qwen Reranker** | Training tokenization | ‚úÖ C√≥ | OK |
| **LLM Model** | `predict_probs()` | ‚ùå Kh√¥ng | **C·∫¶N S·ª¨A** |
| **VIP5 Reranker** | `__init__()` | ‚ùå Kh√¥ng | **C·∫¶N S·ª¨A** |

---

## üîß ƒê·ªÅ xu·∫•t s·ª≠a

### Priority 1: S·ª≠a LLM Model `predict_probs()`

**File**: `rerank/models/llm.py`

**S·ª≠a**:
```python
def predict_probs(self, prompt, num_candidates=None):
    """Predict probabilities for candidates using numbers (1, 2, 3, ...)."""
    # Get max_length from config
    try:
        from config import arg
        max_length = getattr(arg, 'qwen_max_seq_length', 2048)
    except ImportError:
        max_length = 2048  # Default fallback
    
    inputs = self.tokenizer(
        prompt, 
        return_tensors="pt",
        truncation=True,  # ‚úÖ Truncate if too long
        max_length=max_length,  # ‚úÖ Use from config
    ).to(self.model.device)
    
    # ... rest of the code
```

### Priority 2: Th√™m config cho VIP5 v√† s·ª≠a `__init__()`

**File**: `config.py`

**Th√™m**:
```python
parser.add_argument('--vip5_max_text_length', type=int, default=128,
                    help='Maximum text sequence length for VIP5 (default: 128, increase for longer prompts)')
```

**File**: `rerank/methods/vip5_reranker.py`

**S·ª≠a**:
```python
def __init__(
    self,
    ...,
    max_text_length: Optional[int] = None,  # ‚úÖ Optional, l·∫•y t·ª´ config n·∫øu None
    ...
):
    if max_text_length is None:
        try:
            from config import arg
            max_text_length = getattr(arg, 'vip5_max_text_length', 128)
        except ImportError:
            max_text_length = 128  # Default fallback
    
    self.max_text_length = max_text_length
```

---

## ‚úÖ K·∫øt lu·∫≠n

**ƒê√£ s·ª≠ d·ª•ng max_length t·ª´ config**:
- ‚úÖ Qwen3VL Model (t·∫•t c·∫£ modes)
- ‚úÖ Qwen Reranker (tokenization, analysis)

**Ch∆∞a s·ª≠ d·ª•ng max_length t·ª´ config**:
- ‚ùå LLM Model `predict_probs()` - **C·∫¶N S·ª¨A**
- ‚ùå VIP5 Reranker `max_text_length` - **C·∫¶N S·ª¨A**

**Khuy·∫øn ngh·ªã**: S·ª≠a 2 v·∫•n ƒë·ªÅ tr√™n ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ models ƒë·ªÅu s·ª≠ d·ª•ng max_length t·ª´ config khi eval.

