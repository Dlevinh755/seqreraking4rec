# PhÃ¢n tÃ­ch: CÃ³ nÃªn sá»­ dá»¥ng Chat Template Format cho Prompt khi Eval?

## ğŸ” Váº¥n Ä‘á» hiá»‡n táº¡i

### 1. **Training Format**
**File**: `rerank/models/llm.py:186-200`

```python
text = self.tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=False
)
```

**Format**:
```
\n<user_content><|im_end|>\n<|im_start|>assistant\n<response><|im_end|>\n
```

**Äáº·c Ä‘iá»ƒm**:
- âœ… CÃ³ special tokens: `<|im_start|>user\n`, `<|im_start|>assistant\n`, `<|im_end|>`
- âœ… Model Ä‘Æ°á»£c train vá»›i chat template format
- âœ… Model há»c cÃ¡ch predict response sau `<|im_start|>assistant\n`

### 2. **Inference Format (Hiá»‡n táº¡i)**
**File**: `rerank/models/llm.py:296-301`

```python
prompt = build_prompt_from_candidates(...)  # Plain text
inputs = self.tokenizer(
    prompt, 
    return_tensors="pt",
    truncation=True,
    max_length=max_length,
)
```

**Format**:
```
You are a recommendation ranking assistant.

Choose exactly ONE item the user is most likely to interact with next.

User history:
- item1
- item2

Candidate items:
A. candidate1
B. candidate2

Answer with only one letter (A-B).
```

**Äáº·c Ä‘iá»ƒm**:
- âŒ KhÃ´ng cÃ³ special tokens
- âŒ KhÃ´ng cÃ³ chat template format
- âŒ Format khÃ¡c vá»›i training

## âš ï¸ Váº¥n Ä‘á»

### **Format Mismatch giá»¯a Training vÃ  Inference**

1. **Training**: Model Ä‘Æ°á»£c train vá»›i chat template format
   - Input: `'\n<user_content><|im_end|>\n<|im_start|>assistant\n'`
   - Model há»c predict next token sau `<|im_start|>assistant\n`

2. **Inference**: Model nháº­n plain text prompt
   - Input: `'You are a recommendation ranking assistant. ... Answer with only one letter (A-B).'`
   - Model khÃ´ng tháº¥y `<|im_start|>assistant\n` â†’ cÃ³ thá»ƒ bá»‹ confusion

3. **Háº­u quáº£**:
   - Model cÃ³ thá»ƒ khÃ´ng hiá»ƒu context Ä‘Ãºng
   - Performance cÃ³ thá»ƒ giáº£m do format mismatch
   - Model khÃ´ng biáº¿t Ä‘ang á»Ÿ Ä‘Ã¢u trong conversation flow

## âœ… Giáº£i phÃ¡p: Sá»­ dá»¥ng Chat Template Format cho Inference

### **LÃ½ do nÃªn sá»­ dá»¥ng**:

1. **Consistency vá»›i Training**
   - Training vÃ  inference dÃ¹ng cÃ¹ng format
   - Model quen vá»›i format nÃ y
   - Giáº£m distribution shift

2. **ÄÃºng vá»›i Model Design**
   - Qwen models Ä‘Æ°á»£c train vá»›i chat template format
   - Special tokens (`<|im_start|>`, `<|im_end|>`) giÃºp model hiá»ƒu context
   - Model biáº¿t Ä‘ang á»Ÿ Ä‘Ã¢u trong conversation

3. **Better Performance**
   - Model predict next token trong context Ä‘Ãºng
   - Special tokens giÃºp model focus vÃ o response part
   - Giáº£m confusion vá» format

4. **ÄÃºng vá»›i LlamaRec**
   - LlamaRec cÅ©ng sá»­ dá»¥ng chat template format
   - Consistency vá»›i best practices

### **CÃ¡ch implement**:

```python
def predict_probs(self, prompt, num_candidates=None):
    # Convert plain text prompt to chat template format
    messages = [{"role": "user", "content": prompt}]
    
    # Apply chat template with generation prompt
    text = self.tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True  # âœ… Add <|im_start|>assistant\n
    )
    
    # Tokenize
    inputs = self.tokenizer(
        text, 
        return_tensors="pt",
        truncation=True,
        max_length=max_length,
    ).to(self.model.device)
    
    # Predict
    with torch.no_grad():
        outputs = self.model(**inputs)
    
    logits = outputs.logits[:, -1]  # [vocab_size]
    # ... rest of the code
```

### **Format sau khi apply chat template**:

```
<|im_start|>user
You are a recommendation ranking assistant.

Choose exactly ONE item the user is most likely to interact with next.

User history:
- item1
- item2

Candidate items:
A. candidate1
B. candidate2

Answer with only one letter (A-B).<|im_end|>
<|im_start|>assistant
```

**Äáº·c Ä‘iá»ƒm**:
- âœ… CÃ³ `<|im_start|>user\n` á»Ÿ Ä‘áº§u
- âœ… CÃ³ `<|im_end|>` sau user content
- âœ… CÃ³ `<|im_start|>assistant\n` á»Ÿ cuá»‘i (generation prompt)
- âœ… Model predict next token sau `<|im_start|>assistant\n` (giá»‘ng training)

## ğŸ“Š So sÃ¡nh

| Aspect | Plain Text (Hiá»‡n táº¡i) | Chat Template (Äá» xuáº¥t) |
|--------|----------------------|-------------------------|
| **Consistency vá»›i Training** | âŒ KhÃ¡c format | âœ… CÃ¹ng format |
| **Special Tokens** | âŒ KhÃ´ng cÃ³ | âœ… CÃ³ (`<|im_start|>`, `<|im_end|>`) |
| **Model Context** | âš ï¸ KhÃ´ng rÃµ rÃ ng | âœ… RÃµ rÃ ng (user â†’ assistant) |
| **Performance** | âš ï¸ CÃ³ thá»ƒ giáº£m | âœ… Tá»‘t hÆ¡n |
| **LlamaRec Alignment** | âŒ KhÃ´ng | âœ… CÃ³ |

## ğŸ¯ Káº¿t luáº­n

### âœ… **NÃŠN sá»­ dá»¥ng Chat Template Format cho Inference**

**LÃ½ do**:
1. **Consistency**: Training vÃ  inference dÃ¹ng cÃ¹ng format
2. **Performance**: Model hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n vá»›i format quen thuá»™c
3. **Best Practice**: ÄÃºng vá»›i cÃ¡ch Qwen models Ä‘Æ°á»£c design
4. **LlamaRec Alignment**: PhÃ¹ há»£p vá»›i LlamaRec approach

**CÃ¡ch implement**:
- Sá»­a `predict_probs()` Ä‘á»ƒ convert plain text prompt â†’ chat template format
- Sá»­ dá»¥ng `apply_chat_template()` vá»›i `add_generation_prompt=True`
- Äáº£m báº£o format giá»‘ng vá»›i training

**LÆ°u Ã½**:
- Cáº§n test Ä‘á»ƒ Ä‘áº£m báº£o performance khÃ´ng giáº£m
- CÃ³ thá»ƒ cáº§n Ä‘iá»u chá»‰nh `max_length` náº¿u prompt dÃ i hÆ¡n (do thÃªm special tokens)

