{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Model từ Checkpoint và Eval trên Test Candidates\n",
        "\n",
        "Notebook này load model đã train từ checkpoint và chạy evaluation trên test candidates giống như khi train xong.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup và Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd()\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import Dict, List\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import config - handle case when running in notebook (no command line args)\n",
        "try:\n",
        "    # Try to import arg normally\n",
        "    from config import arg\n",
        "except SystemExit:\n",
        "    # If SystemExit occurs (no command line args), create a mock arg object\n",
        "    class MockArg:\n",
        "        def __init__(self):\n",
        "            # Set default values for all config attributes\n",
        "            self.dataset_code = 'beauty'\n",
        "            self.min_rating = 3\n",
        "            self.min_uc = 5\n",
        "            self.min_sc = 5\n",
        "            self.qwen_mode = 'text_only'\n",
        "            self.qwen_model = 'qwen3-0.6b'\n",
        "            self.qwen_max_history = 5\n",
        "            self.qwen_max_candidates = 20\n",
        "            self.use_torch_compile = False\n",
        "            self.rerank_batch_size = 16\n",
        "            self.rerank_epochs = 1\n",
        "            self.rerank_lr = 1e-4\n",
        "            self.rerank_patience = 2\n",
        "            self.rerank_eval_candidates = 20\n",
        "    \n",
        "    arg = MockArg()\n",
        "    print(\"Note: Running in notebook mode with default config values.\")\n",
        "    print(\"To use custom config, set arg attributes manually or run from command line.\")\n",
        "\n",
        "# Import evaluation utilities\n",
        "from evaluation.utils import load_dataset_from_csv, load_rerank_candidates, evaluate_split\n",
        "from evaluation.metrics import recall_at_k, ndcg_at_k, hit_at_k\n",
        "\n",
        "# Import reranker\n",
        "from rerank.models.llm import LLMModel, build_prompt_from_candidates, rank_candidates\n",
        "from rerank.methods.qwen_reranker_unified import QwenReranker\n",
        "\n",
        "print(\"Setup completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cấu hình Checkpoint và Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cấu hình checkpoint path\n",
        "CHECKPOINT_DIR = \"./qwen_rerank\"  # Thư mục chứa checkpoints\n",
        "\n",
        "# Tìm checkpoint mới nhất hoặc chỉ định checkpoint cụ thể\n",
        "checkpoint_path = None\n",
        "if os.path.exists(CHECKPOINT_DIR):\n",
        "    # Tìm checkpoint mới nhất\n",
        "    checkpoints = [d for d in os.listdir(CHECKPOINT_DIR) if d.startswith(\"checkpoint-\")]\n",
        "    if checkpoints:\n",
        "        # Sort by checkpoint number\n",
        "        checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_DIR, checkpoints[-1])\n",
        "        print(f\"Found latest checkpoint: {checkpoint_path}\")\n",
        "    else:\n",
        "        # Nếu không có checkpoint-XXX, thử dùng toàn bộ folder\n",
        "        checkpoint_path = CHECKPOINT_DIR\n",
        "        print(f\"Using checkpoint directory: {checkpoint_path}\")\n",
        "else:\n",
        "    print(f\"Warning: Checkpoint directory {CHECKPOINT_DIR} not found!\")\n",
        "    print(\"Please specify the correct checkpoint path.\")\n",
        "\n",
        "# Dataset config\n",
        "dataset_code = getattr(arg, 'dataset_code', 'beauty')\n",
        "min_rating = getattr(arg, 'min_rating', 3)\n",
        "min_uc = getattr(arg, 'min_uc', 5)\n",
        "min_sc = getattr(arg, 'min_sc', 5)\n",
        "\n",
        "print(f\"\\nDataset: {dataset_code}\")\n",
        "print(f\"Checkpoint: {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Dataset và Test Candidates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "data = load_dataset_from_csv(dataset_code, min_rating, min_uc, min_sc)\n",
        "train = data[\"train\"]\n",
        "val = data[\"val\"]\n",
        "test = data[\"test\"]\n",
        "item_count = data[\"item_count\"]\n",
        "\n",
        "print(f\"Train users: {len(train)}\")\n",
        "print(f\"Val users: {len(val)}\")\n",
        "print(f\"Test users: {len(test)}\")\n",
        "print(f\"Items: {item_count}\")\n",
        "\n",
        "# Load pre-generated candidates\n",
        "print(\"\\nLoading pre-generated candidates...\")\n",
        "all_candidates = load_rerank_candidates(\n",
        "    dataset_code=dataset_code,\n",
        "    min_rating=min_rating,\n",
        "    min_uc=min_uc,\n",
        "    min_sc=min_sc,\n",
        ")\n",
        "\n",
        "test_candidates = all_candidates.get(\"test\", {})\n",
        "print(f\"Test candidates loaded for {len(test_candidates)} users\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Model từ Checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cấu hình model (giống như khi train)\n",
        "from rerank.methods.qwen_reranker_unified import MODEL_MAPPING\n",
        "\n",
        "qwen_mode = getattr(arg, 'qwen_mode', 'text_only')\n",
        "qwen_model = getattr(arg, 'qwen_model', 'qwen3-0.6b')\n",
        "\n",
        "print(f\"Mode: {qwen_mode}\")\n",
        "print(f\"Model: {qwen_model}\")\n",
        "\n",
        "# Load base model (giống như khi train)\n",
        "model_path = MODEL_MAPPING.get(qwen_model.lower(), MODEL_MAPPING['qwen3-0.6b'])\n",
        "print(f\"\\nLoading base model: {model_path}\")\n",
        "\n",
        "# Tạo LLMModel instance\n",
        "llm_model = LLMModel(\n",
        "    train_data=None,  # Không cần training data cho inference\n",
        "    model_name=model_path\n",
        ")\n",
        "\n",
        "# Load base model với cấu hình giống training\n",
        "use_torch_compile = getattr(arg, 'use_torch_compile', False)\n",
        "llm_model.load_model(use_torch_compile=use_torch_compile)\n",
        "\n",
        "# Load checkpoint weights\n",
        "if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "    print(f\"\\nLoading checkpoint from: {checkpoint_path}\")\n",
        "    \n",
        "    # Unsloth lưu checkpoint dưới dạng model folder\n",
        "    # Có thể load bằng FastLanguageModel.from_pretrained với local path\n",
        "    try:\n",
        "        from unsloth import FastLanguageModel\n",
        "        \n",
        "        # Load model từ checkpoint folder\n",
        "        # Unsloth lưu adapter weights trong checkpoint folder\n",
        "        print(\"Loading adapter weights from checkpoint...\")\n",
        "        \n",
        "        # Method 1: Load từ checkpoint folder (nếu là model folder)\n",
        "        if os.path.isdir(checkpoint_path):\n",
        "            # Kiểm tra xem có adapter_model.bin hoặc pytorch_model.bin không\n",
        "            adapter_path = os.path.join(checkpoint_path, \"adapter_model.bin\")\n",
        "            if os.path.exists(adapter_path):\n",
        "                # Load adapter weights\n",
        "                adapter_weights = torch.load(adapter_path, map_location=\"cpu\")\n",
        "                # Merge vào model\n",
        "                from peft import PeftModel\n",
        "                if hasattr(llm_model.model, 'load_adapter'):\n",
        "                    llm_model.model.load_adapter(checkpoint_path)\n",
        "                    print(\"Adapter weights loaded successfully!\")\n",
        "                else:\n",
        "                    # Fallback: load state dict manually\n",
        "                    missing_keys, unexpected_keys = llm_model.model.load_state_dict(adapter_weights, strict=False)\n",
        "                    print(f\"Loaded adapter weights. Missing: {len(missing_keys)}, Unexpected: {len(unexpected_keys)}\")\n",
        "            else:\n",
        "                # Thử load toàn bộ model từ checkpoint folder\n",
        "                try:\n",
        "                    # Unsloth có thể lưu model dưới dạng có thể load trực tiếp\n",
        "                    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "                        model_name=checkpoint_path,\n",
        "                        max_seq_length=2048,\n",
        "                        dtype=torch.float16,\n",
        "                        load_in_4bit=True,\n",
        "                    )\n",
        "                    llm_model.model = model\n",
        "                    llm_model.tokenizer = tokenizer\n",
        "                    print(\"Model loaded from checkpoint folder successfully!\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not load from checkpoint folder: {e}\")\n",
        "                    print(\"Using base model without checkpoint weights.\")\n",
        "        else:\n",
        "            print(f\"Checkpoint path is not a directory: {checkpoint_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not load checkpoint: {e}\")\n",
        "        print(\"Using base model without checkpoint weights.\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Using base model.\")\n",
        "\n",
        "# Set model to eval mode\n",
        "llm_model.model.eval()\n",
        "print(\"\\nModel ready for evaluation!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Setup QwenReranker với Model đã Load\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tạo QwenReranker instance\n",
        "reranker = QwenReranker(\n",
        "    top_k=50,\n",
        "    mode=qwen_mode,\n",
        "    model=qwen_model,\n",
        "    max_history=getattr(arg, 'qwen_max_history', 5),\n",
        "    max_candidates=getattr(arg, 'qwen_max_candidates', 20),\n",
        ")\n",
        "\n",
        "# Gán model đã load vào reranker\n",
        "reranker.llm_model = llm_model\n",
        "\n",
        "# Load item metadata\n",
        "item_id2text = {}\n",
        "item_meta = {}\n",
        "user_history = {}\n",
        "\n",
        "if \"meta\" in data:\n",
        "    for item_id, meta in data[\"meta\"].items():\n",
        "        text = meta.get(\"text\") if meta else None\n",
        "        if text:\n",
        "            item_id2text[item_id] = text\n",
        "        item_meta[item_id] = meta if meta else {}\n",
        "    \n",
        "    # Build user history texts\n",
        "    for user_id, items in train.items():\n",
        "        user_history[user_id] = [\n",
        "            item_id2text.get(item_id, f\"item_{item_id}\")\n",
        "            for item_id in items\n",
        "            if item_id in item_id2text\n",
        "        ]\n",
        "\n",
        "# Set data structures\n",
        "reranker.item_id2text = item_id2text\n",
        "reranker.item_meta = item_meta\n",
        "reranker.user_history = user_history\n",
        "reranker.train_user_history = train  # For history lookup\n",
        "reranker.is_fitted = True  # Mark as fitted\n",
        "\n",
        "print(f\"Reranker setup completed!\")\n",
        "print(f\"Item texts: {len(item_id2text)}\")\n",
        "print(f\"User histories: {len(user_history)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation trên Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation function giống như trong training\n",
        "def evaluate_test_split(split: Dict[int, List[int]], candidates_dict: Dict[int, List[int]], k: int = 10) -> Dict[str, float]:\n",
        "    \"\"\"Compute metrics on test split.\"\"\"\n",
        "    recalls = []\n",
        "    ndcgs = []\n",
        "    hits = []\n",
        "    \n",
        "    users = sorted(split.keys())\n",
        "    \n",
        "    for user_id in tqdm(users, desc=\"Evaluating\"):\n",
        "        gt_items = split.get(user_id, [])\n",
        "        if not gt_items:\n",
        "            continue\n",
        "        \n",
        "        # Get candidates for this user\n",
        "        candidates = candidates_dict.get(user_id, [])\n",
        "        if not candidates:\n",
        "            continue\n",
        "        \n",
        "        # Get user history\n",
        "        history = reranker.train_user_history.get(user_id, [])\n",
        "        if len(history) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Shuffle candidates (giống như trong training)\n",
        "        random.shuffle(candidates)\n",
        "        \n",
        "        # Rerank\n",
        "        try:\n",
        "            reranked = reranker.rerank(user_id, candidates, user_history=history)\n",
        "            ranked_items = [item_id for item_id, _ in reranked[:k]]\n",
        "            \n",
        "            # Compute metrics\n",
        "            r = recall_at_k(ranked_items, gt_items, k)\n",
        "            n = ndcg_at_k(ranked_items, gt_items, k)\n",
        "            h = hit_at_k(ranked_items, gt_items, k)\n",
        "            \n",
        "            recalls.append(r)\n",
        "            ndcgs.append(n)\n",
        "            hits.append(h)\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating user {user_id}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return {\n",
        "        f\"recall@{k}\": float(np.mean(recalls)) if recalls else 0.0,\n",
        "        f\"ndcg@{k}\": float(np.mean(ndcgs)) if ndcgs else 0.0,\n",
        "        f\"hit@{k}\": float(np.mean(hits)) if hits else 0.0,\n",
        "        \"num_users\": len(recalls)\n",
        "    }\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "ks = [1, 5, 10, 20]\n",
        "test_metrics = {}\n",
        "\n",
        "for k in ks:\n",
        "    print(f\"\\nComputing metrics @{k}...\")\n",
        "    metrics = evaluate_test_split(test, test_candidates, k=k)\n",
        "    test_metrics.update(metrics)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Test Evaluation Results\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Mode: {qwen_mode}\")\n",
        "print(f\"Model: {qwen_model}\")\n",
        "print(f\"Checkpoint: {checkpoint_path}\")\n",
        "print(f\"Evaluated users: {test_metrics.get('num_users', 0)}\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Metric':<12} {'@1':>10} {'@5':>10} {'@10':>10} {'@20':>10}\")\n",
        "\n",
        "for metric_name in [\"recall\", \"ndcg\", \"hit\"]:\n",
        "    values = [test_metrics.get(f\"{metric_name}@{k}\", 0.0) for k in ks]\n",
        "    print(f\"{metric_name.capitalize():<12} {values[0]:>10.4f} {values[1]:>10.4f} {values[2]:>10.4f} {values[3]:>10.4f}\")\n",
        "\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. (Optional) So sánh với Validation Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on validation set để so sánh\n",
        "val_candidates = all_candidates.get(\"val\", {})\n",
        "\n",
        "if val_candidates:\n",
        "    print(\"\\nEvaluating on validation set for comparison...\")\n",
        "    val_metrics = {}\n",
        "    \n",
        "    for k in ks:\n",
        "        metrics = evaluate_test_split(val, val_candidates, k=k)\n",
        "        val_metrics.update(metrics)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Validation Evaluation Results\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Evaluated users: {val_metrics.get('num_users', 0)}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Metric':<12} {'@1':>10} {'@5':>10} {'@10':>10} {'@20':>10}\")\n",
        "    \n",
        "    for metric_name in [\"recall\", \"ndcg\", \"hit\"]:\n",
        "        values = [val_metrics.get(f\"{metric_name}@{k}\", 0.0) for k in ks]\n",
        "        print(f\"{metric_name.capitalize():<12} {values[0]:>10.4f} {values[1]:>10.4f} {values[2]:>10.4f} {values[3]:>10.4f}\")\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    print(\"No validation candidates found.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
