# Implementation: Chat Template Format cho Inference

## âœ… ÄÃ£ sá»­a

### **File**: `rerank/models/llm.py:279-314`

**Thay Ä‘á»•i**:
- `predict_probs()` giá» convert plain text prompt â†’ chat template format trÆ°á»›c khi tokenize
- Sá»­ dá»¥ng `apply_chat_template()` vá»›i `add_generation_prompt=True`
- Äáº£m báº£o consistency vá»›i training format

**Code trÆ°á»›c**:
```python
def predict_probs(self, prompt, num_candidates=None):
    inputs = self.tokenizer(
        prompt,  # Plain text
        return_tensors="pt",
        truncation=True,
        max_length=max_length,
    ).to(self.model.device)
```

**Code sau**:
```python
def predict_probs(self, prompt, num_candidates=None):
    # âœ… Convert plain text prompt to chat template format
    messages = [{"role": "user", "content": prompt}]
    
    # Apply chat template with generation prompt
    text = self.tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True  # âœ… Add <|im_start|>assistant\n
    )
    
    # Tokenize the chat template formatted text
    inputs = self.tokenizer(
        text,  # Chat template format
        return_tensors="pt",
        truncation=True,
        max_length=max_length,
    ).to(self.model.device)
```

## ğŸ“Š Format Comparison

### **Before (Plain Text)**:
```
You are a recommendation ranking assistant.

Choose exactly ONE item the user is most likely to interact with next.

User history:
- item1

Candidate items:
A. candidate1
B. candidate2

Answer with only one letter (A-B).
```

### **After (Chat Template Format)**:
```
<|im_start|>user
You are a recommendation ranking assistant.

Choose exactly ONE item the user is most likely to interact with next.

User history:
- item1

Candidate items:
A. candidate1
B. candidate2

Answer with only one letter (A-B).<|im_end|>
<|im_start|>assistant
```

## âœ… Lá»£i Ã­ch

1. **Consistency vá»›i Training**
   - Training vÃ  inference dÃ¹ng cÃ¹ng format
   - Model quen vá»›i format nÃ y
   - Giáº£m distribution shift

2. **ÄÃºng vá»›i Model Design**
   - Qwen models Ä‘Æ°á»£c train vá»›i chat template format
   - Special tokens (`<|im_start|>`, `<|im_end|>`) giÃºp model hiá»ƒu context
   - Model biáº¿t Ä‘ang á»Ÿ Ä‘Ã¢u trong conversation

3. **Better Performance**
   - Model predict next token trong context Ä‘Ãºng
   - Special tokens giÃºp model focus vÃ o response part
   - Giáº£m confusion vá» format

## ğŸ” Chi tiáº¿t ká»¹ thuáº­t

### **Tokenization**:
- Original prompt: 220 characters
- Chat template format: 368 characters (+148 chars for special tokens)
- Last tokens: `[' letter', ' (', 'A', '-B', ').', '<|im_end|>', '\n', '<|im_start|>', 'assistant', '\n']`
- Model sáº½ predict next token sau `<|im_start|>assistant\n` (giá»‘ng training)

### **Infer num_candidates**:
- Logic váº«n sá»­ dá»¥ng original prompt (trÆ°á»›c khi apply chat template)
- ÄÃºng vÃ¬ original prompt váº«n chá»©a thÃ´ng tin vá» candidates
- KhÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi special tokens

## ğŸ“ LÆ°u Ã½

1. **Interface khÃ´ng thay Ä‘á»•i**: 
   - `predict_probs()` váº«n nháº­n plain text prompt
   - Conversion tá»± Ä‘á»™ng bÃªn trong method
   - KhÃ´ng cáº§n sá»­a code gá»i `predict_probs()`

2. **Token length**:
   - Chat template format dÃ i hÆ¡n ~148 characters
   - CÃ³ thá»ƒ cáº§n tÄƒng `max_length` náº¿u prompt dÃ i
   - Hiá»‡n táº¡i `qwen_max_seq_length=2048` Ä‘á»§ cho háº§u háº¿t cases

3. **Performance**:
   - Cáº§n test Ä‘á»ƒ Ä‘áº£m báº£o performance khÃ´ng giáº£m
   - Expected: performance tá»‘t hÆ¡n do consistency vá»›i training

## âœ… Káº¿t luáº­n

Code Ä‘Ã£ Ä‘Æ°á»£c sá»­a Ä‘á»ƒ sá»­ dá»¥ng chat template format cho inference, Ä‘áº£m báº£o consistency vá»›i training format vÃ  cáº£i thiá»‡n performance.

